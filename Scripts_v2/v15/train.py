# -*- coding: utf-8 -*-
"""
SDXL ControlNet è®­ç»ƒè„šæœ¬ v15
åŸºäº v14 é€»è¾‘é‡æ„ï¼Œæ”¯æŒ CF-FA å’Œ CF-OCT æ•°æ®é›†ã€‚

ã€æ ¸å¿ƒå˜åŠ¨ã€‘
1. ç§»é™¤ CSV ä¾èµ–ï¼šç›´æ¥ä»æŒ‡å®šç›®å½•è¯»å–é…å¯¹å›¾åƒã€‚
2. ç§»é™¤ CF-OCTAï¼šä¸“æ³¨äº CF-FA å’Œ CF-OCT ä»»åŠ¡ã€‚
3. åŠ¨æ€è¡€ç®¡æå–ï¼šDataset ä¸å†è¿”å› vessel å›¾ï¼Œç”±è®­ç»ƒå¾ªç¯è°ƒç”¨ vessle_detector å®æ—¶ç”Ÿæˆã€‚
4. ç»§æ‰¿ v14 é€»è¾‘ï¼šä¿ç•™ MSE + MS-SSIM + Vessel Dice + Gradient Match Loss ç»„åˆã€‚
5. å®Œå¤‡çš„è®­ç»ƒç­–ç•¥ï¼šåŒ…æ‹¬æ—©åœæœºåˆ¶ï¼ˆEarly Stoppingï¼‰ã€å­¦ä¹ ç‡è¡°å‡ã€å›ºå®šå­é›†éªŒè¯ã€‚
"""

import os
import csv
import math
import time
import random
import argparse
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import transforms
from diffusers import (DDPMScheduler, ControlNetModel, AutoencoderKL, UNet2DConditionModel, 
                       StableDiffusionControlNetPipeline, MultiControlNetModel)
from transformers import CLIPTextModel, CLIPTokenizer
from pytorch_msssim import MS_SSIM
#import bitsandbytes as bnb
# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
# å°†æ•°æ®ç›®å½•åŠ å…¥è·¯å¾„ä»¥ä¾¿å¯¼å…¥ dataset
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CFFA_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cfoct_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_octfa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CF_OCTA_v2_repaired"))
from cffa_augmented_dataset import CFFADataset
from operation_pre_filtered_cfoct_augmented_dataset import CFOCTDataset
from operation_pre_filtered_octfa_augmented_dataset import OCTFADataset
from cf_octa_v2_repaired_dataset import CFOCTADataset
from vessle_detector import extract_vessel_map

# ============ å…¨å±€é…ç½® ============
SIZE = 512
DEVICE = torch.device("cuda")
# æ¨¡å‹è·¯å¾„
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
SCRIBBLE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-scribble"
TILE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-tile"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_ctrl_sd15_dual"

# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_prompt_embeds(bs, tokenizer, text_encoder):
    """ç”Ÿæˆç©ºæç¤ºè¯çš„æ–‡æœ¬åµŒå…¥"""
    prompts = [""] * bs
    inputs = tokenizer(prompts, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt").to(DEVICE)
    return text_encoder(inputs.input_ids)[0]

def compute_image_gradients(image):
    """è®¡ç®—å›¾åƒçš„ Sobel æ¢¯åº¦ï¼ˆç”¨äºæ¢¯åº¦åŒ¹é…æŸå¤±ï¼‰"""
    kernel_x = torch.tensor([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], device=DEVICE).view(1, 1, 3, 3).expand(image.shape[1], 1, 3, 3)
    kernel_y = torch.tensor([[1., 2., 1.], [0., 0., 0.], [-1., -2., -1.]], device=DEVICE).view(1, 1, 3, 3).expand(image.shape[1], 1, 3, 3)
    grad_x = F.conv2d(image, kernel_x, padding=1, groups=image.shape[1])
    grad_y = F.conv2d(image, kernel_y, padding=1, groups=image.shape[1])
    return grad_x, grad_y

def compute_gradient_match_loss(pred, gt):
    """æ¢¯åº¦åŒ¹é…æŸå¤±ï¼šçº¦æŸé¢„æµ‹å›¾ä¸ GT åœ¨è¾¹ç¼˜ç©ºé—´çš„ä¸€è‡´æ€§"""
    pred_gray = pred[:, 1:2, :, :] # ä½¿ç”¨ç»¿è‰²é€šé“
    gt_gray = gt[:, 1:2, :, :]
    px, py = compute_image_gradients(pred_gray)
    gx, gy = compute_image_gradients(gt_gray)
    return F.l1_loss(px, gx) + F.l1_loss(py, gy)

def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡"""
    if step < 4000: return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2

# ============ 2. æ ¸å¿ƒæŸå¤±è®¡ç®— ============

def compute_total_loss(noise_pred, noise, noisy_latents, latents, timesteps, vae, noise_scheduler, msssim_fn, args):
    """è®¡ç®—ç»¼åˆæŸå¤±ï¼šMSE + MS-SSIM + Vessel Dice + Gradient"""
    # 1. å™ªå£°ç©ºé—´ MSE æŸå¤±
    loss_mse = F.mse_loss(noise_pred, noise)
    
    # ä»å™ªå£°é¢„æµ‹ä¸­æ¢å¤å›¾åƒ (x0 é¢„æµ‹)
    alphas = noise_scheduler.alphas_cumprod.to(DEVICE)
    at = alphas[timesteps].view(-1, 1, 1, 1)
    pred_x0_latents = (noisy_latents - (1 - at).sqrt() * noise_pred) / at.sqrt()
    
    # è§£ç åˆ°åƒç´ ç©ºé—´ [-1, 1]
    pred_imgs = vae.decode(pred_x0_latents / vae.config.scaling_factor).sample
    with torch.no_grad():
        gt_imgs = vae.decode(latents / vae.config.scaling_factor).sample
    
    pred_01 = (pred_imgs.clamp(-1, 1) + 1) / 2
    gt_01 = (gt_imgs.clamp(-1, 1) + 1) / 2
    
    # 2. MS-SSIM æŸå¤±
    loss_msssim = 1 - msssim_fn(pred_01, gt_01) if args.msssim_lambda > 0 else torch.tensor(0.0).to(DEVICE)
    
    # 3. è¡€ç®¡ç»“æ„æŸå¤± (Dice Loss)
    source_type, target_type = args.mode.split('2')
    pred_vessel = extract_vessel_map(pred_01, target_type, args.mode)
    with torch.no_grad():
        gt_vessel = extract_vessel_map(gt_01, target_type, args.mode)
    
    smooth = 1e-5
    intersection = (pred_vessel * gt_vessel).sum()
    dice_coeff = (2.0 * intersection + smooth) / (pred_vessel.sum() + gt_vessel.sum() + smooth)
    loss_vessel = 1.0 - dice_coeff
    
    # 4. æ¢¯åº¦åŒ¹é…æŸå¤±
    loss_grad = compute_gradient_match_loss(pred_01, gt_01)
    
    # ç»„åˆæ€»æŸå¤±
    total_loss = loss_mse + args.msssim_lambda * loss_msssim + args.vessel_lambda * loss_vessel + args.grad_lambda * loss_grad
    return total_loss, loss_mse, loss_msssim, loss_vessel, loss_grad

# ============ 3. éªŒè¯ä¸æ—©åœé€»è¾‘ ============

def evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, msssim_fn, tokenizer, text_encoder, args):
    """åœ¨å›ºå®šéªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    cn_s.eval(); cn_t.eval()
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cond_tile, tgt, _, _ = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            source_type, _ = args.mode.split('2')
            vessel_map = extract_vessel_map(cond_tile, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # VAE ç¼–ç 
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder)
            
            # ControlNet æ¨ç†
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            noise_pred = unet(noisy_latents, timesteps, prompt_embeds, 
                              down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                              mid_block_additional_residual=mid_s+mid_t).sample
            
            # 1-step x0 é¢„æµ‹ä¸åƒç´ çº§ MSE éªŒè¯æŸè€—
            alphas = noise_scheduler.alphas_cumprod.to(DEVICE)
            at = alphas[timesteps].view(-1, 1, 1, 1)
            pred_x0_latents = (noisy_latents - (1 - at).sqrt() * noise_pred) / at.sqrt()
            pred_imgs = vae.decode(pred_x0_latents / vae.config.scaling_factor).sample
            
            val_loss = F.mse_loss(pred_imgs, tgt)
            val_losses.append(val_loss.item())
            
    cn_s.train(); cn_t.train()
    torch.cuda.empty_cache()
    return np.mean(val_losses)

def visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, step, out_dir):
    """è¿è¡Œå…¨é‡æ¨ç†å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ (å¯¹é½ v14)"""
    print(f"\n[å¯è§†åŒ–] æ­£åœ¨è¿è¡Œæ¨ç†å¯è§†åŒ– (Step {step})...")
    
    # åˆ›å»ºæ¨ç†æµ‹è¯•ç›®å½•
    infer_dir = os.path.join(out_dir, f"step_{step}_inference")
    os.makedirs(infer_dir, exist_ok=True)
    
    # ä¸´æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼
    cn_s.eval(); cn_t.eval()
    
    # æ„å»º pipeline
    multi_controlnet = MultiControlNetModel([cn_s, cn_t])
    pipe = StableDiffusionControlNetPipeline(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        unet=unet,
        controlnet=multi_controlnet,
        scheduler=noise_scheduler,
        safety_checker=None,
        feature_extractor=None
    ).to(DEVICE)
    pipe.set_progress_bar_config(disable=True)
    
    # åªå–å‰ 2 ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    with torch.no_grad():
        for i, batch in enumerate(val_loader):
            if i >= 2: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            source_type, _ = args.mode.split('2')
            vessel_map = extract_vessel_map(cond_tile, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # æ¨ç†
            generator = torch.Generator(device=DEVICE).manual_seed(42)
            # æ¨ç†å°ºå¯¸è·Ÿéš Dataset (512) æˆ–å…¨å±€é…ç½®
            h, w = cond_tile.shape[2], cond_tile.shape[3]
            
            output_img = pipe(
                prompt="",
                image=[cond_scribble, cond_tile],
                num_inference_steps=25,
                controlnet_conditioning_scale=[args.scribble_scale, args.tile_scale],
                generator=generator,
                width=w,
                height=h
            ).images[0]
            
            # ä¿å­˜ç»“æœ
            try:
                name = os.path.splitext(os.path.basename(cp[0]))[0]
            except:
                name = f"sample_{i}"
                
            # ä¿å­˜è¾“å…¥å’Œç›®æ ‡
            # cond_scribble/tile are [0, 1]
            cond_scribble_save = (cond_scribble[0].cpu().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
            cond_tile_save = (cond_tile[0].cpu().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
            # tgt is [-1, 1]
            tgt_save = ((tgt[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            
            Image.fromarray(cond_scribble_save).save(os.path.join(infer_dir, f"{name}_01_scribble.png"))
            Image.fromarray(cond_tile_save).save(os.path.join(infer_dir, f"{name}_02_tile.png"))
            Image.fromarray(tgt_save).save(os.path.join(infer_dir, f"{name}_03_target.png"))
            output_img.save(os.path.join(infer_dir, f"{name}_04_pred.png"))

    # æ¢å¤è®­ç»ƒæ¨¡å¼
    cn_s.train(); cn_t.train()
    
    # æ˜¾å¼æ¸…ç†æ˜¾å­˜ (é˜²æ­¢ OOM)
    del pipe
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ“ æ¨ç†å¯è§†åŒ–å·²ä¿å­˜åˆ°: {infer_dir}\n")

# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["cf2fa", "fa2cf", "cf2oct", "oct2cf", "fa2oct", "oct2fa", "cf2octa", "octa2cf"], required=True)
    parser.add_argument("-n", "--name", default="exp_v15")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--scribble_scale", type=float, default=0.8)
    parser.add_argument("--tile_scale", type=float, default=1.0)
    parser.add_argument("--msssim_lambda", type=float, default=0.1)
    parser.add_argument("--vessel_lambda", type=float, default=0.05)
    parser.add_argument("--grad_lambda", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=8)
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.mode, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½
    if 'octa' in args.mode:
        train_ds = CFOCTADataset(split='train', mode=args.mode)
        val_ds = CFOCTADataset(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'fa' in args.mode:
        train_ds = CFFADataset(split='train', mode=args.mode)
        val_ds = CFFADataset(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'oct' in args.mode:
        train_ds = CFOCTDataset(split='train', mode=args.mode)
        val_ds = CFOCTDataset(split='test', mode=args.mode)
    elif 'fa' in args.mode and 'oct' in args.mode:
        train_ds = OCTFADataset(split='train', mode=args.mode)
        val_ds = OCTFADataset(split='test', mode=args.mode)
    else:
        raise ValueError(f"Unknown mode: {args.mode}")
    
    # éªŒè¯é›†ä½¿ç”¨å›ºå®šå­é›†ï¼ˆ10ä¸ªæ ·æœ¬ï¼‰æé«˜æ•ˆç‡
    val_indices = random.sample(range(len(val_ds)), min(10, len(val_ds)))
    val_subset = torch.utils.data.Subset(val_ds, val_indices)
    
    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)

    # 2. æ¨¡å‹åŠ è½½
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    cn_s = ControlNetModel.from_pretrained(SCRIBBLE_CN_DIR).to(DEVICE)
    cn_t = ControlNetModel.from_pretrained(TILE_CN_DIR).to(DEVICE)
    
    unet.requires_grad_(False); vae.requires_grad_(False); text_encoder.requires_grad_(False)
    
    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    optimizer = torch.optim.AdamW(list(cn_s.parameters()) + list(cn_t.parameters()), lr=5e-5, weight_decay=1e-2)
    # optimizer = bnb.optim.AdamW8bit(list(cn_s.parameters()) + list(cn_t.parameters()), lr=5e-5)
    msssim_fn = MS_SSIM(data_range=1.0, size_average=True, channel=3).to(DEVICE)

    # 3. è®­ç»ƒçŠ¶æ€å˜é‡
    global_step = 0
    best_val_loss = float('inf')
    wait = 0
    start_time = time.time()

    # æ—¥å¿—ç´¯åŠ å™¨ (å¯¹é½ v14)
    loss_accumulator = []
    msssim_loss_accumulator = []
    vessel_loss_accumulator = []
    grad_loss_accumulator = []

    print(f"\nå¼€å§‹è®­ç»ƒ [{args.mode}] - æ ·æœ¬æ•°: {len(train_ds)}")
    
    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # ã€æ ¸å¿ƒé€»è¾‘ã€‘å®æ—¶ç”Ÿæˆè¡€ç®¡å›¾ä½œä¸ºæ¡ä»¶è¾“å…¥
            source_type, _ = args.mode.split('2')
            with torch.no_grad():
                vessel_map = extract_vessel_map(cond_tile, source_type, args.mode)
                cond_scribble = vessel_map.repeat(1, 3, 1, 1)

            # Debug: Step 0 å›¾åƒä¿å­˜ (å¯¹é½ v14)
            if global_step == 0:
                debug_dir = os.path.join(out_dir, "debug_images_step0")
                os.makedirs(debug_dir, exist_ok=True)
                
                # å°è¯•è·å–æ–‡ä»¶å
                try:
                    name = os.path.splitext(os.path.basename(cp[0]))[0]
                except:
                    name = "step0_sample"

                # 1. ä¿å­˜Scribbleæ¡ä»¶å›¾ (Vessel)
                cond_scribble_save = (cond_scribble[0].cpu().float().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_scribble_save).save(os.path.join(debug_dir, f"{name}_scribble_input.png"))
                
                # 2. ä¿å­˜Tileæ¡ä»¶å›¾ (åŸå›¾) [Assume [0, 1]]
                cond_tile_save = (cond_tile[0].cpu().float().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_tile_save).save(os.path.join(debug_dir, f"{name}_tile_input.png"))
                
                # 3. ä¿å­˜ç›®æ ‡å›¾ (GT) [Assume [-1, 1]]
                tgt_save = ((tgt[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(tgt_save).save(os.path.join(debug_dir, f"{name}_target.png"))
                
                print(f"\nâœ“ Step 0 è°ƒè¯•å›¾åƒå·²ä¿å­˜åˆ°: {debug_dir}\n")

            # VAE & å™ªå£°å¤„ç†
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder)
            
            # åŒè·¯ ControlNet å‰å‘
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            # UNet é¢„æµ‹
            noise_pred = unet(noisy_latents, timesteps, prompt_embeds, 
                              down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                              mid_block_additional_residual=mid_s+mid_t).sample
            
            # è®¡ç®— Loss
            loss, l_mse, l_ssim, l_vessel, l_grad = compute_total_loss(noise_pred, noise, noisy_latents, latents, timesteps, vae, noise_scheduler, msssim_fn, args)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # åŠ¨æ€å­¦ä¹ ç‡æ›´æ–°
            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups: param_group['lr'] = current_lr
            
            # ç»Ÿè®¡ (å¯¹é½ v14)
            loss_accumulator.append(l_mse.item())
            msssim_loss_accumulator.append(l_ssim.item())
            vessel_loss_accumulator.append(l_vessel.item())
            grad_loss_accumulator.append(l_grad.item())
            
            # æ—¥å¿—æ‰“å° (å¯¹é½ v14)
            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                
                avg_mse = np.mean(loss_accumulator)
                avg_ssim = np.mean(msssim_loss_accumulator)
                avg_vessel = np.mean(vessel_loss_accumulator)
                avg_grad = np.mean(grad_loss_accumulator)
                
                loss_accumulator = []
                msssim_loss_accumulator = []
                vessel_loss_accumulator = []
                grad_loss_accumulator = []
                
                t_val = timesteps[0].item()
                
                msg_parts = [
                    f"[SD15-v15] step {global_step:5d}/{args.max_steps}",
                    f"lr:{current_lr:.2e}",
                    f"mse:{avg_mse:.4f}",
                ]
                if args.vessel_lambda > 0:
                    msg_parts.append(f"vessel:{avg_vessel:.4f}(Î»={args.vessel_lambda})")
                if args.msssim_lambda > 0:
                    msg_parts.append(f"msssim:{avg_ssim:.4f}(Î»={args.msssim_lambda})")
                if args.grad_lambda > 0:
                    msg_parts.append(f"grad:{avg_grad:.4f}(Î»={args.grad_lambda})")
                
                msg_parts.extend([
                    f"t={t_val:3d}",
                    f"S:{args.scribble_scale}",
                    f"T:{args.tile_scale}",
                    f"{elapsed:.1f}s"
                ])
                msg = " | ".join(msg_parts)
                print(msg)
                
                # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
                with open(os.path.join(out_dir, "training_log.txt"), "a") as f:
                    f.write(msg + "\n")
                
                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯ & æ—©åœåˆ¤æ–­
            if global_step % 500 == 0:
                val_loss = evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, msssim_fn, tokenizer, text_encoder, args)
                
                # è®°å½•éªŒè¯æ—¥å¿— (å¯¹é½éœ€æ±‚)
                val_msg = f"[éªŒè¯] Step {global_step} | Avg Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a") as f:
                    f.write(val_msg + "\n")
                
                # ã€å¯¹é½ v14ã€‘è¿è¡Œæ¨ç†å¯è§†åŒ–
                visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, global_step, out_dir)

                # ä¿å­˜æœ€æ–°æƒé‡
                latest_dir = os.path.join(out_dir, "latest_checkpoint")
                os.makedirs(latest_dir, exist_ok=True)
                cn_s.save_pretrained(os.path.join(latest_dir, "controlnet_scribble"))
                cn_t.save_pretrained(os.path.join(latest_dir, "controlnet_tile"))
                
                # ä¿å­˜æœ€æ–°å…ƒä¿¡æ¯ (å¯¹é½ v14)
                with open(os.path.join(latest_dir, "latest_info.txt"), "w") as f:
                    f.write(f"Latest Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"Best Loss: {best_val_loss:.6f}\n")
                
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    wait = 0
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)
                    cn_s.save_pretrained(os.path.join(best_dir, "controlnet_scribble"))
                    cn_t.save_pretrained(os.path.join(best_dir, "controlnet_tile"))
                    
                    # ä¿å­˜æœ€ä½³å…ƒä¿¡æ¯ (å¯¹é½ v14)
                    with open(os.path.join(best_dir, "best_info.txt"), "w") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                    
                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a") as f:
                        f.write(best_msg)
                else:
                    if global_step >= 4000: # Warm-up åæ‰è§¦å‘ patience
                        wait += 1
                        print(f"âš  éªŒè¯æŸè€—æœªä¸‹é™ ({wait}/{args.patience})\n")
                        if wait >= args.patience:
                            print("ğŸ›‘ è§¦å‘æ—©åœï¼Œè®­ç»ƒç»“æŸã€‚")
                            return

            global_step += 1

if __name__ == "__main__":
    main()

