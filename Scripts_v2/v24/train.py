# -*- coding: utf-8 -*-
"""
SDXL ControlNet è®­ç»ƒè„šæœ¬ v21
åŸºäº v18 æ”¹è¿›ï¼Œä¸“æ³¨è§£å†³"ç»“æ„å¥½ä½†çº¹ç†/äº®åº¦ä¸çœŸå®"é—®é¢˜

ã€æ ¸å¿ƒå˜åŠ¨ - é’ˆå¯¹è§†è§‰å›¾çµæµ‹è¯•ã€‘
1. âœ… UNet LoRA è®­ç»ƒï¼šè®© UNet å­¦ä¹ åŒ»å­¦å›¾åƒçš„çº¹ç†å’Œäº®åº¦åˆ†å¸ƒï¼ˆv18 ä¸­ UNet è¢«å†»ç»“ï¼‰
2. âœ… ç§»é™¤æ‰€æœ‰åƒç´ çº§æŸå¤±ï¼šåªä¿ç•™çº¯ç²¹çš„å™ªå£°é¢„æµ‹ MSEï¼ˆç§»é™¤ SSIM/Vessel/Gradient/Texture Lossï¼‰
3. âœ… åŒ»å­¦å›¾åƒ Promptï¼šä½¿ç”¨é¢†åŸŸç‰¹å®šçš„ prompt è€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
4. âœ… Offset Noiseï¼šè§£å†³äº®åº¦åäº®ã€å¯¹æ¯”åº¦ä¸è¶³çš„é—®é¢˜
5. âœ… åŒæ—¶è®­ç»ƒ ControlNet + UNet LoRAï¼Œå„å¸å…¶èŒï¼ˆç»“æ„ vs çº¹ç†ï¼‰
"""

import os
import math
import time
import random
import argparse
import gc
import shutil
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import transforms
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType
#import bitsandbytes as bnb

# å…±äº« Self-Attention ç›¸å…³ï¼ˆv23 æ–°å¢ï¼Œä¿æŒå‰å‘å…¼å®¹ï¼šè€ä»£ç å¦‚ä¸è°ƒç”¨æ–°å‚æ•°ï¼Œè¡Œä¸ºå®Œå…¨ä¸å˜ï¼‰
from shared_self_attention import apply_shared_self_attention

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
# å°†æ•°æ®ç›®å½•åŠ å…¥è·¯å¾„ä»¥ä¾¿å¯¼å…¥ CFFA dataset
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cffa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CFFA_augmented"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2

# ============ å…¨å±€é…ç½® ============
SIZE = 512
DEVICE = torch.device("cuda")
# æ¨¡å‹è·¯å¾„ï¼ˆä»…ä½¿ç”¨åŸºç¡€ SD15 Diffusersï¼‰
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_ctrl_sd15_dual"

# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_prompt_embeds(bs, tokenizer, text_encoder, mode="cf2fa"):
    """
    ç”ŸæˆåŒ»å­¦å›¾åƒé¢†åŸŸç‰¹å®šçš„æç¤ºè¯åµŒå…¥
    
    ã€v21 æ”¹è¿›ã€‘ä¸å†ä½¿ç”¨ç©º promptï¼Œè€Œæ˜¯ä½¿ç”¨é¢†åŸŸç‰¹å®šæè¿°
    è¿™æœ‰åŠ©äºæ¿€æ´»æ¨¡å‹ä¸­ä¸åŒ»å­¦å½±åƒç›¸å…³çš„æ½œåœ¨è¯­ä¹‰åˆ†å¸ƒ
    """
    if 'fa' in mode:
        # FA (è§å…‰è¡€ç®¡é€ å½±) çš„ç‰¹å¾ï¼šé«˜å¯¹æ¯”åº¦ã€é»‘èƒŒæ™¯ã€äº®è¡€ç®¡ã€é¢—ç²’å™ªå£°
        prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    elif 'oct' in mode:
        # OCT çš„ç‰¹å¾ï¼šå±‚çŠ¶ç»“æ„ã€ç°åº¦å›¾
        prompt = "optical coherence tomography, retinal cross section, medical scan, grayscale"
    elif 'cf' in mode:
        # CF (å½©è‰²çœ¼åº•) çš„ç‰¹å¾ï¼šå½©è‰²ã€è‡ªç„¶å…‰ç…§
        prompt = "color fundus photography, retinal image, medical photography"
    else:
        prompt = "medical retinal imaging"
    
    prompts = [prompt] * bs
    inputs = tokenizer(prompts, padding="max_length", max_length=tokenizer.model_max_length, 
                       truncation=True, return_tensors="pt").to(DEVICE)
    return text_encoder(inputs.input_ids)[0]


def get_modality_prompt_embeds(bs, tokenizer, text_encoder, modality: str):
    """
    ã€v23 æ–°å¢ã€‘æ ¹æ®æ¨¡æ€ï¼ˆcf / fa / octï¼‰æ˜¾å¼ç”Ÿæˆ prompt embeddingã€‚
    è¯¥å‡½æ•°ç”¨äº Shared Self-Attention è”åˆ CF-FA è®­ç»ƒåœºæ™¯ï¼Œ
    é¿å…ä¾èµ–åŸæœ‰ "cf2fa" è¿™ç±» mode å­—ç¬¦ä¸²çš„è§£æé€»è¾‘ï¼Œä»è€Œä¿æŒå‰å‘å…¼å®¹ã€‚
    """
    modality = modality.lower()
    if "fa" in modality:
        prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    elif "oct" in modality:
        prompt = "optical coherence tomography, retinal cross section, medical scan, grayscale"
    elif "cf" in modality:
        prompt = "color fundus photography, retinal image, medical photography"
    else:
        prompt = "medical retinal imaging"

    prompts = [prompt] * bs
    inputs = tokenizer(
        prompts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    return text_encoder(inputs.input_ids)[0]

def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡"""
    if step < 4000: return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2

# ============ 2. æ ¸å¿ƒæŸå¤±è®¡ç®— ============

def _gaussian_kernel_1d(kernel_size: int, sigma: float, device, dtype):
    """ç”Ÿæˆå½’ä¸€åŒ– 1D é«˜æ–¯å·ç§¯æ ¸"""
    half = kernel_size // 2
    coords = torch.arange(kernel_size, device=device, dtype=dtype) - half
    gauss = torch.exp(-0.5 * (coords / sigma) ** 2)
    return gauss / gauss.sum()


def gaussian_blur_latent(x, kernel_size=7, sigma=1.5):
    """
    å¯¹ (B, C, H, W) çš„ latent tensor åšå¯åˆ†ç¦»é«˜æ–¯æ¨¡ç³Šï¼ˆä¿æŒæ¢¯åº¦å¯é€šè¿‡ï¼‰ã€‚
    ç”¨äºå°† latent åˆ†è§£ä¸ºä½é¢‘ + é«˜é¢‘ä¸¤éƒ¨åˆ†ã€‚
    """
    C = x.shape[1]
    k = _gaussian_kernel_1d(kernel_size, sigma, x.device, x.dtype)
    pad = kernel_size // 2
    # æ°´å¹³æ–¹å‘
    kw = k.view(1, 1, 1, kernel_size).expand(C, 1, 1, kernel_size)
    x = F.conv2d(x, kw, padding=(0, pad), groups=C)
    # å‚ç›´æ–¹å‘
    kh = k.view(1, 1, kernel_size, 1).expand(C, 1, kernel_size, 1)
    x = F.conv2d(x, kh, padding=(pad, 0), groups=C)
    return x


def compute_hf_texture_loss(pred_x0, gt_x0, kernel_size=7, sigma=1.5):
    """
    åœ¨é¢„æµ‹ x0ï¼ˆlatent ç©ºé—´ï¼‰ä¸Šè®¡ç®—é«˜é¢‘çº¹ç† L1 æŸå¤±ã€‚

    æ­¥éª¤ï¼š
    1. å¯¹ pred_x0 å’Œ gt_x0 åˆ†åˆ«åšé«˜æ–¯æ¨¡ç³Šï¼Œå¾—åˆ°ä½é¢‘è¿‘ä¼¼
    2. ç”¨"åŸå›¾ - ä½é¢‘"å¾—åˆ°é«˜é¢‘æ®‹å·®ï¼ˆåŒ…å«çº¹ç†ã€é¢—ç²’ã€ç»†èŠ‚ï¼‰
    3. å¯¹ä¸¤è€…é«˜é¢‘æ®‹å·®åš L1ï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨é«˜é¢‘ç»´åº¦ä¹Ÿå¯¹é½ GT

    å¥½å¤„ï¼š
    - ä¸éœ€è¦é¢å¤– VAE decodeï¼Œç›´æ¥åœ¨ latent ç©ºé—´è®¡ç®—ï¼Œä»£ä»·æä½
    - å¯å¾®åˆ†ï¼Œæ¢¯åº¦å¯ä»¥ç›´æ¥åä¼ ç»™ ControlNet å’Œ UNet LoRA
    - æ˜ç¡®å‘Šè¯‰æ¨¡å‹"çº¹ç†/é¢—ç²’/é«˜é¢‘ä¿¡æ¯ä¸èƒ½è¢«å¹³å‡æ‰"
    """
    pred_blur = gaussian_blur_latent(pred_x0, kernel_size, sigma)
    gt_blur   = gaussian_blur_latent(gt_x0,   kernel_size, sigma)
    pred_hf = pred_x0 - pred_blur
    gt_hf   = gt_x0   - gt_blur
    return F.l1_loss(pred_hf, gt_hf)


def compute_total_loss(noise_pred, noise, noisy_latents, latents,
                       alphas_cumprod, timesteps, hf_lambda=0.5):
    """
    ã€v22 æ”¹è¿›ã€‘MSE å™ªå£°æŸå¤± + é«˜é¢‘çº¹ç†æŸå¤±ï¼ˆlatent x0 ç©ºé—´ï¼‰

    åŸç†ï¼š
    - loss_mseï¼šæ ‡å‡†å™ªå£°é¢„æµ‹ MSEï¼Œçº¦æŸå…¨é¢‘æ®µçš„å…¨å±€é‡å»º
    - loss_hf ï¼šä» noise_pred åæ¨ pred_x0ï¼Œåœ¨ latent ç©ºé—´å¯¹é«˜é¢‘æ®‹å·®åš L1
                è¿™ä¸€é¡¹ä¸“é—¨è¡¥å¿"æœ‰å½¢æ— éª¨"â€”â€”è¿«ä½¿æ¨¡å‹åœ¨é«˜é¢‘ç»†èŠ‚ä¸Šä¹Ÿè¦å¯¹é½ GT

    å‚æ•°ï¼š
    - hf_lambdaï¼šé«˜é¢‘æŸå¤±æƒé‡ï¼Œæ¨è 0.3ï½1.0ï¼Œè¶Šå¤§é«˜é¢‘çº¦æŸè¶Šå¼º
    """
    # ---- æ ‡å‡† MSE ----
    loss_mse = F.mse_loss(noise_pred, noise)

    # ---- ä» noise_pred åæ¨é¢„æµ‹çš„å¹²å‡€ x0ï¼ˆlatent ç©ºé—´ï¼‰----
    # DDPM å‰å‘ï¼šz_t = sqrt(alpha_t)*x0 + sqrt(1-alpha_t)*noise
    # å› æ­¤ï¼šx0 = (z_t - sqrt(1-alpha_t)*noise_pred) / sqrt(alpha_t)
    alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(noisy_latents.device)
    pred_x0 = (noisy_latents - (1.0 - alpha_t).sqrt() * noise_pred) / (alpha_t.sqrt() + 1e-8)
    # åœ¨å¤§ t æ—¶ pred_x0 æ•°å€¼ä¸ç¨³å®šï¼Œæˆªæ–­åˆ°åˆç†èŒƒå›´
    pred_x0 = pred_x0.clamp(-10.0, 10.0)

    # ---- é«˜é¢‘çº¹ç†æŸå¤± ----
    loss_hf = compute_hf_texture_loss(pred_x0, latents)

    total = loss_mse + hf_lambda * loss_hf
    return total, loss_mse.item(), loss_hf.item()

# ============ 3. éªŒè¯ä¸æ—©åœé€»è¾‘ ============
 
VAL_TIMESTEPS = [200, 500, 800]   # å›ºå®šæ—¶é—´æ­¥ï¼šä½/ä¸­/é«˜å™ªå£°å„å–ä¸€ä¸ªä»£è¡¨ç‚¹


def evaluate_shared_self_attn(val_loader, vae, unet, noise_scheduler, tokenizer, text_encoder, args):
    """
    ã€v23 æ–°å¢ã€‘
    Shared Self-Attention è”åˆ CF-FA è®­ç»ƒçš„éªŒè¯é€»è¾‘ã€‚

    æ€è·¯ï¼š
    - å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œå°† CFï¼ˆcond_tileï¼‰ä¸ FAï¼ˆtgtï¼‰åˆ†åˆ«ç¼–ç ä¸º latentï¼›
    - åœ¨è‹¥å¹²å›ºå®šæ—¶é—´æ­¥ VAL_TIMESTEPS ä¸Šï¼Œä½¿ç”¨å…±äº«å™ªå£° & å…±äº« Self-Attention
      é¢„æµ‹å™ªå£°ï¼Œå¹¶è®¡ç®—ä¸çœŸå®å™ªå£°çš„ MSEï¼›
    - è¿”å›æ‰€æœ‰æ ·æœ¬ä¸æ—¶é—´æ­¥ä¸Šçš„å¹³å‡ MSE ä½œä¸ºéªŒè¯æŒ‡æ ‡ã€‚
    """
    if not ('cf' in args.mode and 'fa' in args.mode):
        raise ValueError("evaluate_shared_self_attn ä»…é€‚ç”¨äºåŒæ—¶åŒ…å« 'cf' å’Œ 'fa' çš„æ¨¡å¼ï¼ˆå¦‚ cf2faï¼‰ã€‚")

    if hasattr(unet, "eval"):
        unet.eval()

    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cond_tile, tgt, _, _ = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]

            # VAE ç¼–ç  CF & FA
            latents_cf = vae.encode(cond_tile).latent_dist.sample() * vae.config.scaling_factor
            latents_fa = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor

            sample_losses = []
            for t_val in VAL_TIMESTEPS:
                timesteps_single = torch.full((b,), t_val, device=DEVICE, dtype=torch.long)

                noise_eps = torch.randn_like(latents_cf)
                noisy_cf = noise_scheduler.add_noise(latents_cf, noise_eps, timesteps_single)
                noisy_fa = noise_scheduler.add_noise(latents_fa, noise_eps, timesteps_single)

                latents_pair = torch.cat([latents_cf, latents_fa], dim=0)
                noisy_latents = torch.cat([noisy_cf, noisy_fa], dim=0)
                noise_pair = torch.cat([noise_eps, noise_eps], dim=0)
                timesteps_pair = torch.cat([timesteps_single, timesteps_single], dim=0)

                prompt_cf = get_modality_prompt_embeds(b, tokenizer, text_encoder, "cf")
                prompt_fa = get_modality_prompt_embeds(b, tokenizer, text_encoder, "fa")
                prompt_embeds = torch.cat([prompt_cf, prompt_fa], dim=0)

                if hasattr(unet, "base_model"):
                    noise_pred = unet.base_model(
                        sample=noisy_latents,
                        timestep=timesteps_pair,
                        encoder_hidden_states=prompt_embeds,
                        return_dict=False,
                    )[0]
                else:
                    noise_pred = unet(
                        noisy_latents,
                        timesteps_pair,
                        prompt_embeds,
                    ).sample

                # è¿™é‡Œåªå…³å¿ƒå™ªå£° MSEï¼Œä»¥è·å¾—æ›´ç¨³å®šçš„éªŒè¯æŒ‡æ ‡
                sample_losses.append(F.mse_loss(noise_pred, noise_pair).item())

            val_losses.append(np.mean(sample_losses))

    if hasattr(unet, "train"):
        unet.train()

    torch.cuda.empty_cache()
    return np.mean(val_losses)


def run_with_shared_self_attention(unet, fn, *args, **kwargs):
    """
    åœ¨ä¸å½±å“è®­ç»ƒé˜¶æ®µçš„å‰æä¸‹ï¼Œä¸´æ—¶å¯ç”¨ Shared Self-Attention è¿è¡Œ fnï¼ˆé€šå¸¸ç”¨äºéªŒè¯æˆ–å¯è§†åŒ–ï¼‰ï¼Œ
    ç»“æŸåæ¢å¤åŸå§‹ AttentionProcessorã€‚
    """
    # ç»Ÿä¸€æ‹¿åˆ°åº•å±‚ UNetï¼ˆå…¼å®¹ PeftModelï¼‰
    core_unet = unet.base_model if hasattr(unet, "base_model") else unet

    # æ²¡æœ‰ attn_processors å°±ç›´æ¥æ‰§è¡Œ
    if not hasattr(core_unet, "attn_processors"):
        return fn(*args, **kwargs)

    # å¤‡ä»½å½“å‰çš„ attention processors
    orig_processors = dict(core_unet.attn_processors)

    # å¯ç”¨ Shared Self-Attention
    apply_shared_self_attention(core_unet, enable_shared=True)

    try:
        return fn(*args, **kwargs)
    finally:
        # æ¢å¤åŸå§‹ processors
        core_unet.set_attn_processor(orig_processors)


@torch.no_grad()
def visualize_random_pairs(unet, vae, tokenizer, text_encoder, num_samples: int, out_dir: str, steps: int = 50):
    """
    ã€v23 æ–°å¢ã€‘ä»éšæœºå™ªå£°ç”Ÿæˆè‹¥å¹²ç»„ CF-FA å›¾åƒå¯¹ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ã€‚

    - ä¸ `test_gen_pairs_random.py` é€»è¾‘ç±»ä¼¼ï¼Œä½†è¿™é‡Œä½œä¸ºè®­ç»ƒæœŸ
      çš„è½»é‡å¯è§†åŒ–å·¥å…·ï¼Œæ¯æ¬¡åªç”Ÿæˆå°‘é‡æ ·æœ¬ï¼ˆé»˜è®¤ 5 å¯¹ï¼‰ã€‚
    - ä¸ºäº†ä¸å¹²æ‰°è®­ç»ƒä½¿ç”¨çš„ schedulerï¼Œè¿™é‡Œå•ç‹¬æ„å»ºä¸€ä¸ªæ–°çš„ DDPMScheduler å®ä¾‹ã€‚
    """
    os.makedirs(out_dir, exist_ok=True)

    # å¤‡ä»½è®­ç»ƒ/æ¨ç†æ¨¡å¼
    unet_was_train = unet.training if hasattr(unet, "training") else False
    if hasattr(unet, "eval"):
        unet.eval()
    if hasattr(vae, "eval"):
        vae.eval()
    if hasattr(text_encoder, "eval"):
        text_encoder.eval()

    # æ–‡æœ¬ promptï¼šä¸€ä¸ª CFï¼Œä¸€ä¸ª FA
    prompt_cf = get_modality_prompt_embeds(1, tokenizer, text_encoder, "cf")
    prompt_fa = get_modality_prompt_embeds(1, tokenizer, text_encoder, "fa")
    prompt_embeds = torch.cat([prompt_cf, prompt_fa], dim=0)  # [2, 77, 768]

    # é‡‡æ ·æ—¶é—´æ­¥ï¼ˆå•ç‹¬çš„ schedulerï¼‰
    from diffusers import DDPMScheduler as _DDPMScheduler  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–

    scheduler = _DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    scheduler.set_timesteps(steps)

    # latent å°ºå¯¸ï¼šSD15 é»˜è®¤ä¸º 4 Ã— (SIZE/8) Ã— (SIZE/8)
    in_channels = (
        unet.base_model.config.in_channels
        if hasattr(unet, "base_model")
        else unet.config.in_channels
    )
    latent_shape = (1, in_channels, SIZE // 8, SIZE // 8)

    def tensor_to_pil(x: torch.Tensor) -> Image.Image:
        x = (x.clamp(-1, 1) + 1) / 2.0  # [0,1]
        x = x.cpu().permute(1, 2, 0).numpy()
        x = (x * 255).round().astype("uint8")
        return Image.fromarray(x)

    for idx in range(num_samples):
        # åˆå§‹å™ªå£°ï¼ˆCF ä¸ FA å…±äº«ï¼‰
        noise_eps = torch.randn(latent_shape, device=DEVICE)

        # CF / FA ä¸¤æ¡è½¨è¿¹å…±ç”¨åŒä¸€ z_T
        latents_cf = noise_eps.clone()
        latents_fa = noise_eps.clone()

        # æ‹¼æ¥æˆè”åˆ batch
        latents = torch.cat([latents_cf, latents_fa], dim=0)

        for t in scheduler.timesteps:
            # UNet é¢„æµ‹å™ªå£°
            if hasattr(unet, "base_model"):
                noise_pred = unet.base_model(
                    sample=latents,
                    timestep=t,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False,
                )[0]
            else:
                noise_pred = unet(
                    latents,
                    t,
                    prompt_embeds,
                ).sample

            # å•æ­¥åå‘æ›´æ–°
            latents = scheduler.step(noise_pred, t, latents).prev_sample

        # æœ€ç»ˆ latent â†’ å›¾åƒ
        latents_cf_final, latents_fa_final = latents.chunk(2, dim=0)

        # è¿˜åŸç¼©æ”¾
        latents_cf_final = latents_cf_final / vae.config.scaling_factor
        latents_fa_final = latents_fa_final / vae.config.scaling_factor

        imgs_cf = vae.decode(latents_cf_final).sample
        imgs_fa = vae.decode(latents_fa_final).sample

        img_cf = tensor_to_pil(imgs_cf[0])
        img_fa = tensor_to_pil(imgs_fa[0])

        # æ¯ä¸€å¯¹å›¾åƒå•ç‹¬ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒåŒ…å« cf.png / fa.png / grid.png
        pair_dir = os.path.join(out_dir, f"pair_{idx:02d}")
        os.makedirs(pair_dir, exist_ok=True)

        cf_path = os.path.join(pair_dir, "cf.png")
        fa_path = os.path.join(pair_dir, "fa.png")
        grid_path = os.path.join(pair_dir, "grid.png")

        img_cf.save(cf_path)
        img_fa.save(fa_path)

        # ç®€å•çš„ 1x2 æ¨ªå‘æ‹¼æ¥ Gridï¼šå·¦ CFï¼Œå³ FA
        w, h = img_cf.size
        grid_img = Image.new("RGB", (w * 2, h))
        grid_img.paste(img_cf, (0, 0))
        grid_img.paste(img_fa, (w, 0))
        grid_img.save(grid_path)

    # æ¢å¤è®­ç»ƒæ¨¡å¼
    if hasattr(unet, "train") and unet_was_train:
        unet.train()

def visualize_inference(*args, **kwargs):
    """
    å ä½å‡½æ•°ï¼šv23 ç‰ˆæœ¬çš„ CFFA è¯¾é¢˜è®­ç»ƒä¸å†åœ¨ train è„šæœ¬ä¸­åš ControlNet å¯è§†åŒ–ã€‚
    å¦‚éœ€å¯è§†åŒ–ï¼Œè¯·åœ¨å•ç‹¬çš„æµ‹è¯•è„šæœ¬ä¸­å®ç°ã€‚
    """
    return

# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============

def main():
    parser = argparse.ArgumentParser()
    # æœ¬è¯¾é¢˜ä»…å…³æ³¨ CFFAï¼ˆCF-FA é…å‡†å¯¹ï¼‰ï¼Œæ¨¡å¼å›ºå®šä¸º cf2fa
    parser.add_argument("--mode", choices=["cf2fa"], default="cf2fa")
    parser.add_argument("-n", "--name", default="exp_v21")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--scribble_scale", type=float, default=0.8)
    parser.add_argument("--tile_scale", type=float, default=1.0)
    # ã€v21ç§»é™¤ã€‘æ‰€æœ‰åƒç´ çº§æŸå¤±çš„ lambda å‚æ•°éƒ½ç§»é™¤äº†
    # ã€v21æ–°å¢ã€‘UNet LoRA ç›¸å…³å‚æ•°
    parser.add_argument("--unet_lora_rank", type=int, default=16, help="UNet LoRA rank")
    parser.add_argument("--unet_lora_alpha", type=int, default=16, help="UNet LoRA alpha")
    parser.add_argument("--offset_noise_strength", type=float, default=0.1, help="Offset noise strength for better contrast")
    parser.add_argument("--hf_lambda", type=float, default=0.5, help="é«˜é¢‘çº¹ç†æŸå¤±æƒé‡ï¼Œæ¨è 0.3~1.0")
    args = parser.parse_args()

    # æ˜¯å¦ä¸º CF-FA æˆå¯¹æ¨¡æ€ï¼ˆä¾‹å¦‚ cf2fa / fa2cfï¼‰ï¼Œæ­¤æ—¶å¯ç”¨ Shared Self-Attention è”åˆè®­ç»ƒã€‚
    is_cf_fa_mode = ('cf' in args.mode and 'fa' in args.mode)

    out_dir = os.path.join(OUT_ROOT, args.mode, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½ï¼ˆæœ¬è„šæœ¬ä»…ç”¨äº CFFA é…å‡†å¯¹ï¼‰
    train_ds = CFFADataset_v2(split='train', mode=args.mode)
    val_ds = CFFADataset_v2(split='test', mode=args.mode)
    
    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=False, num_workers=2)

    # 2. æ¨¡å‹åŠ è½½
    print("\n========== æ¨¡å‹åŠ è½½ ==========")
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    
    # å†»ç»“ VAE å’Œ Text Encoder
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    
    # ============ ã€v21 æ ¸å¿ƒã€‘UNet LoRA é…ç½® ============
    print(f"\n========== UNet LoRA é…ç½® ==========")
    # å…ˆå†»ç»“ UNet åŸå§‹æƒé‡
    unet.requires_grad_(False)
    
    # ä½¿ç”¨ peft åº“åˆ›å»º LoRA é€‚é…å™¨
    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )
    
    # å°† LoRA åº”ç”¨åˆ° UNet
    unet = get_peft_model(unet, lora_config)
    
    # ç»Ÿè®¡å‚æ•°
    unet_lora_params = [p for p in unet.parameters() if p.requires_grad]
    unet_lora_num = sum(p.numel() for p in unet_lora_params)
    unet_total_num = sum(p.numel() for p in unet.parameters())
    
    print(f"âœ“ UNet LoRA å·²åº”ç”¨")
    print(f"  - Rank: {args.unet_lora_rank}, Alpha: {args.unet_lora_alpha}")
    print(f"  - ç›®æ ‡æ¨¡å—: {target_modules}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°: {unet_lora_num:,} ({unet_lora_num/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°: {unet_total_num:,} ({unet_total_num/1e6:.2f}M)")
    print(f"  - å‚æ•°å æ¯”: {unet_lora_num/unet_total_num*100:.2f}%")
    
    total_trainable = unet_lora_num
    print(f"\nâœ“ æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,} ({total_trainable/1e6:.2f}M)")
    
    # ä¼˜åŒ–å™¨é…ç½®
    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    # ä»…è®­ç»ƒ UNet LoRAï¼Œç”¨äºè”åˆ CF-FA ç”Ÿæˆ
    all_trainable_params = unet_lora_params
    optimizer = torch.optim.AdamW(all_trainable_params, lr=5e-5, weight_decay=1e-2)
    
    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=5e-5, weight_decay=1e-2)")
    print(f"  - Offset Noise å¼ºåº¦: {args.offset_noise_strength}")

    # 3. è®­ç»ƒçŠ¶æ€å˜é‡
    global_step = 0
    best_val_loss = float('inf')
    start_time = time.time()

    # æ¯ä¸ªå…ƒç´ ä¸º (total, mse, hf) ä¸‰å…ƒç»„
    loss_accumulator = []

    print(f"\n========== å¼€å§‹è®­ç»ƒ ==========")
    print(f"æ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_ds)} (å…¨é‡ï¼Œå›ºå®šæ—¶é—´æ­¥ {VAL_TIMESTEPS})")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}\n")
    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps:
                break

            # CFFAï¼šcond_tile è§†ä¸º CFï¼Œtgt è§†ä¸º FA
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]

            # VAE åˆ†åˆ«ç¼–ç  CFï¼ˆcond_tileï¼‰ä¸ FAï¼ˆtgtï¼‰
            latents_cf = vae.encode(cond_tile).latent_dist.sample() * vae.config.scaling_factor
            latents_fa = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor

            # å…±äº«å™ªå£°ï¼šåŒä¸€ epsilon ä½œç”¨åœ¨ CF ä¸ FA ä¸Š
            noise_eps = torch.randn_like(latents_cf)
            if args.offset_noise_strength > 0:
                noise_eps = noise_eps + args.offset_noise_strength * torch.randn(
                    latents_cf.shape[0], latents_cf.shape[1], 1, 1, device=latents_cf.device
                )

            timesteps_single = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE
            ).long()

            noisy_cf = noise_scheduler.add_noise(latents_cf, noise_eps, timesteps_single)
            noisy_fa = noise_scheduler.add_noise(latents_fa, noise_eps, timesteps_single)

            # æ‹¼æ¥æˆè”åˆ batchï¼š[CF, FA]
            latents_pair = torch.cat([latents_cf, latents_fa], dim=0)
            noisy_latents = torch.cat([noisy_cf, noisy_fa], dim=0)
            noise_pair = torch.cat([noise_eps, noise_eps], dim=0)
            timesteps_pair = torch.cat([timesteps_single, timesteps_single], dim=0)

            # å„æ¨¡æ€ç‹¬ç«‹çš„æ–‡æœ¬ promptï¼Œå†æ‹¼æ¥
            prompt_cf = get_modality_prompt_embeds(b, tokenizer, text_encoder, "cf")
            prompt_fa = get_modality_prompt_embeds(b, tokenizer, text_encoder, "fa")
            prompt_embeds = torch.cat([prompt_cf, prompt_fa], dim=0)

            # UNet å‰å‘ï¼ˆå†…éƒ¨å·²é€šè¿‡ Shared Self-Attention å…±äº«ç»“æ„ï¼‰
            if hasattr(unet, "base_model"):
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps_pair,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False,
                )[0]
            else:
                noise_pred = unet(
                    noisy_latents,
                    timesteps_pair,
                    prompt_embeds,
                ).sample

            # åœ¨è”åˆ latent ä¸Šè®¡ç®—å™ªå£° + é«˜é¢‘æŸå¤±
            loss, loss_mse_val, loss_hf_val = compute_total_loss(
                noise_pred,
                noise_pair,
                noisy_latents,
                latents_pair,
                noise_scheduler.alphas_cumprod,
                timesteps_pair,
                hf_lambda=args.hf_lambda,
            )

            # åå‘ä¼ æ’­ï¼ˆä¸¤ç§æ¨¡å¼å…±äº«ï¼‰
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # åŠ¨æ€å­¦ä¹ ç‡æ›´æ–°
            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups: param_group['lr'] = current_lr
            
            # ç»Ÿè®¡
            loss_accumulator.append((loss.item(), loss_mse_val, loss_hf_val))
            
            # æ—¥å¿—æ‰“å°
            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                arr = np.array(loss_accumulator)
                avg_loss, avg_mse, avg_hf = arr[:, 0].mean(), arr[:, 1].mean(), arr[:, 2].mean()
                loss_accumulator = []
                
                t_val = timesteps_single[0].item()
                extra = "SSA(cf+fa)"
                
                msg = (f"[v23] Step {global_step:5d}/{args.max_steps} | "
                       f"lr:{current_lr:.2e} | loss:{avg_loss:.4f} "
                       f"(mse:{avg_mse:.4f} hf:{avg_hf:.4f}) | t={t_val:3d} | "
                       f"{extra} | {elapsed:.1f}s")
                print(msg)
                
                # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")
                
                start_time = time.time()
            
            # æ¯ 500 æ­¥éªŒè¯ï¼ˆå¯è¿‘ä¼¼è§†ä¸ºâ€œæ¯è‹¥å¹² epochâ€ï¼‰
            if global_step % 500 == 0:
                # éªŒè¯é˜¶æ®µï¼šä¸´æ—¶å¯ç”¨ Shared Self-Attentionï¼Œä»…åŸºäºè”åˆå™ªå£° MSE
                val_loss = run_with_shared_self_attention(
                    unet,
                    evaluate_shared_self_attn,
                    val_loader, vae, unet, noise_scheduler, tokenizer, text_encoder, args
                )
                
                # è®°å½•éªŒè¯æ—¥å¿—
                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")

                # ===== æŒ‰ step å‘½åçš„éšæœºå™ªå£°å¯è§†åŒ–ï¼ˆä¸ v22 é£æ ¼ä¿æŒä¸€è‡´ï¼‰ =====
                # ä¾‹å¦‚ global_step=500 æ—¶ï¼Œå°†ç»“æœä¿å­˜åˆ°:
                #   out_dir/step_000500_random_pairs/
                step_vis_dir = os.path.join(out_dir, f"step_{global_step:06d}_random_pairs")
                print(f"[å¯è§†åŒ–] åœ¨ {step_vis_dir} ç”Ÿæˆ {10} ç»„éšæœº CF-FA å›¾åƒå¯¹...")
                # å¯è§†åŒ–æ—¶åŒæ ·ä¸´æ—¶å¯ç”¨ Shared Self-Attention
                run_with_shared_self_attention(
                    unet,
                    visualize_random_pairs,
                    unet, vae, tokenizer, text_encoder,
                    10, step_vis_dir, 50,
                )
                
                # ä¿å­˜æœ€æ–°æƒé‡
                latest_dir = os.path.join(out_dir, "latest_checkpoint")
                os.makedirs(latest_dir, exist_ok=True)
                # ä»…ä¿å­˜ UNet LoRA æƒé‡
                unet_lora_dir = os.path.join(latest_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)
                
                # ä¿å­˜æœ€æ–°å…ƒä¿¡æ¯
                with open(os.path.join(latest_dir, "latest_info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Latest Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"Best Loss: {best_val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)
                    # ä¿å­˜æœ€ä½³ UNet LoRA æƒé‡
                    unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(unet_lora_dir)
                    
                    # ä¿å­˜æœ€ä½³å…ƒä¿¡æ¯
                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                    
                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)

                    # åœ¨åˆ·æ–°æœ€ä½³ checkpoint çš„ step ä¸Šï¼Œä¸å†é‡å¤ç”Ÿæˆæˆ–å¤åˆ¶ä¸¤ä»½å¯è§†åŒ–ï¼Œ
                    # è€Œæ˜¯ç›´æ¥å°†å½“å‰ step çš„å¯è§†åŒ–ç›®å½•é‡å‘½åä¸º *_bestï¼Œåªä¿ç•™ä¸€ä»½æ•°æ®ã€‚
                    best_step_vis_dir = os.path.join(out_dir, f"step_{global_step:06d}_random_pairs_best")
                    if os.path.isdir(best_step_vis_dir):
                        shutil.rmtree(best_step_vis_dir)
                    if os.path.isdir(step_vis_dir):
                        os.rename(step_vis_dir, best_step_vis_dir)

            global_step += 1

if __name__ == "__main__":
    main()