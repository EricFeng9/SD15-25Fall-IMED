# -*- coding: utf-8 -*-
"""
SDXL ControlNet è®­ç»ƒè„šæœ¬ v21
åŸºäº v18 æ”¹è¿›ï¼Œä¸“æ³¨è§£å†³"ç»“æ„å¥½ä½†çº¹ç†/äº®åº¦ä¸çœŸå®"é—®é¢˜

ã€æ ¸å¿ƒå˜åŠ¨ - é’ˆå¯¹è§†è§‰å›¾çµæµ‹è¯•ã€‘
1. âœ… UNet LoRA è®­ç»ƒï¼šè®© UNet å­¦ä¹ åŒ»å­¦å›¾åƒçš„çº¹ç†å’Œäº®åº¦åˆ†å¸ƒï¼ˆv18 ä¸­ UNet è¢«å†»ç»“ï¼‰
2. âœ… ç§»é™¤æ‰€æœ‰åƒç´ çº§æŸå¤±ï¼šåªä¿ç•™çº¯ç²¹çš„å™ªå£°é¢„æµ‹ MSEï¼ˆç§»é™¤ SSIM/Vessel/Gradient/Texture Lossï¼‰
3. âœ… åŒ»å­¦å›¾åƒ Promptï¼šä½¿ç”¨é¢†åŸŸç‰¹å®šçš„ prompt è€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
4. âœ… Offset Noiseï¼šè§£å†³äº®åº¦åäº®ã€å¯¹æ¯”åº¦ä¸è¶³çš„é—®é¢˜
5. âœ… åŒæ—¶è®­ç»ƒ ControlNet + UNet LoRAï¼Œå„å¸å…¶èŒï¼ˆç»“æ„ vs çº¹ç†ï¼‰
"""

import os
import math
import time
import random
import argparse
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import transforms
from diffusers import (DDPMScheduler, ControlNetModel, AutoencoderKL, UNet2DConditionModel, 
                       StableDiffusionControlNetPipeline, MultiControlNetModel)
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType
#import bitsandbytes as bnb

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
# å°†æ•°æ®ç›®å½•åŠ å…¥è·¯å¾„ä»¥ä¾¿å¯¼å…¥ dataset
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cffa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CFFA_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cfoct_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_octfa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CF_OCTA_v2_repaired"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2
from operation_pre_filtered_cfoct_augmented_dataset import CFOCTDataset
from operation_pre_filtered_octfa_augmented_dataset import OCTFADataset
from cf_octa_v2_repaired_dataset import CFOCTADataset
from vessle_detector import extract_vessel_map

# ============ å…¨å±€é…ç½® ============
SIZE = 512
DEVICE = torch.device("cuda")
# æ¨¡å‹è·¯å¾„
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
SCRIBBLE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-scribble"
TILE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-tile"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_ctrl_sd15_dual"

# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_prompt_embeds(bs, tokenizer, text_encoder, mode="cf2fa"):
    """
    ç”ŸæˆåŒ»å­¦å›¾åƒé¢†åŸŸç‰¹å®šçš„æç¤ºè¯åµŒå…¥
    
    ã€v21 æ”¹è¿›ã€‘ä¸å†ä½¿ç”¨ç©º promptï¼Œè€Œæ˜¯ä½¿ç”¨é¢†åŸŸç‰¹å®šæè¿°
    è¿™æœ‰åŠ©äºæ¿€æ´»æ¨¡å‹ä¸­ä¸åŒ»å­¦å½±åƒç›¸å…³çš„æ½œåœ¨è¯­ä¹‰åˆ†å¸ƒ
    """
    if 'fa' in mode:
        # FA (è§å…‰è¡€ç®¡é€ å½±) çš„ç‰¹å¾ï¼šé«˜å¯¹æ¯”åº¦ã€é»‘èƒŒæ™¯ã€äº®è¡€ç®¡ã€é¢—ç²’å™ªå£°
        prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    elif 'oct' in mode:
        # OCT çš„ç‰¹å¾ï¼šå±‚çŠ¶ç»“æ„ã€ç°åº¦å›¾
        prompt = "optical coherence tomography, retinal cross section, medical scan, grayscale"
    elif 'cf' in mode:
        # CF (å½©è‰²çœ¼åº•) çš„ç‰¹å¾ï¼šå½©è‰²ã€è‡ªç„¶å…‰ç…§
        prompt = "color fundus photography, retinal image, medical photography"
    else:
        prompt = "medical retinal imaging"
    
    prompts = [prompt] * bs
    inputs = tokenizer(prompts, padding="max_length", max_length=tokenizer.model_max_length, 
                       truncation=True, return_tensors="pt").to(DEVICE)
    return text_encoder(inputs.input_ids)[0]

def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡"""
    if step < 4000: return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2

# ============ 2. æ ¸å¿ƒæŸå¤±è®¡ç®— ============

def compute_total_loss(noise_pred, noise):
    """
    ã€v21 æ ¸å¿ƒæ”¹è¿›ã€‘çº¯ç²¹çš„å™ªå£°é¢„æµ‹æŸå¤±
    
    ç§»é™¤æ‰€æœ‰åƒç´ çº§çº¦æŸï¼ˆSSIM/Vessel/Gradient/Textureï¼‰ï¼Œè®©æ¨¡å‹è‡ªç”±å­¦ä¹ çº¹ç†
    åŸå› ï¼š
    1. Diffusion æ¨¡å‹çš„æœ¬è´¨æ˜¯æ¦‚ç‡ç”Ÿæˆï¼Œåº”è¯¥é€šè¿‡å™ªå£°åˆ†å¸ƒå­¦ä¹ ï¼Œè€Œéåƒç´ å›å½’
    2. åƒç´ çº§ L1/L2 ä¼šå¯¼è‡´"å›å½’å‡å€¼"æ•ˆåº”ï¼Œç”Ÿæˆè¿‡äºå¹³æ»‘çš„ç»“æœ
    3. UNet LoRA ä¼šå­¦ä¹ åˆ°æ­£ç¡®çš„çº¹ç†åˆ†å¸ƒï¼Œä¸éœ€è¦æ˜¾å¼çº¦æŸ
    """
    return F.mse_loss(noise_pred, noise)

# ============ 3. éªŒè¯ä¸æ—©åœé€»è¾‘ ============

def evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args):
    """ã€v21ç®€åŒ–ã€‘åœ¨å›ºå®šéªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼šåªä½¿ç”¨ MSE loss"""
    cn_s.eval(); cn_t.eval()
    # å¦‚æœ unet æ˜¯ PEFT åŒ…è£…çš„ï¼Œä¹Ÿè¦è®¾ç½®ä¸º evalï¼ˆè™½ç„¶å·²ç»å†»ç»“ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
    if hasattr(unet, 'eval'):
        unet.eval()
    
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cond_tile, tgt, _, _ = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            source_type, _ = args.mode.split('2')
            cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
            vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # VAE ç¼–ç 
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder, args.mode)
            
            # ControlNet æ¨ç†
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            # UNet é¢„æµ‹ï¼ˆå¦‚æœæ˜¯ PEFT åŒ…è£…çš„ï¼Œä½¿ç”¨ base_modelï¼‰
            if hasattr(unet, 'base_model'):
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                    mid_block_additional_residual=mid_s+mid_t,
                    return_dict=False
                )[0]
            else:
                noise_pred = unet(
                    noisy_latents, timesteps, prompt_embeds,
                    down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                    mid_block_additional_residual=mid_s+mid_t
                ).sample
            
            # ä½¿ç”¨ç®€åŒ–çš„ MSE æŸå¤±
            loss = compute_total_loss(noise_pred, noise)
            val_losses.append(loss.item())
            
    cn_s.train(); cn_t.train()
    if hasattr(unet, 'train'):
        unet.train()
    torch.cuda.empty_cache()
    return np.mean(val_losses)

def visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, step, out_dir):
    """ã€v21ä¼˜åŒ–ã€‘è¿è¡Œæ¨ç†å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ"""
    print(f"\n[å¯è§†åŒ–] æ­£åœ¨è¿è¡Œæ¨ç†å¯è§†åŒ– (Step {step})...")
    
    # åˆ›å»ºæ¨ç†æµ‹è¯•ç›®å½•
    infer_dir = os.path.join(out_dir, f"step_{step}_inference")
    os.makedirs(infer_dir, exist_ok=True)
    
    # ä¸´æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼
    cn_s.eval(); cn_t.eval()
    
    # ç¡®å®šä½¿ç”¨çš„ prompt
    if 'fa' in args.mode:
        prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    elif 'oct' in args.mode:
        prompt = "optical coherence tomography, retinal cross section, medical scan, grayscale"
    elif 'cf' in args.mode:
        prompt = "color fundus photography, retinal image, medical photography"
    else:
        prompt = "medical retinal imaging"
    
    # æ„å»º pipelineï¼ˆå¦‚æœ unet æ˜¯ PEFT åŒ…è£…çš„ï¼Œä½¿ç”¨ base_modelï¼‰
    multi_controlnet = MultiControlNetModel([cn_s, cn_t])
    unet_for_pipe = unet.base_model if hasattr(unet, 'base_model') else unet
    pipe = StableDiffusionControlNetPipeline(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        unet=unet_for_pipe,
        controlnet=multi_controlnet,
        scheduler=noise_scheduler,
        safety_checker=None,
        feature_extractor=None
    ).to(DEVICE)
    pipe.set_progress_bar_config(disable=True)
    
    # åªå–å‰ 2 ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    with torch.no_grad():
        for i, batch in enumerate(val_loader):
            if i >= 2: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            source_type, _ = args.mode.split('2')
            cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
            vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # æ¨ç†
            generator = torch.Generator(device=DEVICE).manual_seed(42)
            h, w = cond_tile.shape[2], cond_tile.shape[3]
            
            output_img = pipe(
                prompt=prompt,  # ã€v21æ”¹è¿›ã€‘ä½¿ç”¨åŒ»å­¦å›¾åƒ prompt
                image=[cond_scribble, cond_tile],
                num_inference_steps=25,
                controlnet_conditioning_scale=[args.scribble_scale, args.tile_scale],
                generator=generator,
                width=w,
                height=h
            ).images[0]
            
            # ä¿å­˜ç»“æœ
            try:
                name = os.path.splitext(os.path.basename(cp[0]))[0]
            except:
                name = f"sample_{i}"
                
            # ä¿å­˜è¾“å…¥å’Œç›®æ ‡
            cond_scribble_save = (cond_scribble[0].cpu().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
            cond_tile_save = ((cond_tile[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            tgt_save = ((tgt[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            
            Image.fromarray(cond_scribble_save).save(os.path.join(infer_dir, f"{name}_01_scribble.png"))
            Image.fromarray(cond_tile_save).save(os.path.join(infer_dir, f"{name}_02_tile.png"))
            Image.fromarray(tgt_save).save(os.path.join(infer_dir, f"{name}_03_target.png"))
            output_img.save(os.path.join(infer_dir, f"{name}_04_pred.png"))

    # æ¢å¤è®­ç»ƒæ¨¡å¼
    cn_s.train(); cn_t.train()
    
    # æ˜¾å¼æ¸…ç†æ˜¾å­˜ (é˜²æ­¢ OOM)
    del pipe
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ“ æ¨ç†å¯è§†åŒ–å·²ä¿å­˜åˆ°: {infer_dir}\n")

# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["cf2fa", "fa2cf", "cf2oct", "oct2cf", "fa2oct", "oct2fa", "cf2octa", "octa2cf"], required=True)
    parser.add_argument("-n", "--name", default="exp_v21")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--scribble_scale", type=float, default=0.8)
    parser.add_argument("--tile_scale", type=float, default=1.0)
    # ã€v21ç§»é™¤ã€‘æ‰€æœ‰åƒç´ çº§æŸå¤±çš„ lambda å‚æ•°éƒ½ç§»é™¤äº†
    # ã€v21æ–°å¢ã€‘UNet LoRA ç›¸å…³å‚æ•°
    parser.add_argument("--unet_lora_rank", type=int, default=16, help="UNet LoRA rank")
    parser.add_argument("--unet_lora_alpha", type=int, default=16, help="UNet LoRA alpha")
    parser.add_argument("--offset_noise_strength", type=float, default=0.1, help="Offset noise strength for better contrast")
    parser.add_argument("--patience", type=int, default=8)
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.mode, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½
    if 'octa' in args.mode:
        train_ds = CFOCTADataset(split='train', mode=args.mode)
        val_ds = CFOCTADataset(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'fa' in args.mode:
        # ä»…ä½¿ç”¨ operation_pre_filtered_cffa_augmented ç‰ˆæœ¬çš„æ•°æ®é›†
        train_ds = CFFADataset_v2(split='train', mode=args.mode)
        val_ds = CFFADataset_v2(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'oct' in args.mode:
        train_ds = CFOCTDataset(split='train', mode=args.mode)
        val_ds = CFOCTDataset(split='test', mode=args.mode)
    elif 'fa' in args.mode and 'oct' in args.mode:
        train_ds = OCTFADataset(split='train', mode=args.mode)
        val_ds = OCTFADataset(split='test', mode=args.mode)
    else:
        raise ValueError(f"Unknown mode: {args.mode}")
    
    # éªŒè¯é›†ä½¿ç”¨å›ºå®šå­é›†ï¼ˆ10ä¸ªæ ·æœ¬ï¼‰æé«˜æ•ˆç‡
    val_indices = random.sample(range(len(val_ds)), min(10, len(val_ds)))
    val_subset = torch.utils.data.Subset(val_ds, val_indices)
    
    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)

    # 2. æ¨¡å‹åŠ è½½
    print("\n========== æ¨¡å‹åŠ è½½ ==========")
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    cn_s = ControlNetModel.from_pretrained(SCRIBBLE_CN_DIR).to(DEVICE)
    cn_t = ControlNetModel.from_pretrained(TILE_CN_DIR).to(DEVICE)
    
    # å†»ç»“ VAE å’Œ Text Encoder
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    
    # ============ ã€v21 æ ¸å¿ƒã€‘UNet LoRA é…ç½® ============
    print(f"\n========== UNet LoRA é…ç½® ==========")
    # å…ˆå†»ç»“ UNet åŸå§‹æƒé‡
    unet.requires_grad_(False)
    
    # ä½¿ç”¨ peft åº“åˆ›å»º LoRA é€‚é…å™¨
    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )
    
    # å°† LoRA åº”ç”¨åˆ° UNet
    unet = get_peft_model(unet, lora_config)
    
    # ç»Ÿè®¡å‚æ•°
    unet_lora_params = [p for p in unet.parameters() if p.requires_grad]
    unet_lora_num = sum(p.numel() for p in unet_lora_params)
    unet_total_num = sum(p.numel() for p in unet.parameters())
    
    print(f"âœ“ UNet LoRA å·²åº”ç”¨")
    print(f"  - Rank: {args.unet_lora_rank}, Alpha: {args.unet_lora_alpha}")
    print(f"  - ç›®æ ‡æ¨¡å—: {target_modules}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°: {unet_lora_num:,} ({unet_lora_num/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°: {unet_total_num:,} ({unet_total_num/1e6:.2f}M)")
    print(f"  - å‚æ•°å æ¯”: {unet_lora_num/unet_total_num*100:.2f}%")
    
    # ControlNet å‚æ•°ç»Ÿè®¡
    cn_s_num = sum(p.numel() for p in cn_s.parameters() if p.requires_grad)
    cn_t_num = sum(p.numel() for p in cn_t.parameters() if p.requires_grad)
    
    print(f"\nâœ“ ControlNet (åŒæ—¶è®­ç»ƒ)")
    print(f"  - Scribble: {cn_s_num:,} ({cn_s_num/1e6:.2f}M)")
    print(f"  - Tile: {cn_t_num:,} ({cn_t_num/1e6:.2f}M)")
    
    total_trainable = unet_lora_num + cn_s_num + cn_t_num
    print(f"\nâœ“ æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,} ({total_trainable/1e6:.2f}M)")
    
    # ä¼˜åŒ–å™¨é…ç½®
    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    all_trainable_params = list(cn_s.parameters()) + list(cn_t.parameters()) + unet_lora_params
    optimizer = torch.optim.AdamW(all_trainable_params, lr=5e-5, weight_decay=1e-2)
    
    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=5e-5, weight_decay=1e-2)")
    print(f"  - Offset Noise å¼ºåº¦: {args.offset_noise_strength}")
    print(f"  - æ—©åœ Patience: {args.patience}")

    # 3. è®­ç»ƒçŠ¶æ€å˜é‡
    global_step = 0
    best_val_loss = float('inf')
    wait = 0
    start_time = time.time()

    # ã€v21ç®€åŒ–ã€‘åªæœ‰ä¸€ä¸ªlossç´¯åŠ å™¨
    loss_accumulator = []

    print(f"\n========== å¼€å§‹è®­ç»ƒ ==========")
    print(f"æ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_subset)}")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}\n")
    
    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # å®æ—¶ç”Ÿæˆè¡€ç®¡å›¾ä½œä¸ºæ¡ä»¶è¾“å…¥
            source_type, _ = args.mode.split('2')
            with torch.no_grad():
                cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
                vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
                cond_scribble = vessel_map.repeat(1, 3, 1, 1)

            # Debug: Step 0 å›¾åƒä¿å­˜
            if global_step == 0:
                debug_dir = os.path.join(out_dir, "debug_images_step0")
                os.makedirs(debug_dir, exist_ok=True)
                
                try:
                    name = os.path.splitext(os.path.basename(cp[0]))[0]
                except:
                    name = "step0_sample"

                cond_scribble_save = (cond_scribble[0].cpu().float().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_scribble_save).save(os.path.join(debug_dir, f"{name}_scribble_input.png"))
                
                cond_tile_save = ((cond_tile[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_tile_save).save(os.path.join(debug_dir, f"{name}_tile_input.png"))
                
                tgt_save = ((tgt[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(tgt_save).save(os.path.join(debug_dir, f"{name}_target.png"))
                
                print(f"âœ“ Step 0 è°ƒè¯•å›¾åƒå·²ä¿å­˜åˆ°: {debug_dir}\n")

            # VAE ç¼–ç 
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            
            # ã€v21 æ ¸å¿ƒæ”¹è¿›ã€‘æ·»åŠ  Offset Noise æé«˜å¯¹æ¯”åº¦
            # Offset Noise: åœ¨æ ‡å‡†å™ªå£°åŸºç¡€ä¸Šæ·»åŠ ä¸€ä¸ªå…¨å±€åç§»ï¼Œæœ‰åŠ©äºç”Ÿæˆé«˜å¯¹æ¯”åº¦å›¾åƒ
            noise = torch.randn_like(latents)
            if args.offset_noise_strength > 0:
                noise += args.offset_noise_strength * torch.randn(latents.shape[0], latents.shape[1], 1, 1, device=latents.device)
            
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder, args.mode)
            
            # åŒè·¯ ControlNet å‰å‘
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            # UNet é¢„æµ‹ï¼ˆä½¿ç”¨ PEFT åŒ…è£…çš„æ¨¡å‹ï¼‰
            if hasattr(unet, 'base_model'):
                # PEFT åŒ…è£…çš„æ¨¡å‹ï¼Œä½¿ç”¨ base_model
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                    mid_block_additional_residual=mid_s+mid_t,
                    return_dict=False
                )[0]
            else:
                # æ™®é€šæ¨¡å‹
                noise_pred = unet(
                    noisy_latents, timesteps, prompt_embeds,
                    down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                    mid_block_additional_residual=mid_s+mid_t
                ).sample
            
            # ã€v21ç®€åŒ–ã€‘åªè®¡ç®— MSE æŸå¤±
            loss = compute_total_loss(noise_pred, noise)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # åŠ¨æ€å­¦ä¹ ç‡æ›´æ–°
            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups: param_group['lr'] = current_lr
            
            # ç»Ÿè®¡
            loss_accumulator.append(loss.item())
            
            # æ—¥å¿—æ‰“å°
            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                avg_loss = np.mean(loss_accumulator)
                loss_accumulator = []
                
                t_val = timesteps[0].item()
                
                msg = (f"[v21-LoRA] Step {global_step:5d}/{args.max_steps} | "
                       f"lr:{current_lr:.2e} | loss:{avg_loss:.4f} | t={t_val:3d} | "
                       f"S:{args.scribble_scale} T:{args.tile_scale} | {elapsed:.1f}s")
                print(msg)
                
                # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")
                
                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯ & æ—©åœåˆ¤æ–­
            if global_step % 500 == 0:
                val_loss = evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args)
                
                # è®°å½•éªŒè¯æ—¥å¿—
                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")
                
                # è¿è¡Œæ¨ç†å¯è§†åŒ–
                visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, global_step, out_dir)

                # ä¿å­˜æœ€æ–°æƒé‡
                latest_dir = os.path.join(out_dir, "latest_checkpoint")
                os.makedirs(latest_dir, exist_ok=True)
                cn_s.save_pretrained(os.path.join(latest_dir, "controlnet_scribble"))
                cn_t.save_pretrained(os.path.join(latest_dir, "controlnet_tile"))
                # ä¿å­˜ UNet LoRA æƒé‡
                unet_lora_dir = os.path.join(latest_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)
                
                # ä¿å­˜æœ€æ–°å…ƒä¿¡æ¯
                with open(os.path.join(latest_dir, "latest_info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Latest Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"Best Loss: {best_val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    wait = 0
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)
                    cn_s.save_pretrained(os.path.join(best_dir, "controlnet_scribble"))
                    cn_t.save_pretrained(os.path.join(best_dir, "controlnet_tile"))
                    # ä¿å­˜æœ€ä½³ UNet LoRA æƒé‡
                    unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(unet_lora_dir)
                    
                    # ä¿å­˜æœ€ä½³å…ƒä¿¡æ¯
                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                    
                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)
                else:
                    if global_step >= 4000: # Warm-up åæ‰è§¦å‘ patience
                        wait += 1
                        print(f"âš  éªŒè¯æŸè€—æœªä¸‹é™ ({wait}/{args.patience})\n")
                        if wait >= args.patience:
                            print("ğŸ›‘ è§¦å‘æ—©åœï¼Œè®­ç»ƒç»“æŸã€‚")
                            return

            global_step += 1

if __name__ == "__main__":
    main()