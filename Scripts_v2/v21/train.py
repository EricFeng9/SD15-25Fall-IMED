# -*- coding: utf-8 -*-
"""
SDXL ControlNet è®­ç»ƒè„šæœ¬ v15
åŸºäº v14 é€»è¾‘é‡æ„ï¼Œæ”¯æŒ CF-FA å’Œ CF-OCT æ•°æ®é›†ã€‚

ã€æ ¸å¿ƒå˜åŠ¨ã€‘
1. ç§»é™¤ CSV ä¾èµ–ï¼šç›´æ¥ä»æŒ‡å®šç›®å½•è¯»å–é…å¯¹å›¾åƒã€‚
2. ç§»é™¤ CF-OCTAï¼šä¸“æ³¨äº CF-FA å’Œ CF-OCT ä»»åŠ¡ã€‚
3. åŠ¨æ€è¡€ç®¡æå–ï¼šDataset ä¸å†è¿”å› vessel å›¾ï¼Œç”±è®­ç»ƒå¾ªç¯è°ƒç”¨ vessle_detector å®æ—¶ç”Ÿæˆã€‚
4. ç»§æ‰¿ v14 é€»è¾‘ï¼šä¿ç•™ MSE + MS-SSIM + Vessel Dice + Gradient Match Loss ç»„åˆã€‚
5. å®Œå¤‡çš„è®­ç»ƒç­–ç•¥ï¼šåŒ…æ‹¬æ—©åœæœºåˆ¶ï¼ˆEarly Stoppingï¼‰ã€å­¦ä¹ ç‡è¡°å‡ã€å›ºå®šå­é›†éªŒè¯ã€‚
"""

import os
import csv
import math
import time
import random
import argparse
import gc
import inspect
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import transforms
from diffusers import (
    DDPMScheduler,
    ControlNetModel,
    AutoencoderKL,
    UNet2DConditionModel,
    StableDiffusionControlNetPipeline,
    MultiControlNetModel,
)
from transformers import CLIPTextModel, CLIPTokenizer
from pytorch_msssim import MS_SSIM
from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0, AttnProcessor
from peft import LoraConfig, get_peft_model, TaskType
#import bitsandbytes as bnb

def verify_and_fix_lora_application(unet, target_modules):
    """
    éªŒè¯å¹¶ä¿®å¤ LoRA åº”ç”¨ï¼šç¡®ä¿ç›®æ ‡æ¨¡å—è¢«æ­£ç¡®æ›¿æ¢ä¸º LoRA ç‰ˆæœ¬ã€‚
    
    è¿™ä¸ªå‡½æ•°ç”¨äºæ£€æŸ¥ PEFT åº“æ˜¯å¦æ­£ç¡®åœ°å°† target_modules ä¸­çš„ Linear å±‚
    æ›¿æ¢ä¸º LoraLinear å±‚ã€‚å¦‚æœå‘ç°æŸäº›æ¨¡å—ä»ç„¶æ˜¯ Linear è€Œä¸æ˜¯ LoraLinearï¼Œ
    ä¼šæ‰“å°è­¦å‘Šä¿¡æ¯å¹¶æä¾›è§£å†³å»ºè®®ã€‚
    
    å‚æ•°:
        unet: åº”ç”¨äº† LoRA çš„ UNet æ¨¡å‹
        target_modules: ç›®æ ‡æ¨¡å—åç§°åˆ—è¡¨ï¼Œä¾‹å¦‚ ["to_k", "to_q", "to_v", "to_out.0"]
    
    è¿”å›:
        tuple: (replaced_count, not_replaced_count) - å·²æ›¿æ¢å’Œæœªæ›¿æ¢çš„æ¨¡å—æ•°é‡
    
    æ³¨æ„:
        å¦‚æœå‘ç°æ¨¡å—æœªè¢«æ›¿æ¢ï¼Œå¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼š
        1. PEFT åº“ç‰ˆæœ¬é—®é¢˜ - å°è¯•å‡çº§ peft: pip install --upgrade peft
        2. æ¨¡å—åç§°ä¸å®Œå…¨åŒ¹é… - æ£€æŸ¥ target_modules æ˜¯å¦æ­£ç¡®
        3. æ¨¡å—ç»“æ„ç‰¹æ®Š - æŸäº› diffusers ç‰ˆæœ¬å¯èƒ½éœ€è¦ä¸åŒçš„é…ç½®
        
        å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ diffusers å†…ç½®çš„ LoRA æ”¯æŒï¼š
        from diffusers.models.attention_processor import LoRAAttnProcessor
        unet.set_attn_processor(LoRAAttnProcessor(...))
    """
    import torch.nn as nn
    
    replaced_count = 0
    not_replaced_count = 0
    not_replaced_modules = []
    
    # å°è¯•å¯¼å…¥ LoraLinearï¼ˆä¸åŒç‰ˆæœ¬çš„ peft å¯èƒ½è·¯å¾„ä¸åŒï¼‰
    try:
        from peft.tuners.lora import Linear as LoraLinear
    except ImportError:
        try:
            from peft.tuners.lora.layer import Linear as LoraLinear
        except ImportError:
            # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨ç±»å‹åç§°å­—ç¬¦ä¸²æ£€æŸ¥
            LoraLinear = None
    
    # éå†æ‰€æœ‰æ¨¡å—ï¼ŒæŸ¥æ‰¾ç›®æ ‡æ¨¡å—
    for name, module in unet.named_modules():
        module_name = name.split('.')[-1]  # è·å–æ¨¡å—çš„æœ€åä¸€éƒ¨åˆ†åç§°
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
        if module_name in target_modules:
            # æ£€æŸ¥æ¨¡å—ç±»å‹
            if LoraLinear is not None:
                if isinstance(module, LoraLinear):
                    replaced_count += 1
                elif isinstance(module, nn.Linear):
                    not_replaced_count += 1
                    not_replaced_modules.append(name)
            else:
                # ä½¿ç”¨ç±»å‹åç§°å­—ç¬¦ä¸²æ£€æŸ¥
                module_type_name = type(module).__name__
                if 'Lora' in module_type_name or 'lora' in module_type_name.lower():
                    replaced_count += 1
                elif isinstance(module, nn.Linear):
                    not_replaced_count += 1
                    not_replaced_modules.append(name)
    
    print(f"\nã€LoRA åº”ç”¨éªŒè¯ã€‘")
    print(f"  - å·²æ›¿æ¢ä¸º LoRA çš„æ¨¡å—æ•°: {replaced_count}")
    print(f"  - ä»ä¸º Linear çš„æ¨¡å—æ•°: {not_replaced_count}")
    
    if not_replaced_count > 0:
        print(f"  âš ï¸  è­¦å‘Šï¼šä»¥ä¸‹æ¨¡å—æœªè¢«æ›¿æ¢ä¸º LoRA:")
        for mod_name in not_replaced_modules[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"    - {mod_name}")
        if len(not_replaced_modules) > 10:
            print(f"    ... è¿˜æœ‰ {len(not_replaced_modules) - 10} ä¸ªæ¨¡å—")
        
        print(f"\n  æç¤ºï¼šå¦‚æœæ¨¡å—æœªè¢«æ›¿æ¢ï¼Œå¯èƒ½æ˜¯å› ä¸º:")
        print(f"    1. æ¨¡å—åç§°ä¸å®Œå…¨åŒ¹é… target_modules")
        print(f"    2. PEFT åº“ç‰ˆæœ¬é—®é¢˜")
        print(f"    3. æ¨¡å—ç»“æ„ç‰¹æ®Šï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†")
    
    return replaced_count, not_replaced_count

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
# å°†æ•°æ®ç›®å½•åŠ å…¥è·¯å¾„ä»¥ä¾¿å¯¼å…¥ dataset
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cffa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CFFA_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cfoct_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_octfa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CF_OCTA_v2_repaired"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2
from operation_pre_filtered_cfoct_augmented_dataset import CFOCTDataset
from operation_pre_filtered_octfa_augmented_dataset import OCTFADataset
from cf_octa_v2_repaired_dataset import CFOCTADataset
from vessle_detector import extract_vessel_map

# ============ å…¨å±€é…ç½® ============
SIZE = 512
DEVICE = torch.device("cuda")
# æ¨¡å‹è·¯å¾„
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
SCRIBBLE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-scribble"
TILE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-tile"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_ctrl_sd15_dual"

# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_prompt_embeds(bs, tokenizer, text_encoder):
    """ç”Ÿæˆç©ºæç¤ºè¯çš„æ–‡æœ¬åµŒå…¥"""
    prompts = [""] * bs
    inputs = tokenizer(prompts, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt").to(DEVICE)
    return text_encoder(inputs.input_ids)[0]

def compute_image_gradients(image):
    """è®¡ç®—å›¾åƒçš„ Sobel æ¢¯åº¦ï¼ˆç”¨äºæ¢¯åº¦åŒ¹é…æŸå¤±ï¼‰"""
    kernel_x = torch.tensor([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], device=DEVICE).view(1, 1, 3, 3).expand(image.shape[1], 1, 3, 3)
    kernel_y = torch.tensor([[1., 2., 1.], [0., 0., 0.], [-1., -2., -1.]], device=DEVICE).view(1, 1, 3, 3).expand(image.shape[1], 1, 3, 3)
    grad_x = F.conv2d(image, kernel_x, padding=1, groups=image.shape[1])
    grad_y = F.conv2d(image, kernel_y, padding=1, groups=image.shape[1])
    return grad_x, grad_y

def compute_gradient_match_loss(pred, gt):
    """æ¢¯åº¦åŒ¹é…æŸå¤±ï¼šçº¦æŸé¢„æµ‹å›¾ä¸ GT åœ¨è¾¹ç¼˜ç©ºé—´çš„ä¸€è‡´æ€§"""
    pred_gray = pred[:, 1:2, :, :] # ä½¿ç”¨ç»¿è‰²é€šé“
    gt_gray = gt[:, 1:2, :, :]
    px, py = compute_image_gradients(pred_gray)
    gx, gy = compute_image_gradients(gt_gray)
    return F.l1_loss(px, gx) + F.l1_loss(py, gy)

def gaussian_blur(img, kernel_size=7, sigma=1.5):
    """
    å¯¹å›¾åƒåšå¯å¾®åˆ†çš„é«˜æ–¯æ¨¡ç³Šï¼Œç”¨äºåˆ†ç¦»ä½é¢‘/é«˜é¢‘åˆ†é‡
    img: (B, C, H, W)ï¼Œæ•°å€¼èŒƒå›´çº¦ [0, 1]
    """
    channels = img.shape[1]
    device = img.device
    dtype = img.dtype
    
    # 1D é«˜æ–¯æ ¸
    x = torch.arange(kernel_size, device=device, dtype=dtype) - kernel_size // 2
    gauss = torch.exp(-0.5 * (x / sigma) ** 2)
    gauss = gauss / gauss.sum()
    
    kernel_x = gauss.view(1, 1, 1, -1)   # (1,1,1,K)
    kernel_y = gauss.view(1, 1, -1, 1)   # (1,1,K,1)
    
    # ç»„å·ç§¯ï¼šæ¯ä¸ªé€šé“ä½¿ç”¨åŒä¸€ä¸ªæ ¸
    img = F.conv2d(img, kernel_x.expand(channels, 1, 1, -1),
                   padding=(0, kernel_size // 2), groups=channels)
    img = F.conv2d(img, kernel_y.expand(channels, 1, -1, 1),
                   padding=(kernel_size // 2, 0), groups=channels)
    return img

def compute_texture_loss(pred_01, gt_01):
    """
    é«˜é¢‘çº¹ç†åŒ¹é…æŸå¤±ï¼š
    å…ˆç”¨é«˜æ–¯æ¨¡ç³Šåˆ†ç¦»å‡ºä½é¢‘ï¼Œå†å¯¹é«˜é¢‘æ®‹å·® (åŸå›¾-ä½é¢‘) åš L1 çº¦æŸï¼Œ
    é¼“åŠ±æ¨¡å‹å­¦ä¹  FA çš„å™ªå£°/çº¹ç†ç»Ÿè®¡ï¼Œè€Œä¸æ˜¯å…¨éƒ¨æŠ¹å¹³ã€‚
    """
    pred_blur = gaussian_blur(pred_01, kernel_size=7, sigma=1.5)
    gt_blur   = gaussian_blur(gt_01,   kernel_size=7, sigma=1.5)
    pred_hf = pred_01 - pred_blur
    gt_hf   = gt_01   - gt_blur
    return F.l1_loss(pred_hf, gt_hf)

def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡"""
    if step < 4000: return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2

# ============ 2. æ ¸å¿ƒæŸå¤±è®¡ç®— ============

def compute_total_loss(noise_pred, noise, noisy_latents, latents, timesteps, vae, noise_scheduler, msssim_fn, args):
    """è®¡ç®—ç»¼åˆæŸå¤±ï¼šMSE + MS-SSIM + Vessel Dice + Gradient + Texture"""
    # 1. å™ªå£°ç©ºé—´ MSE æŸå¤±
    loss_mse = F.mse_loss(noise_pred, noise)
    
    # ä»å™ªå£°é¢„æµ‹ä¸­æ¢å¤å›¾åƒ (x0 é¢„æµ‹)
    alphas = noise_scheduler.alphas_cumprod.to(DEVICE)
    at = alphas[timesteps].view(-1, 1, 1, 1)
    pred_x0_latents = (noisy_latents - (1 - at).sqrt() * noise_pred) / at.sqrt()
    
    # è§£ç åˆ°åƒç´ ç©ºé—´ [-1, 1]
    pred_imgs = vae.decode(pred_x0_latents / vae.config.scaling_factor).sample
    with torch.no_grad():
        gt_imgs = vae.decode(latents / vae.config.scaling_factor).sample
    
    pred_01 = (pred_imgs.clamp(-1, 1) + 1) / 2
    gt_01 = (gt_imgs.clamp(-1, 1) + 1) / 2
    
    # 2. MS-SSIM æŸå¤±
    loss_msssim = 1 - msssim_fn(pred_01, gt_01) if args.msssim_lambda > 0 else torch.tensor(0.0).to(DEVICE)
    
    # 3. è¡€ç®¡ç»“æ„æŸå¤± (Dice Loss)
    source_type, target_type = args.mode.split('2')
    pred_vessel = extract_vessel_map(pred_01, target_type, args.mode)
    with torch.no_grad():
        gt_vessel = extract_vessel_map(gt_01, target_type, args.mode)
        # ã€é‡è¦ã€‘è®­ç»ƒæ—¶ä½¿ç”¨è¿ç»­è¡€ç®¡å“åº”å›¾ï¼Œä¸ä½¿ç”¨ Otsu äºŒå€¼åŒ–
        # Otsu äºŒå€¼åŒ–ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼ˆæ¯ä¸ªæ ·æœ¬é˜ˆå€¼ä¸åŒï¼ŒæŸå¤±å°ºåº¦ä¸ä¸€è‡´ï¼‰
        # Otsu ä»…åœ¨æ¨ç†/è¯„ä¼°æ—¶ä½¿ç”¨ï¼Œç”¨äºè®¡ç®—äºŒå€¼åŒ–çš„ Dice æŒ‡æ ‡
    
    smooth = 1e-5
    # ä½¿ç”¨è¿ç»­å“åº”å›¾è®¡ç®— Diceï¼Œä¿æŒæ¢¯åº¦å¹³æ»‘å’Œè®­ç»ƒç¨³å®š
    intersection = (pred_vessel * gt_vessel).sum()
    dice_coeff = (2.0 * intersection + smooth) / (pred_vessel.sum() + gt_vessel.sum() + smooth)
    loss_vessel = 1.0 - dice_coeff
    
    # 4. æ¢¯åº¦åŒ¹é…æŸå¤±
    loss_grad = compute_gradient_match_loss(pred_01, gt_01)

    # 5. é«˜é¢‘çº¹ç†æŸå¤±
    loss_tex = compute_texture_loss(pred_01, gt_01) if args.texture_lambda > 0 else torch.tensor(0.0).to(DEVICE)
    
    # ç»„åˆæ€»æŸå¤±
    total_loss = (
        loss_mse
        + args.msssim_lambda * loss_msssim
        + args.vessel_lambda * loss_vessel
        + args.grad_lambda * loss_grad
        + args.texture_lambda * loss_tex
    )
    return total_loss, loss_mse, loss_msssim, loss_vessel, loss_grad, loss_tex

# ============ 3. éªŒè¯ä¸æ—©åœé€»è¾‘ ============

def evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, msssim_fn, tokenizer, text_encoder, args):
    """åœ¨å›ºå®šéªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼šä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„ç»¼åˆæŸå¤±(å« texture)"""
    cn_s.eval(); cn_t.eval()
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cond_tile, tgt, _, _ = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            # ã€v19ä¿®æ­£ã€‘cond_tile ç°åœ¨æ˜¯ [-1, 1] èŒƒå›´
            source_type, _ = args.mode.split('2')
            cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
            vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # VAE ç¼–ç 
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder)
            
            # ControlNet æ¨ç†
            # ã€v19ä¿®æ­£ã€‘cond_tile ç°åœ¨æ˜¯ [-1, 1] èŒƒå›´ï¼Œç¬¦åˆ ControlNet é¢„è®­ç»ƒå‡è®¾
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            # UNet é¢„æµ‹
            # ã€PEFT ä¿®å¤ã€‘ä½¿ç”¨ base_model è®¿é—®åŒ…å« LoRA å±‚çš„æ¨¡å‹ï¼Œç»•è¿‡ PEFT åŒ…è£…å™¨çš„ forward æ–¹æ³•
            # base_model ä»ç„¶åŒ…å« LoRA å±‚ï¼Œä½†ä¸ä¼šè‡ªåŠ¨æ·»åŠ ä¸å…¼å®¹çš„å‚æ•°ï¼ˆå¦‚ input_idsï¼‰
            noise_pred = unet.base_model(
                sample=noisy_latents,
                timestep=timesteps,
                encoder_hidden_states=prompt_embeds,
                              down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                mid_block_additional_residual=mid_s+mid_t,
                return_dict=False
            )[0]
            
            # ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„ç»¼åˆæŸå¤±ä½œä¸ºéªŒè¯æŒ‡æ ‡ï¼ˆåŒ…å«çº¹ç†/è¡€ç®¡/æ¢¯åº¦ç­‰ï¼‰
            total_loss, _, _, _, _, _ = compute_total_loss(
                noise_pred, noise, noisy_latents, latents, timesteps,
                vae, noise_scheduler, msssim_fn, args
            )
            val_losses.append(total_loss.item())
            
    cn_s.train(); cn_t.train()
    torch.cuda.empty_cache()
    return np.mean(val_losses)

def visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, step, out_dir):
    """è¿è¡Œå…¨é‡æ¨ç†å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ (å¯¹é½ v14)"""
    print(f"\n[å¯è§†åŒ–] æ­£åœ¨è¿è¡Œæ¨ç†å¯è§†åŒ– (Step {step})...")
    
    # åˆ›å»ºæ¨ç†æµ‹è¯•ç›®å½•
    infer_dir = os.path.join(out_dir, f"step_{step}_inference")
    os.makedirs(infer_dir, exist_ok=True)
    
    # ä¸´æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼
    cn_s.eval(); cn_t.eval()
    
    # æ„å»º pipeline
    # ã€PEFT ä¿®å¤ã€‘ä½¿ç”¨ unet.base_model è€Œä¸æ˜¯ unetï¼Œé¿å… PEFT åŒ…è£…å™¨è‡ªåŠ¨æ·»åŠ ä¸å…¼å®¹çš„å‚æ•°ï¼ˆå¦‚ input_idsï¼‰
    # base_model ä»ç„¶åŒ…å« LoRA å±‚ï¼Œä½†ä¸ä¼šç»è¿‡ PEFT åŒ…è£…å™¨çš„ forward æ–¹æ³•
    multi_controlnet = MultiControlNetModel([cn_s, cn_t])
    pipe = StableDiffusionControlNetPipeline(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        unet=unet.base_model,
        controlnet=multi_controlnet,
        scheduler=noise_scheduler,
        safety_checker=None,
        feature_extractor=None
    ).to(DEVICE)
    pipe.set_progress_bar_config(disable=True)
    
    # åªå–å‰ 2 ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    with torch.no_grad():
        for i, batch in enumerate(val_loader):
            if i >= 2: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            # ã€v19ä¿®æ­£ã€‘cond_tile ç°åœ¨æ˜¯ [-1, 1] èŒƒå›´
            source_type, _ = args.mode.split('2')
            cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
            vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # æ¨ç†
            generator = torch.Generator(device=DEVICE).manual_seed(42)
            # æ¨ç†å°ºå¯¸è·Ÿéš Dataset (512) æˆ–å…¨å±€é…ç½®
            h, w = cond_tile.shape[2], cond_tile.shape[3]
            
            output_img = pipe(
                prompt="",
                image=[cond_scribble, cond_tile],
                num_inference_steps=25,
                controlnet_conditioning_scale=[args.scribble_scale, args.tile_scale],
                generator=generator,
                width=w,
                height=h
            ).images[0]
            
            # ä¿å­˜ç»“æœ
            try:
                name = os.path.splitext(os.path.basename(cp[0]))[0]
            except:
                name = f"sample_{i}"
                
            # ä¿å­˜è¾“å…¥å’Œç›®æ ‡
            # ã€v19ä¿®æ­£ã€‘cond_tile ç°åœ¨æ˜¯ [-1, 1]ï¼Œcond_scribble æ˜¯ [0, 1]
            cond_scribble_save = (cond_scribble[0].cpu().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
            cond_tile_save = ((cond_tile[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            # tgt is [-1, 1]
            tgt_save = ((tgt[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            
            Image.fromarray(cond_scribble_save).save(os.path.join(infer_dir, f"{name}_01_scribble.png"))
            Image.fromarray(cond_tile_save).save(os.path.join(infer_dir, f"{name}_02_tile.png"))
            Image.fromarray(tgt_save).save(os.path.join(infer_dir, f"{name}_03_target.png"))
            output_img.save(os.path.join(infer_dir, f"{name}_04_pred.png"))

    # æ¢å¤è®­ç»ƒæ¨¡å¼
    cn_s.train(); cn_t.train()
    
    # æ˜¾å¼æ¸…ç†æ˜¾å­˜ (é˜²æ­¢ OOM)
    del pipe
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ“ æ¨ç†å¯è§†åŒ–å·²ä¿å­˜åˆ°: {infer_dir}\n")

# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["cf2fa", "fa2cf", "cf2oct", "oct2cf", "fa2oct", "oct2fa", "cf2octa", "octa2cf"], required=True)
    parser.add_argument("-n", "--name", default="exp_v15")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--scribble_scale", type=float, default=0.8)
    parser.add_argument("--tile_scale", type=float, default=1.0)
    # è°ƒä½ MS-SSIM / æ¢¯åº¦æƒé‡ï¼Œä¸ºçº¹ç†ç•™å‡ºè‡ªç”±åº¦
    parser.add_argument("--msssim_lambda", type=float, default=0.05)
    parser.add_argument("--vessel_lambda", type=float, default=0.05)
    parser.add_argument("--grad_lambda", type=float, default=0.05)
    # æ–°å¢ï¼šé«˜é¢‘çº¹ç†æŸå¤±æƒé‡
    parser.add_argument("--texture_lambda", type=float, default=0.2)
    parser.add_argument("--patience", type=int, default=8)
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.mode, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½
    if 'octa' in args.mode:
        train_ds = CFOCTADataset(split='train', mode=args.mode)
        val_ds = CFOCTADataset(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'fa' in args.mode:
        # ä»…ä½¿ç”¨ operation_pre_filtered_cffa_augmented ç‰ˆæœ¬çš„æ•°æ®é›†
        train_ds = CFFADataset_v2(split='train', mode=args.mode)
        val_ds = CFFADataset_v2(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'oct' in args.mode:
        train_ds = CFOCTDataset(split='train', mode=args.mode)
        val_ds = CFOCTDataset(split='test', mode=args.mode)
    elif 'fa' in args.mode and 'oct' in args.mode:
        train_ds = OCTFADataset(split='train', mode=args.mode)
        val_ds = OCTFADataset(split='test', mode=args.mode)
    else:
        raise ValueError(f"Unknown mode: {args.mode}")
    
    # éªŒè¯é›†ä½¿ç”¨å›ºå®šå­é›†ï¼ˆ10ä¸ªæ ·æœ¬ï¼‰æé«˜æ•ˆç‡
    val_indices = random.sample(range(len(val_ds)), min(10, len(val_ds)))
    val_subset = torch.utils.data.Subset(val_ds, val_indices)
    
    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)

    # 2. æ¨¡å‹åŠ è½½
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    cn_s = ControlNetModel.from_pretrained(SCRIBBLE_CN_DIR).to(DEVICE)
    cn_t = ControlNetModel.from_pretrained(TILE_CN_DIR).to(DEVICE)
    
    # å†»ç»“ VAE / æ–‡æœ¬ç¼–ç å™¨ï¼ŒåŒæ—¶è®­ç»ƒ ControlNet å’Œ UNet LoRA
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    # ControlNet ä¿æŒå¯è®­ç»ƒçŠ¶æ€ï¼ˆä¸å†»ç»“ï¼‰
    # cn_s å’Œ cn_t çš„ requires_grad é»˜è®¤ä¸º Trueï¼Œä¸éœ€è¦æ˜¾å¼è®¾ç½®
    
    # ============ UNet LoRA é…ç½® ============
    unet_lora_rank = 16
    unet_lora_alpha = 16
    
    # å†»ç»“ UNet åŸå§‹æƒé‡
    unet.requires_grad_(False)
    
    # ä½¿ç”¨ peft åº“åˆ›å»º LoRA é€‚é…å™¨
    # ç›®æ ‡æ¨¡å—ï¼šæ³¨æ„åŠ›å±‚çš„å…³é”®ç»„ä»¶
    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
    
    lora_config = LoraConfig(
        r=unet_lora_rank,
        lora_alpha=unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )
    
    # å°† LoRA åº”ç”¨åˆ° UNet
    unet = get_peft_model(unet, lora_config)
    
    # éªŒè¯ LoRA æ˜¯å¦æ­£ç¡®åº”ç”¨
    verify_and_fix_lora_application(unet, target_modules)
    
    # æ”¶é›†å¯è®­ç»ƒçš„ LoRA å‚æ•°
    trainable_params = [p for p in unet.parameters() if p.requires_grad]
    num_trainable_params = sum(p.numel() for p in trainable_params)
    num_total_params = sum(p.numel() for p in unet.parameters())
    
    # æ‰“å° LoRA å‚æ•°ä¿¡æ¯
    print(f"âœ“ ä½¿ç”¨ UNet LoRA è®­ç»ƒ")
    print(f"  - LoRA rank: {unet_lora_rank}, alpha: {unet_lora_alpha}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°é‡: {num_trainable_params:,} ({num_trainable_params/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°é‡: {num_total_params:,} ({num_total_params/1e6:.2f}M)")
    print(f"  - LoRA å‚æ•°å æ¯”: {num_trainable_params/num_total_params*100:.2f}%")
    
    # è°ƒè¯•ï¼šæ‰“å°ä¸€äº› LoRA å‚æ•°åç§°ç¤ºä¾‹
    lora_param_names = [name for name, param in unet.named_parameters() if param.requires_grad]
    if lora_param_names:
        print(f"  - LoRA å‚æ•°ç¤ºä¾‹ (å‰5ä¸ª): {lora_param_names[:5]}")
    else:
        print(f"  âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•å¯è®­ç»ƒçš„ LoRA å‚æ•°ï¼")
    
    # æ”¶é›† ControlNet å¯è®­ç»ƒå‚æ•°
    cn_s_trainable = [p for p in cn_s.parameters() if p.requires_grad]
    cn_t_trainable = [p for p in cn_t.parameters() if p.requires_grad]
    cn_s_num = sum(p.numel() for p in cn_s_trainable)
    cn_t_num = sum(p.numel() for p in cn_t_trainable)
    
    print(f"\nâœ“ åŒæ—¶è®­ç»ƒ ControlNet")
    print(f"  - ControlNet Scribble å¯è®­ç»ƒå‚æ•°é‡: {cn_s_num:,} ({cn_s_num/1e6:.2f}M)")
    print(f"  - ControlNet Tile å¯è®­ç»ƒå‚æ•°é‡: {cn_t_num:,} ({cn_t_num/1e6:.2f}M)")
    print(f"  - ControlNet æ€»å¯è®­ç»ƒå‚æ•°é‡: {cn_s_num + cn_t_num:,} ({(cn_s_num + cn_t_num)/1e6:.2f}M)")
    
    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    # ä¼˜åŒ–å™¨ï¼šåŒæ—¶è®­ç»ƒ ControlNet å’Œ UNet LoRA å‚æ•°
    # ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ä¸º ControlNet å’Œ LoRA è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    all_trainable_params = list(cn_s.parameters()) + list(cn_t.parameters()) + trainable_params
    optimizer = torch.optim.AdamW(
        all_trainable_params,
        lr=5e-5,
        weight_decay=1e-2
    )
    print(f"\nâœ“ ä¼˜åŒ–å™¨é…ç½®ï¼šåŒæ—¶è®­ç»ƒ ControlNet + UNet LoRA")
    print(f"  - æ€»å¯è®­ç»ƒå‚æ•°é‡: {cn_s_num + cn_t_num + num_trainable_params:,} ({(cn_s_num + cn_t_num + num_trainable_params)/1e6:.2f}M)")
    print(f"  - å­¦ä¹ ç‡: 5e-5 (ç»Ÿä¸€å­¦ä¹ ç‡)")
    # optimizer = bnb.optim.AdamW8bit(list(cn_s.parameters()) + list(cn_t.parameters()), lr=5e-5)
    msssim_fn = MS_SSIM(data_range=1.0, size_average=True, channel=3).to(DEVICE)

    # 3. è®­ç»ƒçŠ¶æ€å˜é‡
    global_step = 0
    best_val_loss = float('inf')
    wait = 0
    start_time = time.time()

    # æ—¥å¿—ç´¯åŠ å™¨ (å¯¹é½ v14)
    loss_accumulator = []
    msssim_loss_accumulator = []
    vessel_loss_accumulator = []
    grad_loss_accumulator = []
    texture_loss_accumulator = []

    print(f"\nå¼€å§‹è®­ç»ƒ [{args.mode}] - æ ·æœ¬æ•°: {len(train_ds)}")
    
    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # ã€æ ¸å¿ƒé€»è¾‘ã€‘å®æ—¶ç”Ÿæˆè¡€ç®¡å›¾ä½œä¸ºæ¡ä»¶è¾“å…¥
            # ã€v19ä¿®æ­£ã€‘cond_tile ç°åœ¨æ˜¯ [-1, 1] èŒƒå›´ï¼Œéœ€è¦è½¬ä¸º [0, 1] å†æå–è¡€ç®¡
            source_type, _ = args.mode.split('2')
            with torch.no_grad():
                cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
                vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
                cond_scribble = vessel_map.repeat(1, 3, 1, 1)

            # Debug: Step 0 å›¾åƒä¿å­˜ (å¯¹é½ v14)
            if global_step == 0:
                debug_dir = os.path.join(out_dir, "debug_images_step0")
                os.makedirs(debug_dir, exist_ok=True)
                
                # å°è¯•è·å–æ–‡ä»¶å
                try:
                    name = os.path.splitext(os.path.basename(cp[0]))[0]
                except:
                    name = "step0_sample"

                # 1. ä¿å­˜Scribbleæ¡ä»¶å›¾ (Vessel)
                cond_scribble_save = (cond_scribble[0].cpu().float().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_scribble_save).save(os.path.join(debug_dir, f"{name}_scribble_input.png"))
                
                # 2. ä¿å­˜Tileæ¡ä»¶å›¾ (åŸå›¾) ã€v19ä¿®æ­£ï¼šç°åœ¨æ˜¯ [-1, 1] èŒƒå›´ã€‘
                cond_tile_save = ((cond_tile[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_tile_save).save(os.path.join(debug_dir, f"{name}_tile_input.png"))
                
                # 3. ä¿å­˜ç›®æ ‡å›¾ (GT) [-1, 1]
                tgt_save = ((tgt[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(tgt_save).save(os.path.join(debug_dir, f"{name}_target.png"))
                
                print(f"\nâœ“ Step 0 è°ƒè¯•å›¾åƒå·²ä¿å­˜åˆ°: {debug_dir}\n")

            # VAE & å™ªå£°å¤„ç†
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            noise = torch.randn_like(latents)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder)
            
            # åŒè·¯ ControlNet å‰å‘
            # ã€v19ä¿®æ­£ã€‘cond_tile ç°åœ¨æ˜¯ [-1, 1] èŒƒå›´ï¼Œç¬¦åˆ ControlNet é¢„è®­ç»ƒå‡è®¾
            # ã€æ··åˆè®­ç»ƒç‰ˆæœ¬ã€‘ControlNet éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œä¸ä½¿ç”¨ no_grad
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            # UNet é¢„æµ‹
            # ã€PEFT ä¿®å¤ã€‘ä½¿ç”¨ base_model è®¿é—®åŒ…å« LoRA å±‚çš„æ¨¡å‹ï¼Œç»•è¿‡ PEFT åŒ…è£…å™¨çš„ forward æ–¹æ³•
            # base_model ä»ç„¶åŒ…å« LoRA å±‚ï¼Œä½†ä¸ä¼šè‡ªåŠ¨æ·»åŠ ä¸å…¼å®¹çš„å‚æ•°ï¼ˆå¦‚ input_idsï¼‰
            noise_pred = unet.base_model(
                sample=noisy_latents,
                timestep=timesteps,
                encoder_hidden_states=prompt_embeds,
                              down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                mid_block_additional_residual=mid_s+mid_t,
                return_dict=False
            )[0]
            
            # è®¡ç®— Loss
            loss, l_mse, l_ssim, l_vessel, l_grad, l_tex = compute_total_loss(
                noise_pred, noise, noisy_latents, latents, timesteps, vae, noise_scheduler, msssim_fn, args
            )
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # åŠ¨æ€å­¦ä¹ ç‡æ›´æ–°
            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups: param_group['lr'] = current_lr
            
            # ç»Ÿè®¡ (å¯¹é½ v14)
            loss_accumulator.append(l_mse.item())
            msssim_loss_accumulator.append(l_ssim.item())
            vessel_loss_accumulator.append(l_vessel.item())
            grad_loss_accumulator.append(l_grad.item())
            texture_loss_accumulator.append(l_tex.item())
            
            # æ—¥å¿—æ‰“å° (å¯¹é½ v14)
            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                
                avg_mse = np.mean(loss_accumulator)
                avg_ssim = np.mean(msssim_loss_accumulator)
                avg_vessel = np.mean(vessel_loss_accumulator)
                avg_grad = np.mean(grad_loss_accumulator)
                avg_tex = np.mean(texture_loss_accumulator)
                
                # è®¡ç®—åŠ æƒåçš„å€¼ç”¨äºæ‰“å°
                w_ssim = avg_ssim * args.msssim_lambda
                w_vessel = avg_vessel * args.vessel_lambda
                w_grad = avg_grad * args.grad_lambda
                w_tex = avg_tex * args.texture_lambda
                w_total = avg_mse + w_ssim + w_vessel + w_grad + w_tex
                
                loss_accumulator = []
                msssim_loss_accumulator = []
                vessel_loss_accumulator = []
                grad_loss_accumulator = []
                texture_loss_accumulator = []
                
                t_val = timesteps[0].item()
                
                msg_parts = [
                    f"[SD15-v21æ··åˆ] step {global_step:5d}/{args.max_steps}",
                    f"lr:{current_lr:.2e}",
                    f"total:{w_total:.4f}",
                    f"mse:{avg_mse:.4f}",
                ]
                if args.vessel_lambda > 0:
                    msg_parts.append(f"vessel:{w_vessel:.4f}(Î»={args.vessel_lambda})")
                if args.msssim_lambda > 0:
                    msg_parts.append(f"msssim:{w_ssim:.4f}(Î»={args.msssim_lambda})")
                if args.grad_lambda > 0:
                    msg_parts.append(f"grad:{w_grad:.4f}(Î»={args.grad_lambda})")
                if args.texture_lambda > 0:
                    msg_parts.append(f"tex:{w_tex:.4f}(Î»={args.texture_lambda})")
                
                msg_parts.extend([
                    f"t={t_val:3d}",
                    f"S:{args.scribble_scale}",
                    f"T:{args.tile_scale}",
                    f"{elapsed:.1f}s"
                ])
                msg = " | ".join(msg_parts)
                print(msg)
                
                # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")
                
                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯ & æ—©åœåˆ¤æ–­
            if global_step % 500 == 0:
                val_loss = evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, msssim_fn, tokenizer, text_encoder, args)
                
                # è®°å½•éªŒè¯æ—¥å¿— (å¯¹é½éœ€æ±‚)
                val_msg = f"[éªŒè¯] Step {global_step} | Avg Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")
                
                # ã€å¯¹é½ v14ã€‘è¿è¡Œæ¨ç†å¯è§†åŒ–
                visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, global_step, out_dir)

                # ä¿å­˜æœ€æ–°æƒé‡
                latest_dir = os.path.join(out_dir, "latest_checkpoint")
                os.makedirs(latest_dir, exist_ok=True)
                cn_s.save_pretrained(os.path.join(latest_dir, "controlnet_scribble"))
                cn_t.save_pretrained(os.path.join(latest_dir, "controlnet_tile"))
                # ä¿å­˜ UNet çš„ LoRA æƒé‡ï¼ˆä½¿ç”¨ peft çš„ save_pretrainedï¼‰
                unet_lora_dir = os.path.join(latest_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)
                
                # ä¿å­˜æœ€æ–°å…ƒä¿¡æ¯ (å¯¹é½ v14)
                with open(os.path.join(latest_dir, "latest_info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Latest Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"Best Loss: {best_val_loss:.6f}\n")
                
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    wait = 0
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)
                    cn_s.save_pretrained(os.path.join(best_dir, "controlnet_scribble"))
                    cn_t.save_pretrained(os.path.join(best_dir, "controlnet_tile"))
                    # ä¿å­˜æœ€ä½³æ¨¡å‹å¯¹åº”çš„ UNet LoRA æƒé‡ï¼ˆä½¿ç”¨ peft çš„ save_pretrainedï¼‰
                    unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(unet_lora_dir)
                    
                    # ä¿å­˜æœ€ä½³å…ƒä¿¡æ¯ (å¯¹é½ v14)
                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                    
                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)
                else:
                    if global_step >= 4000: # Warm-up åæ‰è§¦å‘ patience
                        wait += 1
                        print(f"âš  éªŒè¯æŸè€—æœªä¸‹é™ ({wait}/{args.patience})\n")
                        if wait >= args.patience:
                            print("ğŸ›‘ è§¦å‘æ—©åœï¼Œè®­ç»ƒç»“æŸã€‚")
                            return

            global_step += 1

if __name__ == "__main__":
    main()

