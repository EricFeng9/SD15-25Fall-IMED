# -*- coding: utf-8 -*-
"""
SDXL ControlNet è®­ç»ƒè„šæœ¬ v21
åŸºäº v18 æ”¹è¿›ï¼Œä¸“æ³¨è§£å†³"ç»“æ„å¥½ä½†çº¹ç†/äº®åº¦ä¸çœŸå®"é—®é¢˜

ã€æ ¸å¿ƒå˜åŠ¨ - é’ˆå¯¹è§†è§‰å›¾çµæµ‹è¯•ã€‘
1. âœ… UNet LoRA è®­ç»ƒï¼šè®© UNet å­¦ä¹ åŒ»å­¦å›¾åƒçš„çº¹ç†å’Œäº®åº¦åˆ†å¸ƒï¼ˆv18 ä¸­ UNet è¢«å†»ç»“ï¼‰
2. âœ… ç§»é™¤æ‰€æœ‰åƒç´ çº§æŸå¤±ï¼šåªä¿ç•™çº¯ç²¹çš„å™ªå£°é¢„æµ‹ MSEï¼ˆç§»é™¤ SSIM/Vessel/Gradient/Texture Lossï¼‰
3. âœ… åŒ»å­¦å›¾åƒ Promptï¼šä½¿ç”¨é¢†åŸŸç‰¹å®šçš„ prompt è€Œä¸æ˜¯ç©ºå­—ç¬¦ä¸²
4. âœ… Offset Noiseï¼šè§£å†³äº®åº¦åäº®ã€å¯¹æ¯”åº¦ä¸è¶³çš„é—®é¢˜
5. âœ… åŒæ—¶è®­ç»ƒ ControlNet + UNet LoRAï¼Œå„å¸å…¶èŒï¼ˆç»“æ„ vs çº¹ç†ï¼‰
"""

import os
import math
import time
import random
import argparse
import gc
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import transforms
from diffusers import (DDPMScheduler, ControlNetModel, AutoencoderKL, UNet2DConditionModel, 
                       StableDiffusionControlNetPipeline, MultiControlNetModel)
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType
#import bitsandbytes as bnb

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
# å°†æ•°æ®ç›®å½•åŠ å…¥è·¯å¾„ä»¥ä¾¿å¯¼å…¥ dataset
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cffa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CFFA_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_cfoct_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/operation_pre_filtered_octfa_augmented"))
sys.path.append(os.path.join(os.path.dirname(__file__), "../../data/CF_OCTA_v2_repaired"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2
from operation_pre_filtered_cfoct_augmented_dataset import CFOCTDataset
from operation_pre_filtered_octfa_augmented_dataset import OCTFADataset
from cf_octa_v2_repaired_dataset import CFOCTADataset
from vessle_detector import extract_vessel_map

# ============ å…¨å±€é…ç½® ============
SIZE = 512
DEVICE = torch.device("cuda")
# æ¨¡å‹è·¯å¾„
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
SCRIBBLE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-scribble"
TILE_CN_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/controlnet-sd15-tile"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_ctrl_sd15_dual"

# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_prompt_embeds(bs, tokenizer, text_encoder, mode="cf2fa"):
    """
    ç”ŸæˆåŒ»å­¦å›¾åƒé¢†åŸŸç‰¹å®šçš„æç¤ºè¯åµŒå…¥
    
    ã€v21 æ”¹è¿›ã€‘ä¸å†ä½¿ç”¨ç©º promptï¼Œè€Œæ˜¯ä½¿ç”¨é¢†åŸŸç‰¹å®šæè¿°
    è¿™æœ‰åŠ©äºæ¿€æ´»æ¨¡å‹ä¸­ä¸åŒ»å­¦å½±åƒç›¸å…³çš„æ½œåœ¨è¯­ä¹‰åˆ†å¸ƒ
    """
    if 'fa' in mode:
        # FA (è§å…‰è¡€ç®¡é€ å½±) çš„ç‰¹å¾ï¼šé«˜å¯¹æ¯”åº¦ã€é»‘èƒŒæ™¯ã€äº®è¡€ç®¡ã€é¢—ç²’å™ªå£°
        prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    elif 'oct' in mode:
        # OCT çš„ç‰¹å¾ï¼šå±‚çŠ¶ç»“æ„ã€ç°åº¦å›¾
        prompt = "optical coherence tomography, retinal cross section, medical scan, grayscale"
    elif 'cf' in mode:
        # CF (å½©è‰²çœ¼åº•) çš„ç‰¹å¾ï¼šå½©è‰²ã€è‡ªç„¶å…‰ç…§
        prompt = "color fundus photography, retinal image, medical photography"
    else:
        prompt = "medical retinal imaging"
    
    prompts = [prompt] * bs
    inputs = tokenizer(prompts, padding="max_length", max_length=tokenizer.model_max_length, 
                       truncation=True, return_tensors="pt").to(DEVICE)
    return text_encoder(inputs.input_ids)[0]

def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡"""
    if step < 4000: return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2

# ============ 2. æ ¸å¿ƒæŸå¤±è®¡ç®— ============

def _gaussian_kernel_1d(kernel_size: int, sigma: float, device, dtype):
    """ç”Ÿæˆå½’ä¸€åŒ– 1D é«˜æ–¯å·ç§¯æ ¸"""
    half = kernel_size // 2
    coords = torch.arange(kernel_size, device=device, dtype=dtype) - half
    gauss = torch.exp(-0.5 * (coords / sigma) ** 2)
    return gauss / gauss.sum()


def gaussian_blur_latent(x, kernel_size=7, sigma=1.5):
    """
    å¯¹ (B, C, H, W) çš„ latent tensor åšå¯åˆ†ç¦»é«˜æ–¯æ¨¡ç³Šï¼ˆä¿æŒæ¢¯åº¦å¯é€šè¿‡ï¼‰ã€‚
    ç”¨äºå°† latent åˆ†è§£ä¸ºä½é¢‘ + é«˜é¢‘ä¸¤éƒ¨åˆ†ã€‚
    """
    C = x.shape[1]
    k = _gaussian_kernel_1d(kernel_size, sigma, x.device, x.dtype)
    pad = kernel_size // 2
    # æ°´å¹³æ–¹å‘
    kw = k.view(1, 1, 1, kernel_size).expand(C, 1, 1, kernel_size)
    x = F.conv2d(x, kw, padding=(0, pad), groups=C)
    # å‚ç›´æ–¹å‘
    kh = k.view(1, 1, kernel_size, 1).expand(C, 1, kernel_size, 1)
    x = F.conv2d(x, kh, padding=(pad, 0), groups=C)
    return x


def compute_hf_texture_loss(pred_x0, gt_x0, kernel_size=7, sigma=1.5):
    """
    åœ¨é¢„æµ‹ x0ï¼ˆlatent ç©ºé—´ï¼‰ä¸Šè®¡ç®—é«˜é¢‘çº¹ç† L1 æŸå¤±ã€‚

    æ­¥éª¤ï¼š
    1. å¯¹ pred_x0 å’Œ gt_x0 åˆ†åˆ«åšé«˜æ–¯æ¨¡ç³Šï¼Œå¾—åˆ°ä½é¢‘è¿‘ä¼¼
    2. ç”¨"åŸå›¾ - ä½é¢‘"å¾—åˆ°é«˜é¢‘æ®‹å·®ï¼ˆåŒ…å«çº¹ç†ã€é¢—ç²’ã€ç»†èŠ‚ï¼‰
    3. å¯¹ä¸¤è€…é«˜é¢‘æ®‹å·®åš L1ï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨é«˜é¢‘ç»´åº¦ä¹Ÿå¯¹é½ GT

    å¥½å¤„ï¼š
    - ä¸éœ€è¦é¢å¤– VAE decodeï¼Œç›´æ¥åœ¨ latent ç©ºé—´è®¡ç®—ï¼Œä»£ä»·æä½
    - å¯å¾®åˆ†ï¼Œæ¢¯åº¦å¯ä»¥ç›´æ¥åä¼ ç»™ ControlNet å’Œ UNet LoRA
    - æ˜ç¡®å‘Šè¯‰æ¨¡å‹"çº¹ç†/é¢—ç²’/é«˜é¢‘ä¿¡æ¯ä¸èƒ½è¢«å¹³å‡æ‰"
    """
    pred_blur = gaussian_blur_latent(pred_x0, kernel_size, sigma)
    gt_blur   = gaussian_blur_latent(gt_x0,   kernel_size, sigma)
    pred_hf = pred_x0 - pred_blur
    gt_hf   = gt_x0   - gt_blur
    return F.l1_loss(pred_hf, gt_hf)


def compute_total_loss(noise_pred, noise, noisy_latents, latents,
                       alphas_cumprod, timesteps, hf_lambda=0.5):
    """
    ã€v22 æ”¹è¿›ã€‘MSE å™ªå£°æŸå¤± + é«˜é¢‘çº¹ç†æŸå¤±ï¼ˆlatent x0 ç©ºé—´ï¼‰

    åŸç†ï¼š
    - loss_mseï¼šæ ‡å‡†å™ªå£°é¢„æµ‹ MSEï¼Œçº¦æŸå…¨é¢‘æ®µçš„å…¨å±€é‡å»º
    - loss_hf ï¼šä» noise_pred åæ¨ pred_x0ï¼Œåœ¨ latent ç©ºé—´å¯¹é«˜é¢‘æ®‹å·®åš L1
                è¿™ä¸€é¡¹ä¸“é—¨è¡¥å¿"æœ‰å½¢æ— éª¨"â€”â€”è¿«ä½¿æ¨¡å‹åœ¨é«˜é¢‘ç»†èŠ‚ä¸Šä¹Ÿè¦å¯¹é½ GT

    å‚æ•°ï¼š
    - hf_lambdaï¼šé«˜é¢‘æŸå¤±æƒé‡ï¼Œæ¨è 0.3ï½1.0ï¼Œè¶Šå¤§é«˜é¢‘çº¦æŸè¶Šå¼º
    """
    # ---- æ ‡å‡† MSE ----
    loss_mse = F.mse_loss(noise_pred, noise)

    # ---- ä» noise_pred åæ¨é¢„æµ‹çš„å¹²å‡€ x0ï¼ˆlatent ç©ºé—´ï¼‰----
    # DDPM å‰å‘ï¼šz_t = sqrt(alpha_t)*x0 + sqrt(1-alpha_t)*noise
    # å› æ­¤ï¼šx0 = (z_t - sqrt(1-alpha_t)*noise_pred) / sqrt(alpha_t)
    alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(noisy_latents.device)
    pred_x0 = (noisy_latents - (1.0 - alpha_t).sqrt() * noise_pred) / (alpha_t.sqrt() + 1e-8)
    # åœ¨å¤§ t æ—¶ pred_x0 æ•°å€¼ä¸ç¨³å®šï¼Œæˆªæ–­åˆ°åˆç†èŒƒå›´
    pred_x0 = pred_x0.clamp(-10.0, 10.0)

    # ---- é«˜é¢‘çº¹ç†æŸå¤± ----
    loss_hf = compute_hf_texture_loss(pred_x0, latents)

    total = loss_mse + hf_lambda * loss_hf
    return total, loss_mse.item(), loss_hf.item()

# ============ 3. éªŒè¯ä¸æ—©åœé€»è¾‘ ============

VAL_TIMESTEPS = [200, 500, 800]   # å›ºå®šæ—¶é—´æ­¥ï¼šä½/ä¸­/é«˜å™ªå£°å„å–ä¸€ä¸ªä»£è¡¨ç‚¹

def evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args):
    """
    å…¨é‡éªŒè¯é›† + å›ºå®šæ—¶é—´æ­¥ï¼Œæ¶ˆé™¤éšæœºæ€§ã€‚

    å¯¹æ¯ä¸ªéªŒè¯æ ·æœ¬ï¼Œåœ¨ VAL_TIMESTEPS=[200,500,800] ä¸‰ä¸ªå›ºå®šæ—¶é—´æ­¥ä¸Šåˆ†åˆ«è®¡ç®— MSEï¼Œ
    å–å¹³å‡ä½œä¸ºè¯¥æ ·æœ¬çš„éªŒè¯æŸå¤±ã€‚è¿™æ ·éªŒè¯æŸå¤±åªéšæ¨¡å‹æƒé‡å˜åŒ–ï¼Œä¸å—éšæœº t å½±å“ï¼Œ
    å¯ä»¥å¯é åœ°ç”¨äº best checkpoint é€‰æ‹©å’Œæ—©åœåˆ¤æ–­ã€‚
    """
    cn_s.eval(); cn_t.eval()
    if hasattr(unet, 'eval'):
        unet.eval()

    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cond_tile, tgt, _, _ = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]

            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            source_type, _ = args.mode.split('2')
            cond_tile_01 = (cond_tile + 1) / 2
            vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)

            # VAE ç¼–ç ï¼ˆåªåšä¸€æ¬¡ï¼‰
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder, args.mode)

            sample_losses = []
            for t_val in VAL_TIMESTEPS:
                timesteps = torch.full((b,), t_val, device=DEVICE, dtype=torch.long)
                noise = torch.randn_like(latents)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
                down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)

                if hasattr(unet, 'base_model'):
                    noise_pred = unet.base_model(
                        sample=noisy_latents,
                        timestep=timesteps,
                        encoder_hidden_states=prompt_embeds,
                        down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                        mid_block_additional_residual=mid_s+mid_t,
                        return_dict=False
                    )[0]
                else:
                    noise_pred = unet(
                        noisy_latents, timesteps, prompt_embeds,
                        down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                        mid_block_additional_residual=mid_s+mid_t
                    ).sample

                sample_losses.append(F.mse_loss(noise_pred, noise).item())

            val_losses.append(np.mean(sample_losses))

    cn_s.train(); cn_t.train()
    if hasattr(unet, 'train'):
        unet.train()
    torch.cuda.empty_cache()
    return np.mean(val_losses)

def visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, step, out_dir):
    """ã€v21ä¼˜åŒ–ã€‘è¿è¡Œæ¨ç†å¹¶ä¿å­˜å¯è§†åŒ–ç»“æœ"""
    print(f"\n[å¯è§†åŒ–] æ­£åœ¨è¿è¡Œæ¨ç†å¯è§†åŒ– (Step {step})...")
    
    # åˆ›å»ºæ¨ç†æµ‹è¯•ç›®å½•
    infer_dir = os.path.join(out_dir, f"step_{step}_inference")
    os.makedirs(infer_dir, exist_ok=True)
    
    # ä¸´æ—¶åˆ‡æ¢åˆ° eval æ¨¡å¼
    cn_s.eval(); cn_t.eval()
    
    # ç¡®å®šä½¿ç”¨çš„ prompt
    if 'fa' in args.mode:
        prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    elif 'oct' in args.mode:
        prompt = "optical coherence tomography, retinal cross section, medical scan, grayscale"
    elif 'cf' in args.mode:
        prompt = "color fundus photography, retinal image, medical photography"
    else:
        prompt = "medical retinal imaging"
    
    # æ„å»º pipelineï¼ˆå¦‚æœ unet æ˜¯ PEFT åŒ…è£…çš„ï¼Œä½¿ç”¨ base_modelï¼‰
    multi_controlnet = MultiControlNetModel([cn_s, cn_t])
    unet_for_pipe = unet.base_model if hasattr(unet, 'base_model') else unet
    pipe = StableDiffusionControlNetPipeline(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        unet=unet_for_pipe,
        controlnet=multi_controlnet,
        scheduler=noise_scheduler,
        safety_checker=None,
        feature_extractor=None
    ).to(DEVICE)
    pipe.set_progress_bar_config(disable=True)
    
    # åªå–å‰ 2 ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    with torch.no_grad():
        for i, batch in enumerate(val_loader):
            if i >= 2: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            
            # å®æ—¶æå–è¡€ç®¡å›¾ä½œä¸º Scribble è¾“å…¥
            source_type, _ = args.mode.split('2')
            cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
            vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
            cond_scribble = vessel_map.repeat(1, 3, 1, 1)
            
            # æ¨ç†
            generator = torch.Generator(device=DEVICE).manual_seed(42)
            h, w = cond_tile.shape[2], cond_tile.shape[3]
            
            output_img = pipe(
                prompt=prompt,  # ã€v21æ”¹è¿›ã€‘ä½¿ç”¨åŒ»å­¦å›¾åƒ prompt
                image=[cond_scribble, cond_tile],
                num_inference_steps=25,
                controlnet_conditioning_scale=[args.scribble_scale, args.tile_scale],
                generator=generator,
                width=w,
                height=h
            ).images[0]
            
            # ä¿å­˜ç»“æœ
            try:
                name = os.path.splitext(os.path.basename(cp[0]))[0]
            except:
                name = f"sample_{i}"
                
            # ä¿å­˜è¾“å…¥å’Œç›®æ ‡
            cond_scribble_save = (cond_scribble[0].cpu().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
            cond_tile_save = ((cond_tile[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            tgt_save = ((tgt[0].cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
            
            Image.fromarray(cond_scribble_save).save(os.path.join(infer_dir, f"{name}_01_scribble.png"))
            Image.fromarray(cond_tile_save).save(os.path.join(infer_dir, f"{name}_02_tile.png"))
            Image.fromarray(tgt_save).save(os.path.join(infer_dir, f"{name}_03_target.png"))
            output_img.save(os.path.join(infer_dir, f"{name}_04_pred.png"))

    # æ¢å¤è®­ç»ƒæ¨¡å¼
    cn_s.train(); cn_t.train()
    
    # æ˜¾å¼æ¸…ç†æ˜¾å­˜ (é˜²æ­¢ OOM)
    del pipe
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ“ æ¨ç†å¯è§†åŒ–å·²ä¿å­˜åˆ°: {infer_dir}\n")

# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["cf2fa", "fa2cf", "cf2oct", "oct2cf", "fa2oct", "oct2fa", "cf2octa", "octa2cf"], required=True)
    parser.add_argument("-n", "--name", default="exp_v21")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--scribble_scale", type=float, default=0.8)
    parser.add_argument("--tile_scale", type=float, default=1.0)
    # ã€v21ç§»é™¤ã€‘æ‰€æœ‰åƒç´ çº§æŸå¤±çš„ lambda å‚æ•°éƒ½ç§»é™¤äº†
    # ã€v21æ–°å¢ã€‘UNet LoRA ç›¸å…³å‚æ•°
    parser.add_argument("--unet_lora_rank", type=int, default=16, help="UNet LoRA rank")
    parser.add_argument("--unet_lora_alpha", type=int, default=16, help="UNet LoRA alpha")
    parser.add_argument("--offset_noise_strength", type=float, default=0.1, help="Offset noise strength for better contrast")
    parser.add_argument("--hf_lambda", type=float, default=0.5, help="é«˜é¢‘çº¹ç†æŸå¤±æƒé‡ï¼Œæ¨è 0.3~1.0")
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.mode, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½
    if 'octa' in args.mode:
        train_ds = CFOCTADataset(split='train', mode=args.mode)
        val_ds = CFOCTADataset(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'fa' in args.mode:
        # ä»…ä½¿ç”¨ operation_pre_filtered_cffa_augmented ç‰ˆæœ¬çš„æ•°æ®é›†
        train_ds = CFFADataset_v2(split='train', mode=args.mode)
        val_ds = CFFADataset_v2(split='test', mode=args.mode)
    elif 'cf' in args.mode and 'oct' in args.mode:
        train_ds = CFOCTDataset(split='train', mode=args.mode)
        val_ds = CFOCTDataset(split='test', mode=args.mode)
    elif 'fa' in args.mode and 'oct' in args.mode:
        train_ds = OCTFADataset(split='train', mode=args.mode)
        val_ds = OCTFADataset(split='test', mode=args.mode)
    else:
        raise ValueError(f"Unknown mode: {args.mode}")
    
    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=False, num_workers=2)

    # 2. æ¨¡å‹åŠ è½½
    print("\n========== æ¨¡å‹åŠ è½½ ==========")
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    cn_s = ControlNetModel.from_pretrained(SCRIBBLE_CN_DIR).to(DEVICE)
    cn_t = ControlNetModel.from_pretrained(TILE_CN_DIR).to(DEVICE)
    
    # å†»ç»“ VAE å’Œ Text Encoder
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    
    # ============ ã€v21 æ ¸å¿ƒã€‘UNet LoRA é…ç½® ============
    print(f"\n========== UNet LoRA é…ç½® ==========")
    # å…ˆå†»ç»“ UNet åŸå§‹æƒé‡
    unet.requires_grad_(False)
    
    # ä½¿ç”¨ peft åº“åˆ›å»º LoRA é€‚é…å™¨
    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )
    
    # å°† LoRA åº”ç”¨åˆ° UNet
    unet = get_peft_model(unet, lora_config)
    
    # ç»Ÿè®¡å‚æ•°
    unet_lora_params = [p for p in unet.parameters() if p.requires_grad]
    unet_lora_num = sum(p.numel() for p in unet_lora_params)
    unet_total_num = sum(p.numel() for p in unet.parameters())
    
    print(f"âœ“ UNet LoRA å·²åº”ç”¨")
    print(f"  - Rank: {args.unet_lora_rank}, Alpha: {args.unet_lora_alpha}")
    print(f"  - ç›®æ ‡æ¨¡å—: {target_modules}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°: {unet_lora_num:,} ({unet_lora_num/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°: {unet_total_num:,} ({unet_total_num/1e6:.2f}M)")
    print(f"  - å‚æ•°å æ¯”: {unet_lora_num/unet_total_num*100:.2f}%")
    
    # ControlNet å‚æ•°ç»Ÿè®¡
    cn_s_num = sum(p.numel() for p in cn_s.parameters() if p.requires_grad)
    cn_t_num = sum(p.numel() for p in cn_t.parameters() if p.requires_grad)
    
    print(f"\nâœ“ ControlNet (åŒæ—¶è®­ç»ƒ)")
    print(f"  - Scribble: {cn_s_num:,} ({cn_s_num/1e6:.2f}M)")
    print(f"  - Tile: {cn_t_num:,} ({cn_t_num/1e6:.2f}M)")
    
    total_trainable = unet_lora_num + cn_s_num + cn_t_num
    print(f"\nâœ“ æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,} ({total_trainable/1e6:.2f}M)")
    
    # ä¼˜åŒ–å™¨é…ç½®
    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    all_trainable_params = list(cn_s.parameters()) + list(cn_t.parameters()) + unet_lora_params
    optimizer = torch.optim.AdamW(all_trainable_params, lr=5e-5, weight_decay=1e-2)
    
    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=5e-5, weight_decay=1e-2)")
    print(f"  - Offset Noise å¼ºåº¦: {args.offset_noise_strength}")

    # 3. è®­ç»ƒçŠ¶æ€å˜é‡
    global_step = 0
    best_val_loss = float('inf')
    start_time = time.time()

    # æ¯ä¸ªå…ƒç´ ä¸º (total, mse, hf) ä¸‰å…ƒç»„
    loss_accumulator = []

    print(f"\n========== å¼€å§‹è®­ç»ƒ ==========")
    print(f"æ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_ds)} (å…¨é‡ï¼Œå›ºå®šæ—¶é—´æ­¥ {VAL_TIMESTEPS})")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}\n")
    
    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps: break
            
            cond_tile, tgt, cp, tp = batch
            cond_tile, tgt = cond_tile.to(DEVICE), tgt.to(DEVICE)
            b = tgt.shape[0]
            
            # å®æ—¶ç”Ÿæˆè¡€ç®¡å›¾ä½œä¸ºæ¡ä»¶è¾“å…¥
            source_type, _ = args.mode.split('2')
            with torch.no_grad():
                cond_tile_01 = (cond_tile + 1) / 2  # [-1, 1] â†’ [0, 1]
                vessel_map = extract_vessel_map(cond_tile_01, source_type, args.mode)
                cond_scribble = vessel_map.repeat(1, 3, 1, 1)

            # Debug: Step 0 å›¾åƒä¿å­˜
            if global_step == 0:
                debug_dir = os.path.join(out_dir, "debug_images_step0")
                os.makedirs(debug_dir, exist_ok=True)
                
                try:
                    name = os.path.splitext(os.path.basename(cp[0]))[0]
                except:
                    name = "step0_sample"

                cond_scribble_save = (cond_scribble[0].cpu().float().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_scribble_save).save(os.path.join(debug_dir, f"{name}_scribble_input.png"))
                
                cond_tile_save = ((cond_tile[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(cond_tile_save).save(os.path.join(debug_dir, f"{name}_tile_input.png"))
                
                tgt_save = ((tgt[0].cpu().float().permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
                Image.fromarray(tgt_save).save(os.path.join(debug_dir, f"{name}_target.png"))
                
                print(f"âœ“ Step 0 è°ƒè¯•å›¾åƒå·²ä¿å­˜åˆ°: {debug_dir}\n")

            # VAE ç¼–ç 
            latents = vae.encode(tgt).latent_dist.sample() * vae.config.scaling_factor
            
            # ã€v21 æ ¸å¿ƒæ”¹è¿›ã€‘æ·»åŠ  Offset Noise æé«˜å¯¹æ¯”åº¦
            # Offset Noise: åœ¨æ ‡å‡†å™ªå£°åŸºç¡€ä¸Šæ·»åŠ ä¸€ä¸ªå…¨å±€åç§»ï¼Œæœ‰åŠ©äºç”Ÿæˆé«˜å¯¹æ¯”åº¦å›¾åƒ
            noise = torch.randn_like(latents)
            if args.offset_noise_strength > 0:
                noise += args.offset_noise_strength * torch.randn(latents.shape[0], latents.shape[1], 1, 1, device=latents.device)
            
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            prompt_embeds = get_prompt_embeds(b, tokenizer, text_encoder, args.mode)
            
            # åŒè·¯ ControlNet å‰å‘
            down_s, mid_s = cn_s(noisy_latents, timesteps, prompt_embeds, cond_scribble, args.scribble_scale, return_dict=False)
            down_t, mid_t = cn_t(noisy_latents, timesteps, prompt_embeds, cond_tile, args.tile_scale, return_dict=False)
            
            # UNet é¢„æµ‹ï¼ˆä½¿ç”¨ PEFT åŒ…è£…çš„æ¨¡å‹ï¼‰
            if hasattr(unet, 'base_model'):
                # PEFT åŒ…è£…çš„æ¨¡å‹ï¼Œä½¿ç”¨ base_model
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                    mid_block_additional_residual=mid_s+mid_t,
                    return_dict=False
                )[0]
            else:
                # æ™®é€šæ¨¡å‹
                noise_pred = unet(
                    noisy_latents, timesteps, prompt_embeds,
                    down_block_additional_residuals=[s+t for s,t in zip(down_s, down_t)],
                    mid_block_additional_residual=mid_s+mid_t
                ).sample
            
            # ã€v22ã€‘MSE + é«˜é¢‘çº¹ç†æŸå¤±
            loss, loss_mse_val, loss_hf_val = compute_total_loss(
                noise_pred, noise, noisy_latents, latents,
                noise_scheduler.alphas_cumprod, timesteps,
                hf_lambda=args.hf_lambda
            )

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # åŠ¨æ€å­¦ä¹ ç‡æ›´æ–°
            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups: param_group['lr'] = current_lr
            
            # ç»Ÿè®¡
            loss_accumulator.append((loss.item(), loss_mse_val, loss_hf_val))
            
            # æ—¥å¿—æ‰“å°
            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                arr = np.array(loss_accumulator)
                avg_loss, avg_mse, avg_hf = arr[:, 0].mean(), arr[:, 1].mean(), arr[:, 2].mean()
                loss_accumulator = []
                
                t_val = timesteps[0].item()
                
                msg = (f"[v22-LoRA] Step {global_step:5d}/{args.max_steps} | "
                       f"lr:{current_lr:.2e} | loss:{avg_loss:.4f} "
                       f"(mse:{avg_mse:.4f} hf:{avg_hf:.4f}) | t={t_val:3d} | "
                       f"S:{args.scribble_scale} T:{args.tile_scale} | {elapsed:.1f}s")
                print(msg)
                
                # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")
                
                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯
            if global_step % 500 == 0:
                val_loss = evaluate(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args)
                
                # è®°å½•éªŒè¯æ—¥å¿—
                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")
                
                # è¿è¡Œæ¨ç†å¯è§†åŒ–
                visualize_inference(val_loader, vae, unet, cn_s, cn_t, noise_scheduler, tokenizer, text_encoder, args, global_step, out_dir)

                # ä¿å­˜æœ€æ–°æƒé‡
                latest_dir = os.path.join(out_dir, "latest_checkpoint")
                os.makedirs(latest_dir, exist_ok=True)
                cn_s.save_pretrained(os.path.join(latest_dir, "controlnet_scribble"))
                cn_t.save_pretrained(os.path.join(latest_dir, "controlnet_tile"))
                # ä¿å­˜ UNet LoRA æƒé‡
                unet_lora_dir = os.path.join(latest_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)
                
                # ä¿å­˜æœ€æ–°å…ƒä¿¡æ¯
                with open(os.path.join(latest_dir, "latest_info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Latest Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"Best Loss: {best_val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)
                    cn_s.save_pretrained(os.path.join(best_dir, "controlnet_scribble"))
                    cn_t.save_pretrained(os.path.join(best_dir, "controlnet_tile"))
                    # ä¿å­˜æœ€ä½³ UNet LoRA æƒé‡
                    unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(unet_lora_dir)
                    
                    # ä¿å­˜æœ€ä½³å…ƒä¿¡æ¯
                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                    
                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)

            global_step += 1

if __name__ == "__main__":
    main()