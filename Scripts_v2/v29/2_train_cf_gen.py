# -*- coding: utf-8 -*-
"""
CF ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬ (V22-4-CF-GEN)
--------------------------------

åŸºäº v22-2 æ”¹è¿›ï¼Œä¸“æ³¨è§£å†³ç”Ÿæˆ CF çš„ä¸‰å¤§ç¼ºé™·ï¼š
1. å¡‘æ–™/æ°´å½©è´¨æ„Ÿ â†’ LoRA æ‰©å±•åˆ° Conv/ResNet å±‚ + æé«˜ rank åˆ° 32
2. å¾®è¡€ç®¡æ¶ˆå¤± â†’ LoRA è¦†ç›–æ›´å¤šå±‚ + è®­ç»ƒæ—¶ä¼ æ„Ÿå™¨å™ªå£°å¢å¼º
3. é»„æ–‘é»‘æ´ â†’ é™ä½ Offset Noise 0.1â†’0.04 + é™ä½ CFG 7.5â†’3.5 + åŠ¨æ€ CFG

ã€v22-4 æ ¸å¿ƒæ”¹åŠ¨ã€‘
A. LoRA target_modules æ‰©å±•åˆ° conv1/conv2/conv_shortcut/time_emb_proj
B. LoRA rank æé«˜åˆ° 32ï¼ˆé»˜è®¤ï¼‰ï¼Œå®¹é‡ç¿»å€
C. Offset Noise é™è‡³ 0.04ï¼Œé¿å…é»„æ–‘è¿‡æš—
D. CFG Scale é™è‡³ 3.5ï¼Œæ¨ç†å¯è§†åŒ–ä½¿ç”¨åŠ¨æ€ CFG
E. è®­ç»ƒæ—¶éšæœºæ³¨å…¥ä¼ æ„Ÿå™¨å™ªå£°ï¼Œæ•™æ¨¡å‹"çœŸå®å›¾åƒæœ‰é¢—ç²’æ„Ÿ"
F. å…³é—­ VAE force_upcast åŠ é€Ÿè®­ç»ƒ
"""

import os
import math
import time
import argparse
import gc
import shutil
import json
import random

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# å¯¼å…¥ CFFA æ•°æ®é›†
import sys

CURRENT_DIR = os.path.dirname(__file__)
sys.path.append(os.path.join(CURRENT_DIR, "../../data/operation_pre_filtered_cffa_augmented"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2  # noqa: E402

# ============ åŠ è½½ VLM ç”Ÿæˆçš„ Caption é¢˜åº“ ============
CAPTION_FILE = os.path.join(CURRENT_DIR, "cf_captions.json")
try:
    with open(CAPTION_FILE, "r", encoding="utf-8") as f:
        CF_CAPTIONS = json.load(f)
    print(f"âœ“ æˆåŠŸåŠ è½½ VLM é¢˜åº“ï¼Œå…±åŒ…å« {len(CF_CAPTIONS)} æ¡æè¿°")
    # æå–æ‰€æœ‰ value ä½œä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œä¾›éšæœºé‡‡æ ·ï¼ˆå¯è§†åŒ–æ—¶ç”¨ï¼‰
    ALL_PROMPTS_LIST = list(CF_CAPTIONS.values())
except FileNotFoundError:
    print(f"[è­¦å‘Š] æœªæ‰¾åˆ° {CAPTION_FILE}ï¼Œå°†å›é€€åˆ°é»˜è®¤ promptï¼")
    CF_CAPTIONS = {}
    ALL_PROMPTS_LIST = ["color fundus photography, retinal image, medical photography"]


# ============ å…¨å±€é…ç½® ============

SIZE = 512
DEVICE = torch.device("cuda")
# æ¨¡å‹è·¯å¾„ï¼ˆä¸ v22/train.py ä¿æŒä¸€è‡´ï¼‰
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
# ã€v22-4ã€‘ä½¿ç”¨ sd-vae-ft-mse VAEï¼ˆé‡å»ºè¯¯å·®æ›´ä½ï¼Œä¿ç•™æ›´å¤šé«˜é¢‘ç»†èŠ‚ï¼‰
VAE_MODEL_PATH = "/data/student/Fengjunming/SDXL_ControlNet/models/sd-vae-ft-mse"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_ctrl_sd15_dual_cf_gen"


# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_caption_key_from_path(cf_path):
    """
    ä»å®Œæ•´çš„ CF å›¾ç‰‡è·¯å¾„æå–é¢˜åº“ä¸­çš„ keyã€‚
    è·¯å¾„æ ¼å¼: .../002_01_aug3/002_01.png
    è¿”å›: 002_01_aug3/002_01
    """
    basename = os.path.basename(cf_path)  # 002_01.png
    dirname = os.path.basename(os.path.dirname(cf_path))  # 002_01_aug3
    filename_no_ext = os.path.splitext(basename)[0]  # 002_01
    return f"{dirname}/{filename_no_ext}"


def encode_dynamic_prompts(prompts: list, tokenizer, text_encoder):
    """
    æ¥æ”¶ä¸€ä¸ªåŒ…å«å¤šä¸ª prompt å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œè¿”å›ç¼–ç åçš„ text embedsã€‚
    æ›¿ä»£åŸæ¥çš„ get_cf_prompt_embeds å‡½æ•°ï¼Œæ”¯æŒåŠ¨æ€æ–‡æœ¬ã€‚
    """
    inputs = tokenizer(
        prompts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    return text_encoder(inputs.input_ids)[0]


def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡ï¼ˆä¸ v22 ä¸€è‡´ï¼‰ã€‚"""
    if step < 4000:
        return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2


# ============ 2. æŸå¤±ï¼ˆç›´æ¥å¤ç”¨ v22 é€»è¾‘ï¼‰ ============

def _gaussian_kernel_1d(kernel_size: int, sigma: float, device, dtype):
    half = kernel_size // 2
    coords = torch.arange(kernel_size, device=device, dtype=dtype) - half
    gauss = torch.exp(-0.5 * (coords / sigma) ** 2)
    return gauss / gauss.sum()


def gaussian_blur_latent(x, kernel_size=7, sigma=1.5):
    C = x.shape[1]
    k = _gaussian_kernel_1d(kernel_size, sigma, x.device, x.dtype)
    pad = kernel_size // 2
    # æ°´å¹³æ–¹å‘
    kw = k.view(1, 1, 1, kernel_size).expand(C, 1, 1, kernel_size)
    x = F.conv2d(x, kw, padding=(0, pad), groups=C)
    # å‚ç›´æ–¹å‘
    kh = k.view(1, 1, kernel_size, 1).expand(C, 1, kernel_size, 1)
    x = F.conv2d(x, kh, padding=(pad, 0), groups=C)
    return x


def compute_hf_texture_loss(pred_x0, gt_x0, kernel_size=7, sigma=1.5):
    pred_blur = gaussian_blur_latent(pred_x0, kernel_size, sigma)
    gt_blur = gaussian_blur_latent(gt_x0, kernel_size, sigma)
    pred_hf = pred_x0 - pred_blur
    gt_hf = gt_x0 - gt_blur
    return F.l1_loss(pred_hf, gt_hf)


def compute_total_loss(noise_pred, noise, noisy_latents, latents,
                       alphas_cumprod, timesteps, hf_lambda=0.5):
    """
    ä¸ v22 ç›¸åŒï¼šå™ªå£° MSE + latent é«˜é¢‘ L1ã€‚
    """
    loss_mse = F.mse_loss(noise_pred, noise)

    alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(noisy_latents.device)
    pred_x0 = (noisy_latents - (1.0 - alpha_t).sqrt() * noise_pred) / (alpha_t.sqrt() + 1e-8)
    pred_x0 = pred_x0.clamp(-10.0, 10.0)

    loss_hf = compute_hf_texture_loss(pred_x0, latents)

    total = loss_mse + hf_lambda * loss_hf
    return total, loss_mse.item(), loss_hf.item()


# ============ 3. éªŒè¯ä¸å¯è§†åŒ– ============

VAL_TIMESTEPS = [200, 500, 800]


def evaluate_cf(val_loader, vae, unet, noise_scheduler, tokenizer, text_encoder, args):
    """
    éªŒè¯ CF ç”Ÿæˆæ¨¡å‹ï¼šåœ¨å›ºå®šæ—¶é—´æ­¥ä¸Šè¯„ä¼°å™ªå£°é¢„æµ‹ MSEã€‚
    """
    if hasattr(unet, "eval"):
        unet.eval()

    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cf, _, cp, _ = batch  # CFFADataset_v2: (cond_tile=CF, tgt=FA, cp=CFè·¯å¾„, ...)
            cf = cf.to(DEVICE)
            b = cf.shape[0]

            latents = vae.encode(cf).latent_dist.sample() * vae.config.scaling_factor
            
            # [ä¿®æ”¹] åŠ¨æ€æŸ¥è¡¨è·å–ä¸“å± Prompt
            batch_prompts = []
            for path in cp:
                key = get_caption_key_from_path(path)
                desc = CF_CAPTIONS.get(key, "color fundus photography, retinal image, medical photography")
                batch_prompts.append(desc)
            prompt_embeds = encode_dynamic_prompts(batch_prompts, tokenizer, text_encoder)

            sample_losses = []
            for t_val in VAL_TIMESTEPS:
                timesteps = torch.full((b,), t_val, device=DEVICE, dtype=torch.long)
                noise = torch.randn_like(latents)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                if hasattr(unet, "base_model"):
                    noise_pred = unet.base_model(
                        sample=noisy_latents,
                        timestep=timesteps,
                        encoder_hidden_states=prompt_embeds,
                        return_dict=False,
                    )[0]
                else:
                    noise_pred = unet(
                        noisy_latents, timesteps, prompt_embeds,
                    ).sample

                sample_losses.append(F.mse_loss(noise_pred, noise).item())

            val_losses.append(np.mean(sample_losses))

    if hasattr(unet, "train"):
        unet.train()

    torch.cuda.empty_cache()
    return np.mean(val_losses)


@torch.no_grad()
def visualize_random_cf(unet, vae, tokenizer, text_encoder, uncond_embeds,
                        num_samples: int, out_dir: str, steps: int = 50, cfg_scale: float = 3.5):
    """
    ä»çº¯å™ªå£°ç”Ÿæˆè‹¥å¹² CF å›¾åƒï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ã€‚
    ç›®å½•ç»“æ„: out_dir/pair_XX/cf.png
    
    æ·»åŠ äº† CFG æ”¯æŒï¼Œä½¿ç”¨é¢„è®¡ç®—çš„æ— æ¡ä»¶ embeddingã€‚
    """
    os.makedirs(out_dir, exist_ok=True)

    if hasattr(unet, "eval"):
        unet.eval()
    if hasattr(vae, "eval"):
        vae.eval()
    if hasattr(text_encoder, "eval"):
        text_encoder.eval()

    scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    scheduler.set_timesteps(steps)

    in_channels = (
        unet.base_model.config.in_channels
        if hasattr(unet, "base_model")
        else unet.config.in_channels
    )
    latent_shape = (1, in_channels, SIZE // 8, SIZE // 8)

    def tensor_to_pil(x: torch.Tensor) -> Image.Image:
        x = (x.clamp(-1, 1) + 1) / 2.0
        x = x.cpu().permute(1, 2, 0).numpy()
        x = (x * 255).round().astype("uint8")
        return Image.fromarray(x)

    for idx in range(num_samples):
        # [ä¿®æ”¹] éšæœºæŠ½å–ä¸€æ¡ä¸“å± Prompt
        current_prompt_str = random.choice(ALL_PROMPTS_LIST)
        prompt_cf = encode_dynamic_prompts([current_prompt_str], tokenizer, text_encoder)
        
        z = torch.randn(latent_shape, device=DEVICE)
        latents = z.clone()
        
        # ã€v22-4ã€‘è·å–æ€»æ­¥æ•°ç”¨äºåŠ¨æ€ CFG
        t_max = scheduler.config.num_train_timesteps

        for t in scheduler.timesteps:
            # ã€v22-4 åŠ¨æ€ CFGã€‘å‰æœŸé«˜ CFG ç¡®å®šç»“æ„ï¼ŒåæœŸç¨ä½ CFG ä¿ç•™çº¹ç†
            # èŒƒå›´: cfg_scale*0.7 ~ cfg_scale*1.0 (å¦‚ cfg=3.5 â†’ 2.45~3.5)
            dynamic_cfg = cfg_scale * (0.7 + 0.3 * (t.float() / t_max))
            
            # [CFG] åŒæ—¶è®¡ç®—æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
            if hasattr(unet, "base_model"):
                # æ¡ä»¶é¢„æµ‹
                noise_pred_text = unet.base_model(
                    sample=latents,
                    timestep=t,
                    encoder_hidden_states=prompt_cf,
                    return_dict=False,
                )[0]
                # æ— æ¡ä»¶é¢„æµ‹
                noise_pred_uncond = unet.base_model(
                    sample=latents,
                    timestep=t,
                    encoder_hidden_states=uncond_embeds,
                    return_dict=False,
                )[0]
            else:
                noise_pred_text = unet(
                    latents,
                    t,
                    prompt_cf,
                ).sample
                noise_pred_uncond = unet(
                    latents,
                    t,
                    uncond_embeds,
                ).sample
            
            # CFG å…¬å¼ï¼ˆä½¿ç”¨åŠ¨æ€ scaleï¼‰
            noise_pred = noise_pred_uncond + dynamic_cfg * (noise_pred_text - noise_pred_uncond)

            latents = scheduler.step(noise_pred, t, latents).prev_sample

        latents_final = latents / vae.config.scaling_factor
        imgs_cf = vae.decode(latents_final).sample
        img_cf = tensor_to_pil(imgs_cf[0])

        pair_dir = os.path.join(out_dir, f"pair_{idx:02d}")
        os.makedirs(pair_dir, exist_ok=True)
        img_cf.save(os.path.join(pair_dir, "cf.png"))


# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-n", "--name", default="cf_gen_v22_4")
    parser.add_argument("--max_steps", type=int, default=15000)
    # ã€v22-4ã€‘LoRA rank/alpha æé«˜åˆ° 32ï¼Œå®¹é‡ç¿»å€ä»¥æ•æ‰æ›´å¤šçº¹ç†å’Œè¡€ç®¡ç»†èŠ‚
    parser.add_argument("--unet_lora_rank", type=int, default=32, help="UNet LoRA rankï¼ˆv22-4: 32ï¼‰")
    parser.add_argument("--unet_lora_alpha", type=int, default=32, help="UNet LoRA alphaï¼ˆv22-4: 32ï¼‰")
    # ã€v22-4ã€‘Offset Noise é™è‡³ 0.04ï¼Œé¿å…é»„æ–‘åŒºäº§ç”Ÿä¸è‡ªç„¶çš„é»‘æ´
    parser.add_argument("--offset_noise_strength", type=float, default=0.04, help="Offset noiseï¼ˆv22-4: 0.04ï¼‰")
    parser.add_argument("--hf_lambda", type=float, default=0.5, help="é«˜é¢‘çº¹ç†æŸå¤±æƒé‡ï¼Œæ¨è 0.3~1.0")
    parser.add_argument("--uncond_prob", type=float, default=0.1, help="è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒæ–‡æœ¬æ¡ä»¶çš„æ¦‚ç‡ï¼ˆç”¨äº CFGï¼‰ï¼Œæ¨è 0.1")
    # ã€v22-4ã€‘CFG Scale é™è‡³ 3.5ï¼Œå‡å°‘åˆ†å¸ƒå¤–åŒºåŸŸçš„ä¼ªå½±æ”¾å¤§
    parser.add_argument("--cfg_scale", type=float, default=3.5, help="å¯è§†åŒ– CFG scaleï¼ˆv22-4: 3.5ï¼‰")
    # ã€v22-4 æ–°å¢ã€‘è®­ç»ƒæ—¶ä¼ æ„Ÿå™¨å™ªå£°å¢å¼ºæ¦‚ç‡
    parser.add_argument("--sensor_noise_prob", type=float, default=0.5, help="è®­ç»ƒæ—¶å¯¹ CF æ³¨å…¥ä¼ æ„Ÿå™¨å™ªå£°çš„æ¦‚ç‡")
    parser.add_argument("--sensor_noise_max", type=float, default=0.04, help="ä¼ æ„Ÿå™¨å™ªå£°æœ€å¤§å¼ºåº¦")
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½ï¼ˆä»…ä½¿ç”¨ CFFAï¼‰
    train_ds = CFFADataset_v2(split='train', mode="cf2fa")
    val_ds = CFFADataset_v2(split='test', mode="cf2fa")

    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)

    # 2. æ¨¡å‹åŠ è½½
    print("\n========== æ¨¡å‹åŠ è½½ ==========")
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    # ã€v22-4ã€‘ä½¿ç”¨ sd-vae-ft-mseï¼ˆé‡å»ºè¯¯å·®æ›´ä½ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰
    vae = AutoencoderKL.from_pretrained(VAE_MODEL_PATH).to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)

    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    
    # é¢„è®¡ç®—æ— æ¡ä»¶ embeddingï¼ˆç”¨äº CFG è®­ç»ƒå’Œå¯è§†åŒ–ï¼‰
    print(f"\n========== CFG é…ç½® ==========")
    print(f"  - æ— æ¡ä»¶è®­ç»ƒæ¦‚ç‡: {args.uncond_prob * 100:.1f}%")
    print(f"  - å¯è§†åŒ– CFG Scale: {args.cfg_scale}")
    uncond_embeds = encode_dynamic_prompts([""], tokenizer, text_encoder)

    print(f"\n========== UNet LoRA é…ç½® ==========")
    unet.requires_grad_(False)

    # ã€v22-4 æ ¸å¿ƒæ”¹è¿›ã€‘LoRA æ‰©å±•åˆ° Conv/ResNet å±‚
    # - Attention å±‚ï¼ˆto_k/q/v/outï¼‰ï¼šæ§åˆ¶è¯­ä¹‰å¸ƒå±€å’Œå…¨å±€ç»“æ„
    # - Conv å±‚ï¼ˆconv1/conv2/conv_shortcutï¼‰ï¼šæ§åˆ¶åƒç´ çº§çº¹ç†å’Œé¢—ç²’æ„Ÿ
    # - æ—¶é—´åµŒå…¥å±‚ï¼ˆtime_emb_projï¼‰ï¼šæ§åˆ¶ä¸åŒå™ªå£°æ°´å¹³ä¸‹çš„è¡Œä¸º
    target_modules = [
        # Attention æŠ•å½±å±‚ â€” è¯­ä¹‰/ç»“æ„
        "to_k", "to_q", "to_v", "to_out.0",
        # ResNet å·ç§¯å±‚ â€” çº¹ç†/é¢—ç²’æ„Ÿï¼ˆv22-4 æ–°å¢ï¼Œæœ€å…³é”®çš„æ”¹è¿›ï¼‰
        "conv1", "conv2", "conv_shortcut",
        # æ—¶é—´åµŒå…¥æŠ•å½± â€” å™ªå£°æ°´å¹³å“åº”
        "time_emb_proj",
    ]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )

    unet = get_peft_model(unet, lora_config)

    unet_lora_params = [p for p in unet.parameters() if p.requires_grad]
    unet_lora_num = sum(p.numel() for p in unet_lora_params)
    unet_total_num = sum(p.numel() for p in unet.parameters())

    print(f"âœ“ UNet LoRA å·²åº”ç”¨")
    print(f"  - Rank: {args.unet_lora_rank}, Alpha: {args.unet_lora_alpha}")
    print(f"  - ç›®æ ‡æ¨¡å—: {target_modules}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°: {unet_lora_num:,} ({unet_lora_num/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°: {unet_total_num:,} ({unet_total_num/1e6:.2f}M)")
    print(f"  - å‚æ•°å æ¯”: {unet_lora_num/unet_total_num*100:.2f}%")

    total_trainable = unet_lora_num
    print(f"\nâœ“ æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,} ({total_trainable/1e6:.2f}M)")

    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    all_trainable_params = unet_lora_params
    optimizer = torch.optim.AdamW(all_trainable_params, lr=5e-5, weight_decay=1e-2)

    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=5e-5, weight_decay=1e-2)")
    print(f"  - Offset Noise å¼ºåº¦: {args.offset_noise_strength}")

    # 3. è®­ç»ƒçŠ¶æ€
    global_step = 0
    best_val_loss = float('inf')
    start_time = time.time()
    loss_accumulator = []

    print(f"\n========== å¼€å§‹è®­ç»ƒ CF ç”Ÿæˆæ¨¡å‹ (v22-4) ==========")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_ds)} (å…¨é‡ï¼Œå›ºå®šæ—¶é—´æ­¥ {VAL_TIMESTEPS})")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}")
    print(f"ä¼ æ„Ÿå™¨å™ªå£°å¢å¼º: prob={args.sensor_noise_prob}, max={args.sensor_noise_max}\n")

    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps:
                break

            cf, _, cp, _ = batch  # cp æ˜¯å½“å‰æ‰¹æ¬¡å›¾ç‰‡çš„ç»å¯¹è·¯å¾„å…ƒç»„
            cf = cf.to(DEVICE)
            b = cf.shape[0]

            # ã€v22-4 æ–°å¢ã€‘è®­ç»ƒæ—¶éšæœºæ³¨å…¥ä¼ æ„Ÿå™¨å™ªå£°
            # è®©æ¨¡å‹å­¦ä¹ åˆ°çœŸå®çœ¼åº•å›¾çš„é¢—ç²’æ„Ÿï¼Œé¿å…ç”Ÿæˆè¿‡äºå¹³æ»‘çš„å¡‘æ–™è´¨æ„Ÿ
            if random.random() < args.sensor_noise_prob:
                noise_level = random.uniform(0.005, args.sensor_noise_max)
                sensor_noise = torch.randn_like(cf) * noise_level
                cf = (cf + sensor_noise).clamp(-1, 1)

            # VAE ç¼–ç 
            latents = vae.encode(cf).latent_dist.sample() * vae.config.scaling_factor

            # [ä¿®æ”¹] åŠ¨æ€æŸ¥è¡¨è·å–ä¸“å± Prompt
            batch_prompts = []
            for path in cp:
                key = get_caption_key_from_path(path)
                desc = CF_CAPTIONS.get(key, "color fundus photography, retinal image, medical photography")
                batch_prompts.append(desc)
                
                # [è°ƒè¯•ç”¨] æ‰“å°å‰å‡ ä¸ª step çš„ promptï¼Œç¡®ä¿çœŸçš„åŠ è½½æˆåŠŸäº†
                if global_step < 2 and len(batch_prompts) == 1:
                    is_matched = key in CF_CAPTIONS
                    status = "âœ“ åŒ¹é…æˆåŠŸ" if is_matched else "âœ— ä½¿ç”¨ fallback"
                    print(f"\n[DEBUG Step {global_step}] {status}")
                    print(f"  è·¯å¾„: {path}")
                    print(f"  Key: {key}")
                    print(f"  Prompt: {desc[:80]}...")
            
            # [CFG è®­ç»ƒ] éšæœºä¸¢å¼ƒæ–‡æœ¬æ¡ä»¶ï¼Œè®©æ¨¡å‹å­¦ä¹ æ— æ¡ä»¶ç”Ÿæˆ
            if random.random() < args.uncond_prob:
                batch_prompts = [""] * len(batch_prompts)
                if global_step < 2:
                    print(f"  [CFG] å½“å‰æ‰¹æ¬¡ä½¿ç”¨æ— æ¡ä»¶è®­ç»ƒï¼ˆç©ºæ–‡æœ¬ï¼‰")
            
            # è½¬æ¢ä¸º Embeddings
            prompt_embeds = encode_dynamic_prompts(batch_prompts, tokenizer, text_encoder)

            # Offset Noise
            noise = torch.randn_like(latents)
            if args.offset_noise_strength > 0:
                noise += args.offset_noise_strength * torch.randn(
                    latents.shape[0], latents.shape[1], 1, 1, device=latents.device
                )

            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE
            ).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            # UNet å‰å‘
            if hasattr(unet, "base_model"):
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    return_dict=False,
                )[0]
            else:
                noise_pred = unet(
                    noisy_latents, timesteps, prompt_embeds,
                ).sample

            # æŸå¤±
            loss, loss_mse_val, loss_hf_val = compute_total_loss(
                noise_pred, noise, noisy_latents, latents,
                noise_scheduler.alphas_cumprod, timesteps,
                hf_lambda=args.hf_lambda,
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups:
                param_group['lr'] = current_lr

            loss_accumulator.append((loss.item(), loss_mse_val, loss_hf_val))

            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                arr = np.array(loss_accumulator)
                avg_loss, avg_mse, avg_hf = arr[:, 0].mean(), arr[:, 1].mean(), arr[:, 2].mean()
                loss_accumulator = []

                t_val = timesteps[0].item()
                msg = (f"[cf-gen] Step {global_step:5d}/{args.max_steps} | "
                       f"lr:{current_lr:.2e} | loss:{avg_loss:.4f} "
                       f"(mse:{avg_mse:.4f} hf:{avg_hf:.4f}) | t={t_val:3d} | "
                       f"{elapsed:.1f}s")
                print(msg)
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")

                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯ + å¯è§†åŒ– + checkpoint
            if global_step % 500 == 0:
                val_loss = evaluate_cf(val_loader, vae, unet, noise_scheduler, tokenizer, text_encoder, args)

                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")

                # å¯è§†åŒ–éšæœºç”Ÿæˆçš„ CF å›¾åƒ
                vis_dir = os.path.join(out_dir, f"step_{global_step:06d}_random_cf")
                print(f"[å¯è§†åŒ–] åœ¨ {vis_dir} ç”Ÿæˆ 10 å¼ éšæœº CF å›¾åƒï¼ˆCFG scale={args.cfg_scale}ï¼‰...")
                visualize_random_cf(unet, vae, tokenizer, text_encoder, uncond_embeds, 10, vis_dir, 50, args.cfg_scale)

                # latest checkpoints (æ»šåŠ¨ä¿ç•™æœ€è¿‘ 3 ä¸ª)
                latest_root = os.path.join(out_dir, "latest_checkpoints")
                os.makedirs(latest_root, exist_ok=True)
                latest_step_dir = os.path.join(latest_root, f"step_{global_step:06d}")
                os.makedirs(latest_step_dir, exist_ok=True)

                unet_lora_dir = os.path.join(latest_step_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)

                with open(os.path.join(latest_step_dir, "info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Offset Noise: {args.offset_noise_strength}\n")

                # æ»šåŠ¨åˆ é™¤å¤šä½™çš„ latest
                subdirs = sorted(
                    [d for d in os.listdir(latest_root) if d.startswith("step_")]
                )
                if len(subdirs) > 3:
                    for old in subdirs[:-3]:
                        shutil.rmtree(os.path.join(latest_root, old))

                # best checkpoint
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)

                    best_unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(best_unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(best_unet_lora_dir)

                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Offset Noise: {args.offset_noise_strength}\n")

                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„ CF ç”Ÿæˆæ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)

            global_step += 1


if __name__ == "__main__":
    main()

