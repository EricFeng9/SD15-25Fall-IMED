# -*- coding: utf-8 -*-
"""
Dual-UNet CF-FA Generation Training Script (v27)
-------------------------------------------------

ã€æ ¸å¿ƒæ”¹è¿› - è§£å†³v23/v24/v26çš„è‡´å‘½é—®é¢˜ã€‘

é—®é¢˜è¯Šæ–­:
1. v26/v23/v24 ä½¿ç”¨ Shared Self-Attention å¯¼è‡´FAå›¾å¤±å»è‡ªèº«ç»“æ„,åªèƒ½ç”Ÿæˆå™ªç‚¹
2. v26 å°†CFå’ŒFAç”¨åŒä¸€å™ªå£°ã€åŒä¸€UNetå¤„ç†,ä½†ä¸¤è€…åˆ†å¸ƒå·®å¼‚å·¨å¤§(å½©è‰² vs é»‘ç™½é«˜å¯¹æ¯”åº¦)
3. v25 åˆ†è¾¨ç‡å¤ªä½(256x512)

v27 è§£å†³æ–¹æ¡ˆ:
1. âŒ ä¸ä½¿ç”¨ Shared Self-Attention (è¿™æ˜¯æ¯’è¯!)
2. âœ… ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„ UNet LoRA: unet_cf å’Œ unet_fa
3. âœ… åœ¨ latent space æ·»åŠ ç»“æ„ä¸€è‡´æ€§çº¦æŸ (è¡€ç®¡ç»“æ„å¯¹é½)
4. âœ… 512x512 å…¨åˆ†è¾¨ç‡è®­ç»ƒ
5. âœ… åˆ†åˆ«ä¸ºCFå’ŒFAä½¿ç”¨ä¸åŒçš„å™ªå£°å’Œtimestep,é¿å…å¼ºåˆ¶è€¦åˆ

è®­ç»ƒç›®æ ‡:
- ä»çº¯å™ªå£°ç”Ÿæˆç»“æ„å…¨æ–°ã€ä½†é£æ ¼çœŸå®çš„ CF-FA é…å¯¹å›¾åƒ
- CFå’ŒFAä¹‹é—´æœ‰ä¸€è‡´çš„è¡€ç®¡ç»“æ„(é€šè¿‡ç»“æ„ä¸€è‡´æ€§æŸå¤±çº¦æŸ)
- æ¯ä¸ªæ¨¡æ€ä¿æŒè‡ªå·±çš„çº¹ç†å’Œäº®åº¦åˆ†å¸ƒç‰¹å¾
"""

import os
import math
import time
import argparse
import shutil

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from PIL import Image
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# æ•°æ®é›†
import sys
CURRENT_DIR = os.path.dirname(__file__)
sys.path.append(os.path.join(CURRENT_DIR, "../../data/operation_pre_filtered_cffa"))
from operation_pre_filtered_cffa_dataset import CFFADataset

# ============ å…¨å±€é…ç½® ============
SIZE = 512
DEVICE = torch.device("cuda")
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sd15-diffusers"
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_dual_unet_cffa_v27"


# ============ è¾…åŠ©å‡½æ•° ============

def get_cf_prompt_embeds(bs, tokenizer, text_encoder):
    """CF (å½©è‰²çœ¼åº•) çš„ prompt"""
    prompt = "color fundus photography, retinal image, medical photography, natural lighting"
    prompts = [prompt] * bs
    inputs = tokenizer(
        prompts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    return text_encoder(inputs.input_ids)[0]


def get_fa_prompt_embeds(bs, tokenizer, text_encoder):
    """FA (è§å…‰è¡€ç®¡é€ å½±) çš„ prompt"""
    prompt = "fluorescein angiography, retinal fundus vessel, medical imaging, high contrast, monochrome"
    prompts = [prompt] * bs
    inputs = tokenizer(
        prompts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    return text_encoder(inputs.input_ids)[0]


def get_dynamic_lr(step, max_steps, base_lr=2e-5, min_lr=5e-6):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡ï¼ˆé™ä½åˆå§‹lré¿å…éœ‡è¡ï¼‰"""
    if step < 4000:
        return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2


# ============ æŸå¤±å‡½æ•° ============

def _gaussian_kernel_1d(kernel_size: int, sigma: float, device, dtype):
    """ç”Ÿæˆ1Dé«˜æ–¯æ ¸"""
    half = kernel_size // 2
    coords = torch.arange(kernel_size, device=device, dtype=dtype) - half
    gauss = torch.exp(-0.5 * (coords / sigma) ** 2)
    return gauss / gauss.sum()


def gaussian_blur_latent(x, kernel_size=7, sigma=1.5):
    """å¯¹latentåšé«˜æ–¯æ¨¡ç³Š"""
    C = x.shape[1]
    k = _gaussian_kernel_1d(kernel_size, sigma, x.device, x.dtype)
    pad = kernel_size // 2
    # æ°´å¹³
    kw = k.view(1, 1, 1, kernel_size).expand(C, 1, 1, kernel_size)
    x = F.conv2d(x, kw, padding=(0, pad), groups=C)
    # å‚ç›´
    kh = k.view(1, 1, kernel_size, 1).expand(C, 1, kernel_size, 1)
    x = F.conv2d(x, kh, padding=(pad, 0), groups=C)
    return x


def compute_hf_texture_loss(pred_x0, gt_x0, kernel_size=7, sigma=1.5):
    """é«˜é¢‘çº¹ç†æŸå¤± - åœ¨latentç©ºé—´"""
    pred_blur = gaussian_blur_latent(pred_x0, kernel_size, sigma)
    gt_blur = gaussian_blur_latent(gt_x0, kernel_size, sigma)
    pred_hf = pred_x0 - pred_blur
    gt_hf = gt_x0 - gt_blur
    return F.l1_loss(pred_hf, gt_hf)


def extract_structure_map(latent):
    """
    ä»latentä¸­æå–ç»“æ„å›¾(ç”¨äºç»“æ„ä¸€è‡´æ€§çº¦æŸ)
    ä½¿ç”¨Sobelç®—å­æå–è¾¹ç¼˜/æ¢¯åº¦ä½œä¸ºç»“æ„è¡¨ç¤º
    """
    # Sobel ç®—å­
    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                           dtype=latent.dtype, device=latent.device).view(1, 1, 3, 3)
    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                           dtype=latent.dtype, device=latent.device).view(1, 1, 3, 3)
    
    # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«è®¡ç®—æ¢¯åº¦
    B, C, H, W = latent.shape
    grad_x = F.conv2d(latent.view(B*C, 1, H, W), sobel_x, padding=1).view(B, C, H, W)
    grad_y = F.conv2d(latent.view(B*C, 1, H, W), sobel_y, padding=1).view(B, C, H, W)
    
    # æ¢¯åº¦å¹…å€¼ä½œä¸ºç»“æ„å›¾
    structure = torch.sqrt(grad_x**2 + grad_y**2 + 1e-8)
    return structure


def compute_structure_consistency_loss(lat_cf, lat_fa):
    """
    ç»“æ„ä¸€è‡´æ€§æŸå¤± - ç¡®ä¿CFå’ŒFAæœ‰ç›¸ä¼¼çš„è¡€ç®¡ç»“æ„
    """
    struct_cf = extract_structure_map(lat_cf)
    struct_fa = extract_structure_map(lat_fa)
    
    # ç›´æ¥è®¡ç®—L1ï¼Œä¸åšå½’ä¸€åŒ–ï¼ˆä¿ç•™åŸå§‹æ¢¯åº¦å¼ºåº¦ä¿¡æ¯ï¼‰
    return F.l1_loss(struct_cf, struct_fa)


def compute_single_modality_loss(noise_pred, noise, noisy_latents, latents,
                                  alphas_cumprod, timesteps, hf_lambda=0.5):
    """
    å•ä¸ªæ¨¡æ€çš„æŸå¤±: MSE + é«˜é¢‘çº¹ç†æŸå¤±
    """
    # MSE å™ªå£°æŸå¤±
    loss_mse = F.mse_loss(noise_pred, noise)
    
    # ä»noise_predåæ¨x0
    alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(noisy_latents.device)
    pred_x0 = (noisy_latents - (1.0 - alpha_t).sqrt() * noise_pred) / (alpha_t.sqrt() + 1e-8)
    pred_x0 = pred_x0.clamp(-10.0, 10.0)
    
    # é«˜é¢‘çº¹ç†æŸå¤±
    loss_hf = compute_hf_texture_loss(pred_x0, latents)
    
    total = loss_mse + hf_lambda * loss_hf
    return total, loss_mse.item(), loss_hf.item(), pred_x0


# ============ éªŒè¯å‡½æ•° ============

VAL_TIMESTEPS = [200, 500, 800]


def tensor_to_pil(x: torch.Tensor) -> Image.Image:
    """Tensorè½¬PILå›¾åƒ"""
    x = (x.clamp(-1, 1) + 1) / 2.0
    x = x.cpu().permute(1, 2, 0).numpy()
    x = (x * 255).round().astype("uint8")
    return Image.fromarray(x)


@torch.no_grad()
def evaluate_dual_unet(val_loader, vae, unet_cf, unet_fa, noise_scheduler, 
                       tokenizer, text_encoder, args):
    """éªŒè¯å‡½æ•° - åœ¨å›ºå®šæ—¶é—´æ­¥ä¸Šè¯„ä¼°"""
    if hasattr(unet_cf, "eval"):
        unet_cf.eval()
    if hasattr(unet_fa, "eval"):
        unet_fa.eval()
    
    losses = []
    for batch in val_loader:
        cf, fa, _, _ = batch
        cf, fa = cf.to(DEVICE), fa.to(DEVICE)
        b = cf.shape[0]
        
        # VAEç¼–ç 
        lat_cf = vae.encode(cf).latent_dist.sample() * vae.config.scaling_factor
        lat_fa = vae.encode(fa).latent_dist.sample() * vae.config.scaling_factor
        
        # Prompt
        prompt_cf = get_cf_prompt_embeds(b, tokenizer, text_encoder)
        prompt_fa = get_fa_prompt_embeds(b, tokenizer, text_encoder)
        
        sample_losses = []
        for t_val in VAL_TIMESTEPS:
            timesteps = torch.full((b,), t_val, device=DEVICE, dtype=torch.long)
            
            # CFåˆ†æ”¯
            noise_cf = torch.randn_like(lat_cf)
            lat_cf_t = noise_scheduler.add_noise(lat_cf, noise_cf, timesteps)
            
            if hasattr(unet_cf, "base_model"):
                noise_pred_cf = unet_cf.base_model(
                    sample=lat_cf_t,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_cf,
                    return_dict=False,
                )[0]
            else:
                noise_pred_cf = unet_cf(lat_cf_t, timesteps, prompt_cf).sample
            
            loss_cf = F.mse_loss(noise_pred_cf, noise_cf)
            
            # FAåˆ†æ”¯
            noise_fa = torch.randn_like(lat_fa)
            lat_fa_t = noise_scheduler.add_noise(lat_fa, noise_fa, timesteps)
            
            if hasattr(unet_fa, "base_model"):
                noise_pred_fa = unet_fa.base_model(
                    sample=lat_fa_t,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_fa,
                    return_dict=False,
                )[0]
            else:
                noise_pred_fa = unet_fa(lat_fa_t, timesteps, prompt_fa).sample
            
            loss_fa = F.mse_loss(noise_pred_fa, noise_fa)
            
            sample_losses.append((loss_cf.item() + loss_fa.item()) / 2)
        
        losses.append(np.mean(sample_losses))
    
    if hasattr(unet_cf, "train"):
        unet_cf.train()
    if hasattr(unet_fa, "train"):
        unet_fa.train()
    
    torch.cuda.empty_cache()
    return float(np.mean(losses))


@torch.no_grad()
def visualize_random_pairs(unet_cf, unet_fa, vae, tokenizer, text_encoder,
                           num_samples: int, out_dir: str, steps: int = 50):
    """
    ä»çº¯å™ªå£°ç”ŸæˆCF-FAé…å¯¹å›¾åƒ
    å…³é”®: ä½¿ç”¨ç»“æ„ä¸€è‡´æ€§å¼•å¯¼,è®©CFå’ŒFAåœ¨å»å™ªè¿‡ç¨‹ä¸­é€æ­¥å¯¹é½ç»“æ„
    """
    os.makedirs(out_dir, exist_ok=True)
    
    if hasattr(unet_cf, "eval"):
        unet_cf.eval()
    if hasattr(unet_fa, "eval"):
        unet_fa.eval()
    if hasattr(vae, "eval"):
        vae.eval()
    if hasattr(text_encoder, "eval"):
        text_encoder.eval()
    
    prompt_cf = get_cf_prompt_embeds(1, tokenizer, text_encoder)
    prompt_fa = get_fa_prompt_embeds(1, tokenizer, text_encoder)
    
    scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    scheduler.set_timesteps(steps)
    
    in_channels = (
        unet_cf.base_model.config.in_channels
        if hasattr(unet_cf, "base_model")
        else unet_cf.config.in_channels
    )
    
    for idx in range(num_samples):
        # ä»åŒä¸€ä¸ªå™ªå£°åˆå§‹åŒ–(ä¿è¯åˆå§‹ç»“æ„ç›¸ä¼¼)
        z0 = torch.randn(1, in_channels, SIZE // 8, SIZE // 8, device=DEVICE)
        lat_cf = z0.clone()
        lat_fa = z0.clone()
        
        for t in scheduler.timesteps:
            t_tensor = torch.full((1,), t, device=DEVICE, dtype=torch.long)
            
            # CFåˆ†æ”¯å»å™ª
            if hasattr(unet_cf, "base_model"):
                noise_pred_cf = unet_cf.base_model(
                    sample=lat_cf,
                    timestep=t_tensor,
                    encoder_hidden_states=prompt_cf,
                    return_dict=False,
                )[0]
            else:
                noise_pred_cf = unet_cf(lat_cf, t_tensor, prompt_cf).sample
            
            lat_cf = scheduler.step(noise_pred_cf, t, lat_cf).prev_sample
            
            # FAåˆ†æ”¯å»å™ª
            if hasattr(unet_fa, "base_model"):
                noise_pred_fa = unet_fa.base_model(
                    sample=lat_fa,
                    timestep=t_tensor,
                    encoder_hidden_states=prompt_fa,
                    return_dict=False,
                )[0]
            else:
                noise_pred_fa = unet_fa(lat_fa, t_tensor, prompt_fa).sample
            
            lat_fa = scheduler.step(noise_pred_fa, t, lat_fa).prev_sample
        
        # è§£ç 
        lat_cf_final = lat_cf / vae.config.scaling_factor
        lat_fa_final = lat_fa / vae.config.scaling_factor
        
        img_cf = vae.decode(lat_cf_final).sample[0]
        img_fa = vae.decode(lat_fa_final).sample[0]
        
        img_cf_pil = tensor_to_pil(img_cf)
        img_fa_pil = tensor_to_pil(img_fa)
        
        pair_dir = os.path.join(out_dir, f"pair_{idx:02d}")
        os.makedirs(pair_dir, exist_ok=True)
        img_cf_pil.save(os.path.join(pair_dir, "cf.png"))
        img_fa_pil.save(os.path.join(pair_dir, "fa.png"))


# ============ ä¸»è®­ç»ƒå‡½æ•° ============

def main():
    parser = argparse.ArgumentParser(description="Dual-UNet CF-FA ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬ v27")
    parser.add_argument("-n", "--name", default="dual_unet_cffa_v27")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--unet_lora_rank", type=int, default=16)
    parser.add_argument("--unet_lora_alpha", type=int, default=16)
    parser.add_argument("--offset_noise_strength", type=float, default=0.1)
    parser.add_argument("--hf_lambda", type=float, default=0.5, help="é«˜é¢‘çº¹ç†æŸå¤±æƒé‡")
    parser.add_argument("--struct_lambda", type=float, default=0.3, help="ç»“æ„ä¸€è‡´æ€§æŸå¤±æƒé‡")
    args = parser.parse_args()
    
    out_dir = os.path.join(OUT_ROOT, args.name)
    os.makedirs(out_dir, exist_ok=True)
    
    # æ•°æ®åŠ è½½
    print("\n========== æ•°æ®åŠ è½½ ==========")
    # å…ˆåŠ è½½å…¨éƒ¨æ•°æ®
    full_ds = CFFADataset(split="train", mode="cf2fa")  # åŠ è½½æ‰€æœ‰æ•°æ®
    
    # æ‰‹åŠ¨åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (90% train, 10% val)
    total_samples = len(full_ds)
    train_size = int(0.9 * total_samples)
    val_size = total_samples - train_size
    
    train_ds, val_ds = random_split(full_ds, [train_size, val_size], 
                                     generator=torch.Generator().manual_seed(42))
    
    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_ds)}")
    
    # æ¨¡å‹åŠ è½½
    print("\n========== æ¨¡å‹åŠ è½½ ==========")
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    
    # åŠ è½½ä¸¤ä¸ªç‹¬ç«‹çš„UNet (å…±äº«é¢„è®­ç»ƒæƒé‡,ä½†ç‹¬ç«‹è®­ç»ƒ)
    unet_cf = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    unet_fa = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)
    
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet_cf.requires_grad_(False)
    unet_fa.requires_grad_(False)
    
    print("\n========== UNet LoRA é…ç½® ==========")
    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )
    
    # ä¸ºä¸¤ä¸ªUNetåˆ†åˆ«åº”ç”¨LoRA
    unet_cf = get_peft_model(unet_cf, lora_config)
    unet_fa = get_peft_model(unet_fa, lora_config)
    
    trainable_cf = [p for p in unet_cf.parameters() if p.requires_grad]
    trainable_fa = [p for p in unet_fa.parameters() if p.requires_grad]
    n_trainable_cf = sum(p.numel() for p in trainable_cf)
    n_trainable_fa = sum(p.numel() for p in trainable_fa)
    
    print(f"âœ“ UNet-CF LoRA å¯è®­ç»ƒå‚æ•°: {n_trainable_cf:,} ({n_trainable_cf/1e6:.2f}M)")
    print(f"âœ“ UNet-FA LoRA å¯è®­ç»ƒå‚æ•°: {n_trainable_fa:,} ({n_trainable_fa/1e6:.2f}M)")
    print(f"âœ“ æ€»å¯è®­ç»ƒå‚æ•°: {(n_trainable_cf + n_trainable_fa):,} ({(n_trainable_cf + n_trainable_fa)/1e6:.2f}M)")
    
    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    optimizer = torch.optim.AdamW(trainable_cf + trainable_fa, lr=5e-5, weight_decay=1e-2)
    
    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=5e-5, weight_decay=1e-2)")
    print(f"  - Offset Noise: {args.offset_noise_strength}")
    print(f"  - é«˜é¢‘æŸå¤±æƒé‡: {args.hf_lambda}")
    print(f"  - ç»“æ„ä¸€è‡´æ€§æƒé‡: {args.struct_lambda}")
    
    # è®­ç»ƒçŠ¶æ€
    global_step = 0
    best_val = float("inf")
    start_time = time.time()
    loss_acc = []
    
    print("\n========== å¼€å§‹è®­ç»ƒ Dual-UNet CF-FA ç”Ÿæˆæ¨¡å‹ ==========")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}\n")
    
    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps:
                break
            
            cf, fa, _, _ = batch
            cf, fa = cf.to(DEVICE), fa.to(DEVICE)
            b = cf.shape[0]
            
            # VAEç¼–ç 
            lat_cf = vae.encode(cf).latent_dist.sample() * vae.config.scaling_factor
            lat_fa = vae.encode(fa).latent_dist.sample() * vae.config.scaling_factor
            
            # ä¸ºCFå’ŒFAç”Ÿæˆå™ªå£°ï¼ˆç‹¬ç«‹ï¼‰å’Œæ—¶é—´æ­¥ï¼ˆå…±äº«ï¼‰
            # å…³é”®ï¼šä½¿ç”¨ç›¸åŒçš„timestepï¼Œè¿™æ ·pred_x0çš„å™ªå£°æ°´å¹³ä¸€è‡´ï¼Œç»“æ„çº¦æŸæ‰æœ‰æ„ä¹‰
            noise_cf = torch.randn_like(lat_cf)
            noise_fa = torch.randn_like(lat_fa)
            
            if args.offset_noise_strength > 0:
                noise_cf = noise_cf + args.offset_noise_strength * torch.randn(
                    lat_cf.shape[0], lat_cf.shape[1], 1, 1, device=lat_cf.device
                )
                noise_fa = noise_fa + args.offset_noise_strength * torch.randn(
                    lat_fa.shape[0], lat_fa.shape[1], 1, 1, device=lat_fa.device
                )
            
            # å…±äº«timestepï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE
            ).long()
            timesteps_cf = timesteps
            timesteps_fa = timesteps
            
            lat_cf_t = noise_scheduler.add_noise(lat_cf, noise_cf, timesteps_cf)
            lat_fa_t = noise_scheduler.add_noise(lat_fa, noise_fa, timesteps_fa)
            
            # Prompt
            prompt_cf = get_cf_prompt_embeds(b, tokenizer, text_encoder)
            prompt_fa = get_fa_prompt_embeds(b, tokenizer, text_encoder)
            
            # CFåˆ†æ”¯å‰å‘
            if hasattr(unet_cf, "base_model"):
                noise_pred_cf = unet_cf.base_model(
                    sample=lat_cf_t,
                    timestep=timesteps_cf,
                    encoder_hidden_states=prompt_cf,
                    return_dict=False,
                )[0]
            else:
                noise_pred_cf = unet_cf(lat_cf_t, timesteps_cf, prompt_cf).sample
            
            # FAåˆ†æ”¯å‰å‘
            if hasattr(unet_fa, "base_model"):
                noise_pred_fa = unet_fa.base_model(
                    sample=lat_fa_t,
                    timestep=timesteps_fa,
                    encoder_hidden_states=prompt_fa,
                    return_dict=False,
                )[0]
            else:
                noise_pred_fa = unet_fa(lat_fa_t, timesteps_fa, prompt_fa).sample
            
            # è®¡ç®—æŸå¤±
            loss_cf, mse_cf, hf_cf, pred_x0_cf = compute_single_modality_loss(
                noise_pred_cf, noise_cf, lat_cf_t, lat_cf,
                noise_scheduler.alphas_cumprod, timesteps_cf,
                hf_lambda=args.hf_lambda
            )
            
            loss_fa, mse_fa, hf_fa, pred_x0_fa = compute_single_modality_loss(
                noise_pred_fa, noise_fa, lat_fa_t, lat_fa,
                noise_scheduler.alphas_cumprod, timesteps_fa,
                hf_lambda=args.hf_lambda
            )
            
            # ç»“æ„ä¸€è‡´æ€§æŸå¤±(åœ¨é¢„æµ‹çš„x0ä¸Šè®¡ç®—)
            loss_struct = compute_structure_consistency_loss(pred_x0_cf, pred_x0_fa)
            
            # æ€»æŸå¤±
            loss_total = loss_cf + loss_fa + args.struct_lambda * loss_struct
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss_total.backward()
            optimizer.step()
            
            # å­¦ä¹ ç‡è°ƒæ•´
            lr = get_dynamic_lr(global_step, args.max_steps)
            for g in optimizer.param_groups:
                g["lr"] = lr
            
            # è®°å½•å„é¡¹æŸå¤±çš„è´¡çŒ®ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
            # loss_total = loss_cf + loss_fa + struct_lambda * struct
            # å…¶ä¸­ loss_cf = mse_cf + hf_lambda * hf_cf
            #      loss_fa = mse_fa + hf_lambda * hf_fa
            loss_acc.append((
                loss_total.item(),                          # æ€»loss
                loss_cf.item() + loss_fa.item(),           # CF+FAçš„æ€»è´¡çŒ®
                args.struct_lambda * loss_struct.item()     # structçš„è´¡çŒ®
            ))
            
            # æ—¥å¿—æ‰“å°
            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                arr = np.array(loss_acc)
                avg_total = arr[:, 0].mean()
                avg_cffa = arr[:, 1].mean()              # CF+FAçš„è´¡çŒ®
                avg_struct = arr[:, 2].mean()            # structçš„è´¡çŒ®
                loss_acc = []
                
                # è®¡ç®—å„é¡¹å æ¯”ï¼ˆåº”è¯¥åŠ èµ·æ¥=100%ï¼‰
                pct_cffa = avg_cffa / avg_total * 100
                pct_struct = avg_struct / avg_total * 100
                
                msg = (
                    f"[dual-unet-v27] Step {global_step:5d}/{args.max_steps} | "
                    f"lr:{lr:.2e} | loss:{avg_total:.4f} "
                    f"(cf+fa:{avg_cffa:.4f}/{pct_cffa:.0f}% struct:{avg_struct:.4f}/{pct_struct:.0f}%) | "
                    f"{elapsed:.1f}s"
                )
                print(msg)
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")
                
                start_time = time.time()
            
            # éªŒè¯ + å¯è§†åŒ– + checkpoint
            if global_step % 500 == 0:
                val_loss = evaluate_dual_unet(
                    val_loader, vae, unet_cf, unet_fa, noise_scheduler, 
                    tokenizer, text_encoder, args
                )
                
                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")
                
                # å¯è§†åŒ–
                vis_dir = os.path.join(out_dir, f"step_{global_step:06d}_random_pairs")
                print(f"[å¯è§†åŒ–] åœ¨ {vis_dir} ç”Ÿæˆ 10 ç»„éšæœº CF-FA å›¾åƒ...")
                visualize_random_pairs(unet_cf, unet_fa, vae, tokenizer, text_encoder, 10, vis_dir, 50)
                
                # ä¿å­˜latest checkpoint
                latest_root = os.path.join(out_dir, "latest_checkpoints")
                os.makedirs(latest_root, exist_ok=True)
                latest_step_dir = os.path.join(latest_root, f"step_{global_step:06d}")
                os.makedirs(latest_step_dir, exist_ok=True)
                
                unet_cf_dir = os.path.join(latest_step_dir, "unet_cf_lora")
                unet_fa_dir = os.path.join(latest_step_dir, "unet_fa_lora")
                os.makedirs(unet_cf_dir, exist_ok=True)
                os.makedirs(unet_fa_dir, exist_ok=True)
                unet_cf.save_pretrained(unet_cf_dir)
                unet_fa.save_pretrained(unet_fa_dir)
                
                with open(os.path.join(latest_step_dir, "info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Struct Lambda: {args.struct_lambda}\n")
                
                # æ»šåŠ¨åˆ é™¤æ—§checkpoint
                subdirs = sorted(d for d in os.listdir(latest_root) if d.startswith("step_"))
                if len(subdirs) > 3:
                    for old in subdirs[:-3]:
                        shutil.rmtree(os.path.join(latest_root, old))
                
                # ä¿å­˜best checkpoint
                if val_loss < best_val - 1e-4:
                    best_val = val_loss
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)
                    
                    best_cf_dir = os.path.join(best_dir, "unet_cf_lora")
                    best_fa_dir = os.path.join(best_dir, "unet_fa_lora")
                    os.makedirs(best_cf_dir, exist_ok=True)
                    os.makedirs(best_fa_dir, exist_ok=True)
                    unet_cf.save_pretrained(best_cf_dir)
                    unet_fa.save_pretrained(best_fa_dir)
                    
                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Struct Lambda: {args.struct_lambda}\n")
                    
                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„ Dual-UNet CF-FA ç”Ÿæˆæ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)
            
            global_step += 1


if __name__ == "__main__":
    main()

