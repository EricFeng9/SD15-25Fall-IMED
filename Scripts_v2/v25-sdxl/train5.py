# -*- coding: utf-8 -*-
"""
Joint CF-FA Generation Training Script (v25-SDXL)
-------------------------------------------------

ç›®æ ‡ï¼š
- åŸºäº CFFA é…å‡†å¥½çš„çœŸå® CF-FA å›¾åƒå¯¹ï¼Œè®­ç»ƒä¸€ä¸ª"è”åˆç”Ÿæˆ"æ‰©æ•£æ¨¡å‹ï¼Œ
  ä½¿å…¶å¯ä»¥ä»çº¯å™ªå£°ç›´æ¥ç”Ÿæˆç»“æ„å…¨æ–°ã€ä½†é£æ ¼/åŸŸä¸ CFFA å®Œå…¨ä¸€è‡´çš„ CF-FA æˆå¯¹å›¾åƒã€‚

æ ¸å¿ƒè®¾è®¡ï¼ˆSDXLç‰ˆæœ¬ï¼‰ï¼š
- å°†ä¸€å¯¹é…å‡†å¥½çš„ CF(å·¦) å’Œ FA(å³) **ç›´æ¥æ‹¼æ¥**æˆ 1024x512 çš„å•å›¾ï¼š
    - CF: 512x512ï¼ŒFA: 512x512
    - åœ¨å®½åº¦ç»´åº¦ä¸Šæ‹¼æ¥ï¼š joint = cat([CF, FA], dim=3) -> [B,3,512,1024]
    - **æ— éœ€å‹ç¼©ï¼Œä¿ç•™å®Œæ•´åˆ†è¾¨ç‡å’Œè¡€ç®¡ç»†èŠ‚**
- ä½¿ç”¨ SDXL çš„ VAE å¯¹ joint è¿›è¡Œç¼–ç ï¼Œlatent shape: [B,4,64,128]ï¼ˆä¿¡æ¯é‡ç¿»å€ï¼‰
- ä½¿ç”¨ SDXL çš„åŒ Text Encoder + Time IDs æœºåˆ¶
- UNet + LoRA åªå¯¹ joint å›¾åƒå»ºæ¨¡
- æ–‡æœ¬æç¤ºåªç”¨ä¸€æ¡å›ºå®š promptï¼Œè®­ç»ƒç›®æ ‡ä¸ºæ ‡å‡†å™ªå£° MSE + å¯é€‰ latent é«˜é¢‘ L1ã€‚

è®­ç»ƒè¾“å‡ºç›®å½•ï¼š
- /results/out_joint_sdxl_cffa_pairs/{name}/
  - training_log.txt / validation_log.txt
  - step_xxxxxx_random_pairs/ ä¸‹ä¿å­˜è‹¥å¹² joint ç”Ÿæˆå›¾ï¼ˆæ‹†åˆ†ä¸º cf.png / fa.pngï¼‰
  - latest_checkpoints/step_xxxxxx/unet_lora/
  - best_checkpoint/unet_lora/
"""

import os
import math
import time
import argparse
import shutil

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# å¯¼å…¥ CFFA æ•°æ®é›†
import sys

CURRENT_DIR = os.path.dirname(__file__)
sys.path.append(os.path.join(CURRENT_DIR, "../../data/operation_pre_filtered_cffa_augmented"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2  # noqa: E402

# å¯¼å…¥åŸå§‹ CFFA æ•°æ®é›†ï¼ˆç”¨äºéªŒè¯æ—¶çš„ from_data ç”Ÿæˆï¼‰
sys.path.append(os.path.join(CURRENT_DIR, "../../data/operation_pre_filtered_cffa"))
from operation_pre_filtered_cffa_dataset import CFFADataset as CFFARegDataset  # noqa: E402


# ============ å…¨å±€é…ç½® ============

SIZE = 512  # å•å¼ å›¾åƒå°ºå¯¸ï¼ˆCFå’ŒFAå„512x512ï¼‰
JOINT_HEIGHT = 512  # Jointå›¾åƒé«˜åº¦
JOINT_WIDTH = 1024  # Jointå›¾åƒå®½åº¦ï¼ˆ512+512ï¼‰
DEVICE = torch.device("cuda")

# SDXL æ¨¡å‹è·¯å¾„
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sdxl-base"

# Joint ç”Ÿæˆæ¨¡å‹è¾“å‡ºæ ¹ç›®å½•
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_joint_sdxl_cffa_pairs"

# åŸå§‹ CFFA æ•°æ®é›†æ ¹ç›®å½•ï¼ˆç”¨äºéªŒè¯æ—¶çš„ from_data ç”Ÿæˆï¼‰
CFFA_BASE_DIR = "/data/student/Fengjunming/SDXL_ControlNet/data/operation_pre_filtered_cffa"


# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_joint_prompt_embeds_sdxl(bs, tokenizer, tokenizer_2, text_encoder, text_encoder_2, 
                                  use_cfg=False):
    """
    ç”¨äº Joint CF-FA ç”Ÿæˆçš„å›ºå®šæ–‡æœ¬æç¤ºï¼ˆSDXLç‰ˆæœ¬ï¼‰ã€‚
    SDXL ä½¿ç”¨ä¸¤ä¸ª Text Encoderï¼š
    - text_encoder: CLIP-ViT-L/14
    - text_encoder_2: OpenCLIP-ViT-bigG/14
    
    Args:
        use_cfg: æ˜¯å¦è¿”å›åŒ…å«æ— æ¡ä»¶åµŒå…¥çš„æ‰¹æ¬¡ï¼ˆç”¨äº CFGï¼‰
    
    è¿”å›ï¼š
    - prompt_embeds: [bs, 77, 2048] æ‹¼æ¥åçš„æ–‡æœ¬åµŒå…¥ï¼ˆå¦‚æœuse_cfg=Trueï¼Œåˆ™ä¸º[bs*2, 77, 2048]ï¼‰
    - pooled_prompt_embeds: [bs, 1280] æ± åŒ–åçš„æ–‡æœ¬åµŒå…¥ï¼ˆå¦‚æœuse_cfg=Trueï¼Œåˆ™ä¸º[bs*2, 1280]ï¼‰
    """
    # æ­£å‘æç¤ºè¯ï¼ˆå¢å¼ºç»†èŠ‚æè¿°ï¼‰
    positive_prompt = (
        "A seamless pair of medical retinal images. Left is color fundus, right is fluorescein angiography. "
        "high resolution, sharp, highly detailed, clear vessels."
    )
    prompts = [positive_prompt] * bs
    
    # ç¬¬ä¸€ä¸ª Text Encoder (CLIP-ViT-L/14)
    inputs_1 = tokenizer(
        prompts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    outputs_1 = text_encoder(inputs_1.input_ids, output_hidden_states=True)
    prompt_embeds_1 = outputs_1.hidden_states[-2]  # å€’æ•°ç¬¬äºŒå±‚ [bs, 77, 768]
    
    # ç¬¬äºŒä¸ª Text Encoder (OpenCLIP-ViT-bigG/14)
    inputs_2 = tokenizer_2(
        prompts,
        padding="max_length",
        max_length=tokenizer_2.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    outputs_2 = text_encoder_2(inputs_2.input_ids, output_hidden_states=True)
    prompt_embeds_2 = outputs_2.hidden_states[-2]  # å€’æ•°ç¬¬äºŒå±‚ [bs, 77, 1280]
    pooled_prompt_embeds = outputs_2.text_embeds  # æ± åŒ–è¾“å‡º [bs, 1280]
    
    # æ‹¼æ¥ä¸¤ä¸ªç¼–ç å™¨çš„è¾“å‡º
    prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)  # [bs, 77, 2048]
    
    # å¦‚æœéœ€è¦ CFGï¼Œæ·»åŠ æ— æ¡ä»¶åµŒå…¥
    if use_cfg:
        # è´Ÿå‘æç¤ºè¯
        negative_prompt = "blurry, out of focus, smooth, painting, soft edges, low contrast, low quality"
        neg_prompts = [negative_prompt] * bs
        
        # ç¬¬ä¸€ä¸ª Text Encoder (è´Ÿå‘)
        neg_inputs_1 = tokenizer(
            neg_prompts,
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        ).to(DEVICE)
        neg_outputs_1 = text_encoder(neg_inputs_1.input_ids, output_hidden_states=True)
        neg_embeds_1 = neg_outputs_1.hidden_states[-2]
        
        # ç¬¬äºŒä¸ª Text Encoder (è´Ÿå‘)
        neg_inputs_2 = tokenizer_2(
            neg_prompts,
            padding="max_length",
            max_length=tokenizer_2.model_max_length,
            truncation=True,
            return_tensors="pt",
        ).to(DEVICE)
        neg_outputs_2 = text_encoder_2(neg_inputs_2.input_ids, output_hidden_states=True)
        neg_embeds_2 = neg_outputs_2.hidden_states[-2]
        neg_pooled = neg_outputs_2.text_embeds
        
        # æ‹¼æ¥è´Ÿå‘åµŒå…¥
        neg_embeds = torch.cat([neg_embeds_1, neg_embeds_2], dim=-1)  # [bs, 77, 2048]
        
        # åˆå¹¶ï¼š[negative, positive]
        prompt_embeds = torch.cat([neg_embeds, prompt_embeds], dim=0)  # [bs*2, 77, 2048]
        pooled_prompt_embeds = torch.cat([neg_pooled, pooled_prompt_embeds], dim=0)  # [bs*2, 1280]
    
    return prompt_embeds, pooled_prompt_embeds


def compute_time_ids(original_size=(JOINT_HEIGHT, JOINT_WIDTH), crops_coords_top_left=(0, 0)):
    """
    è®¡ç®— SDXL çš„ Time IDsï¼ˆç”¨äºå‘ŠçŸ¥æ¨¡å‹å›¾åƒå°ºå¯¸ä¿¡æ¯ï¼‰ã€‚
    
    Args:
        original_size: (height, width) åŸå§‹å›¾åƒå°ºå¯¸
        crops_coords_top_left: (top, left) è£å‰ªèµ·ç‚¹åæ ‡
    
    Returns:
        add_time_ids: [1, 6] tensor
    """
    target_size = original_size  # è®­ç»ƒæ—¶ä¸åšè£å‰ªï¼Œtarget = original
    add_time_ids = list(original_size + crops_coords_top_left + target_size)
    # ç»“æœ: [512, 1024, 0, 0, 512, 1024]
    add_time_ids = torch.tensor([add_time_ids], dtype=torch.long, device=DEVICE)
    return add_time_ids


def get_dynamic_lr(step, max_steps, base_lr=2.5e-5, min_lr=5e-6):
    """
    ã€å…³é”®ä¿®å¤ã€‘é™ä½å­¦ä¹ ç‡ï¼Œè®© HF Loss èƒ½ç²¾ç»†é›•ç¢è¡€ç®¡è¾¹ç¼˜
    - base_lr: 5e-5 -> 2.5e-5ï¼ˆé™ä½50%ï¼‰
    - min_lr: 1e-5 -> 5e-6
    """
    if step < 4000:
        return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2


# ============ 2. æŸå¤±ï¼ˆå¤ç”¨ v22 é€»è¾‘ï¼‰ ============

def _gaussian_kernel_1d(kernel_size: int, sigma: float, device, dtype):
    half = kernel_size // 2
    coords = torch.arange(kernel_size, device=device, dtype=dtype) - half
    gauss = torch.exp(-0.5 * (coords / sigma) ** 2)
    return gauss / gauss.sum()


def gaussian_blur_latent(x, kernel_size=3, sigma=0.5):
    """
    ã€å…³é”®ä¿®å¤ã€‘ç¼©å°é«˜æ–¯æ ¸ï¼Œè®©æ¨¡å‹çœŸæ­£å­¦åˆ°æ¯›ç»†è¡€ç®¡çº§åˆ«çš„é«˜é¢‘ç»†èŠ‚ï¼
    - kernel_size=3ï¼ˆåŸ7ï¼‰ï¼šåœ¨latentç©ºé—´ç›¸å½“äºåŸå›¾24åƒç´ ï¼ˆåŸ56åƒç´ ï¼‰
    - sigma=0.5ï¼ˆåŸ1.5ï¼‰ï¼šæ›´é”åˆ©çš„è¾¹ç¼˜æå–
    """
    C = x.shape[1]
    k = _gaussian_kernel_1d(kernel_size, sigma, x.device, x.dtype)
    pad = kernel_size // 2
    # æ°´å¹³æ–¹å‘
    kw = k.view(1, 1, 1, kernel_size).expand(C, 1, 1, kernel_size)
    x = F.conv2d(x, kw, padding=(0, pad), groups=C)
    # å‚ç›´æ–¹å‘
    kh = k.view(1, 1, kernel_size, 1).expand(C, 1, kernel_size, 1)
    x = F.conv2d(x, kh, padding=(pad, 0), groups=C)
    return x


def compute_hf_texture_loss(pred_x0, gt_x0, kernel_size=3, sigma=0.5):
    """ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨æ›´å°çš„æ ¸æå–çœŸæ­£çš„é«˜é¢‘ç»†èŠ‚"""
    pred_blur = gaussian_blur_latent(pred_x0, kernel_size, sigma)
    gt_blur = gaussian_blur_latent(gt_x0, kernel_size, sigma)
    pred_hf = pred_x0 - pred_blur
    gt_hf = gt_x0 - gt_blur
    return F.l1_loss(pred_hf, gt_hf)


def compute_total_loss(noise_pred, noise, noisy_latents, latents,
                       alphas_cumprod, timesteps, hf_lambda=0.5):
    """
    å™ªå£° MSE + æ—¶é—´æ„ŸçŸ¥çš„ latent é«˜é¢‘ L1ã€‚
    é«˜é¢‘æŸå¤±åªåœ¨æ‰©æ•£æœ«æœŸï¼ˆt < 300ï¼‰ç”Ÿæ•ˆï¼Œé¿å…åœ¨æ—©æœŸæ³¨å…¥å™ªå£°ã€‚
    """
    loss_mse = F.mse_loss(noise_pred, noise)
    loss_hf = torch.tensor(0.0, device=noise.device)
    
    # ã€æ ¸å¿ƒæ”¹åŠ¨ã€‘åªåœ¨æ‰©æ•£æœ«æœŸï¼ˆt < 300ï¼‰è®¡ç®—é«˜é¢‘æŸå¤±ï¼Œé”åŒ–è¡€ç®¡
    if hf_lambda > 0 and timesteps.float().mean() < 300:
        alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(noisy_latents.device)
        pred_x0 = (noisy_latents - (1.0 - alpha_t).sqrt() * noise_pred) / (alpha_t.sqrt() + 1e-8)
        pred_x0 = pred_x0.clamp(-10.0, 10.0)
        
        loss_hf = compute_hf_texture_loss(pred_x0, latents)

    total = loss_mse + hf_lambda * loss_hf
    return total, loss_mse.item(), loss_hf.item()


# ============ 3. éªŒè¯ä¸å¯è§†åŒ– ============

VAL_TIMESTEPS = [200, 500, 800]


def build_joint_image(cf, fa):
    """
    å°† CF, FA (B,3,512,512, [-1,1]) **ç›´æ¥æ‹¼æ¥**æˆ joint (B,3,512,1024)ã€‚
    
    SDXLç‰ˆæœ¬ï¼šæ— éœ€å‹ç¼©ï¼Œä¿ç•™å®Œæ•´åˆ†è¾¨ç‡ï¼
    """
    # ç›´æ¥åœ¨å®½åº¦ç»´åº¦æ‹¼æ¥ï¼Œæ— éœ€æ’å€¼
    joint = torch.cat([cf, fa], dim=3)  # [B, 3, 512, 1024]
    return joint


def evaluate_joint(val_loader, vae, unet, noise_scheduler, tokenizer, tokenizer_2, 
                   text_encoder, text_encoder_2, args):
    """
    éªŒè¯ Joint ç”Ÿæˆæ¨¡å‹ï¼šåœ¨å›ºå®šæ—¶é—´æ­¥ä¸Šè¯„ä¼°å™ªå£°é¢„æµ‹ MSEï¼ˆSDXLç‰ˆæœ¬ï¼‰ã€‚
    """
    if hasattr(unet, "eval"):
        unet.eval()

    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cf, fa, _, _ = batch  # dataset: (cond_tile=CF, tgt=FA, ...)
            cf, fa = cf.to(DEVICE), fa.to(DEVICE)
            b = cf.shape[0]

            joint = build_joint_image(cf, fa)  # [B, 3, 512, 1024]
            latents = vae.encode(joint).latent_dist.sample() * vae.config.scaling_factor
            # latents shape: [B, 4, 64, 128]
            
            prompt_embeds, pooled_prompt_embeds = get_joint_prompt_embeds_sdxl(
                b, tokenizer, tokenizer_2, text_encoder, text_encoder_2
            )
            
            # è®¡ç®— time_ids
            time_ids = compute_time_ids().repeat(b, 1)  # [b, 6]

            sample_losses = []
            for t_val in VAL_TIMESTEPS:
                timesteps = torch.full((b,), t_val, device=DEVICE, dtype=torch.long)
                noise = torch.randn_like(latents)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # SDXL UNet è°ƒç”¨
                if hasattr(unet, "base_model"):
                    noise_pred = unet.base_model(
                        sample=noisy_latents,
                        timestep=timesteps,
                        encoder_hidden_states=prompt_embeds,
                        added_cond_kwargs={
                            "text_embeds": pooled_prompt_embeds,
                            "time_ids": time_ids,
                        },
                        return_dict=False,
                    )[0]
                else:
                    noise_pred = unet(
                        noisy_latents, 
                        timesteps, 
                        prompt_embeds,
                        added_cond_kwargs={
                            "text_embeds": pooled_prompt_embeds,
                            "time_ids": time_ids,
                        },
                    ).sample

                sample_losses.append(F.mse_loss(noise_pred, noise).item())

            val_losses.append(np.mean(sample_losses))

    if hasattr(unet, "train"):
        unet.train()

    torch.cuda.empty_cache()
    return np.mean(val_losses)


@torch.no_grad()
def visualize_random_pairs(unet, vae, tokenizer, tokenizer_2, text_encoder, text_encoder_2,
                           num_samples: int, out_dir: str, steps: int = 50, strength: float = 0.6,
                           guidance_scale: float = 6.5):
    """
    ã€å…³é”®ä¿®å¤ã€‘åŸºäºçœŸå® CF-FA å¯¹è¿›è¡Œ img2img ç”Ÿæˆï¼Œä½¿ç”¨ CFG ç”Ÿæˆé”åˆ©å›¾åƒï¼
    
    - ä»éªŒè¯é›†éšæœºé€‰å– num_samples ä¸ªæ ·æœ¬
    - å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œ img2img å¤„ç†ï¼ˆstrength=0.6ï¼‰
    - **ä½¿ç”¨ CFG (guidance_scale) è®©è¡€ç®¡è¾¹ç¼˜æå…¶é”åˆ©**
    - **ä½¿ç”¨ EulerDiscreteScheduler æ›¿ä»£ DDPMï¼Œå‡å°‘å¹³æ»‘**
    
    æ¯ä¸ªæ ·æœ¬ä¿å­˜ä¸ºï¼š
      pair_xx/joint.png                      (ç”Ÿæˆçš„æ‹¼æ¥å›¾)
      pair_xx/cf.png                         (ç”Ÿæˆçš„ CF)
      pair_xx/fa.png                         (ç”Ÿæˆçš„ FA)
      pair_xx/{orig_name}_joint_origin.png   (åŸå§‹æ‹¼æ¥å›¾ï¼Œè¾“å…¥æ¨¡å‹çš„å›¾åƒ)
      pair_xx/{orig_name}_cf_origin.png      (åŸå§‹ CF)
      pair_xx/{orig_name}_fa_origin.png      (åŸå§‹ FA)
    """
    try:
        os.makedirs(out_dir, exist_ok=True)

        if hasattr(unet, "eval"):
            unet.eval()
        if hasattr(vae, "eval"):
            vae.eval()
        if hasattr(text_encoder, "eval"):
            text_encoder.eval()
        if hasattr(text_encoder_2, "eval"):
            text_encoder_2.eval()

        # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ CFGï¼Œè·å–æ­£å‘å’Œè´Ÿå‘æç¤ºè¯åµŒå…¥
        prompt_embeds, pooled_prompt_embeds = get_joint_prompt_embeds_sdxl(
            1, tokenizer, tokenizer_2, text_encoder, text_encoder_2, use_cfg=True
        )
        # prompt_embeds: [2, 77, 2048] - [negative, positive]
        # pooled_prompt_embeds: [2, 1280]
        
        time_ids = compute_time_ids()  # [1, 6]
        # ä¸º CFG å¤åˆ¶ time_ids
        time_ids = torch.cat([time_ids, time_ids], dim=0)  # [2, 6]

        # ã€ä¿®å¤ã€‘ä½¿ç”¨ DDPMSchedulerï¼ˆæ”¯æŒ img2imgï¼‰ï¼Œä¿ç•™ CFG åŠŸèƒ½
        # EulerDiscreteScheduler ä¸æ”¯æŒä»ä¸­é—´æ—¶é—´æ­¥å¼€å§‹ï¼ˆimg2img åœºæ™¯ï¼‰
        scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
        scheduler.set_timesteps(steps)
        timesteps = scheduler.timesteps

        # è®¡ç®— img2img èµ·å§‹æ—¶é—´æ­¥
        strength = float(strength)
        strength = max(0.0, min(1.0, strength))
        num_t = len(timesteps)
        start_index = int((1.0 - strength) * (num_t - 1))
        start_index = min(max(start_index, 0), num_t - 1)
        start_t = timesteps[start_index]

        # åŠ è½½éªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ splitï¼Œå› ä¸º operation_pre_filtered_cffa çš„ test split ä¸ºç©ºï¼‰
        print(f"  - åŠ è½½éªŒè¯æ•°æ®é›†: {CFFA_BASE_DIR}")
        dataset = CFFARegDataset(root_dir=CFFA_BASE_DIR, split="train", mode="cf2fa")
        print(f"  - æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        if len(dataset) == 0:
            print(f"  âš ï¸ è­¦å‘Š: éªŒè¯æ•°æ®é›†ä¸ºç©ºï¼Œè·³è¿‡å¯è§†åŒ–")
            return
        
        # éšæœºé€‰æ‹© num_samples ä¸ªæ ·æœ¬
        indices = np.random.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)
        print(f"  - å°†ç”Ÿæˆ {len(indices)} ä¸ªå¯è§†åŒ–æ ·æœ¬ (CFG={guidance_scale})")

        def tensor_to_pil(x: torch.Tensor) -> Image.Image:
            x = (x.clamp(-1, 1) + 1) / 2.0
            x = x.cpu().permute(1, 2, 0).numpy()
            x = (x * 255).round().astype("uint8")
            return Image.fromarray(x)

        for idx, data_idx in enumerate(indices):
            try:
                cf, fa, cp, fp = dataset[data_idx]
                
                # è½¬æ¢ä¸º batch å¹¶ç§»åˆ°è®¾å¤‡
                cf = cf.unsqueeze(0).to(DEVICE)  # [1, 3, 512, 512]
                fa = fa.unsqueeze(0).to(DEVICE)  # [1, 3, 512, 512]
                
                # ä¿å­˜åŸå§‹å›¾åƒ
                cf_origin_pil = tensor_to_pil(cf[0])
                fa_origin_pil = tensor_to_pil(fa[0])
                
                # è·å–åŸå§‹æ–‡ä»¶å
                try:
                    cf_path = cp if isinstance(cp, str) else str(cp)
                except Exception:
                    cf_path = "unknown"
                orig_stem = os.path.splitext(os.path.basename(cf_path))[0]
                
                # æ„å»º joint å›¾åƒ
                joint = build_joint_image(cf, fa)  # [1, 3, 512, 1024]
                
                # ä¿å­˜åŸå§‹ joint å›¾åƒï¼ˆç”¨äºå¯¹æ¯”ï¼‰
                joint_origin_pil = tensor_to_pil(joint[0])
                
                # ç¼–ç ä¸º latent
                latents_clean = vae.encode(joint).latent_dist.sample() * vae.config.scaling_factor
                
                # åœ¨ start_t ä¸ŠåŠ å™ªï¼ˆå°†æ ‡é‡ timestep è½¬æ¢ä¸ºæ‰¹æ¬¡æ ¼å¼ï¼‰
                noise = torch.randn_like(latents_clean)
                start_t_batch = start_t.unsqueeze(0) if start_t.dim() == 0 else start_t
                latents = scheduler.add_noise(latents_clean, noise, start_t_batch)
                
                # ã€å…³é”®ä¿®å¤ã€‘ä» start_index å¼€å§‹é€†æ‰©æ•£åˆ° 0ï¼Œä½¿ç”¨ CFG
                for t in timesteps[start_index:]:
                    # ä¸º CFG å¤åˆ¶ latentsï¼ˆä¸éœ€è¦ scale_model_inputï¼ŒDDPM ä¸éœ€è¦ï¼‰
                    latent_model_input = torch.cat([latents, latents], dim=0)  # [2, 4, 64, 128]
                    
                    if hasattr(unet, "base_model"):
                        noise_pred = unet.base_model(
                            sample=latent_model_input,
                            timestep=t,
                            encoder_hidden_states=prompt_embeds,
                            added_cond_kwargs={
                                "text_embeds": pooled_prompt_embeds,
                                "time_ids": time_ids,
                            },
                            return_dict=False,
                        )[0]
                    else:
                        noise_pred = unet(
                            latent_model_input,
                            t,
                            prompt_embeds,
                            added_cond_kwargs={
                                "text_embeds": pooled_prompt_embeds,
                                "time_ids": time_ids,
                            },
                        ).sample
                    
                    # ã€å…³é”®ä¿®å¤ã€‘CFG å…¬å¼ï¼šnoise_pred = uncond + guidance_scale * (cond - uncond)
                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)
                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)

                    latents = scheduler.step(noise_pred, t, latents).prev_sample

                # è§£ç å¹¶æ‹†åˆ†
                latents_final = latents / vae.config.scaling_factor
                imgs_joint = vae.decode(latents_final).sample  # [1,3,512,1024], [-1,1]
                joint_img = imgs_joint[0]

                # æ‹†åˆ† joint -> CF/FAï¼ˆæ— éœ€æ’å€¼ï¼Œç›´æ¥æŒ‰å®½åº¦åˆ‡åˆ†ï¼‰
                # joint_img shape: [3, 512, 1024]
                cf_full = joint_img[:, :, :SIZE]  # [3, 512, 512]
                fa_full = joint_img[:, :, SIZE:]  # [3, 512, 512]

                img_joint = tensor_to_pil(joint_img)
                img_cf = tensor_to_pil(cf_full)
                img_fa = tensor_to_pil(fa_full)

                pair_dir = os.path.join(out_dir, f"pair_{idx:02d}")
                os.makedirs(pair_dir, exist_ok=True)
                
                # ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
                img_joint.save(os.path.join(pair_dir, "joint.png"))
                img_cf.save(os.path.join(pair_dir, "cf.png"))
                img_fa.save(os.path.join(pair_dir, "fa.png"))
                
                # ä¿å­˜åŸå§‹å›¾åƒ
                joint_origin_pil.save(os.path.join(pair_dir, f"{orig_stem}_joint_origin.png"))
                cf_origin_pil.save(os.path.join(pair_dir, f"{orig_stem}_cf_origin.png"))
                fa_origin_pil.save(os.path.join(pair_dir, f"{orig_stem}_fa_origin.png"))
                
                print(f"  âœ“ å·²ç”Ÿæˆ pair_{idx:02d}")
                
            except Exception as e:
                print(f"  âœ— ç”Ÿæˆ pair_{idx:02d} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    except Exception as e:
        print(f"  âœ— å¯è§†åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============


def main():
    parser = argparse.ArgumentParser(description="Joint CF-FA ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬ v25-SDXLï¼ˆä¿®å¤æ¨¡ç³Šé—®é¢˜ï¼‰")
    parser.add_argument("-n", "--name", default="joint_cffa_v25_sdxl")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--unet_lora_rank", type=int, default=16, help="UNet LoRA rank")
    parser.add_argument("--unet_lora_alpha", type=int, default=16, help="UNet LoRA alpha")
    parser.add_argument("--offset_noise_strength", type=float, default=0.1, help="Offset noise strength for better contrast")
    parser.add_argument("--hf_lambda", type=float, default=0.5, help="é«˜é¢‘çº¹ç†æŸå¤±æƒé‡ï¼Œæ¨è 0.3~1.0")
    parser.add_argument("--vis_strength", type=float, default=0.6, help="éªŒè¯å¯è§†åŒ–æ—¶ img2img çš„å™ªå£°å¼ºåº¦ (0~1]ï¼Œè¶Šå¤§è¶Šåç¦»åŸå›¾ï¼Œé»˜è®¤ 0.6")
    parser.add_argument("--guidance_scale", type=float, default=6.5, help="ã€å…³é”®ã€‘CFG å¼•å¯¼ç³»æ•°ï¼Œ5.0~7.5 ä¼šè®©è¡€ç®¡æå…¶é”åˆ©ï¼Œé»˜è®¤ 6.5")
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½ï¼ˆä»…ä½¿ç”¨ CFFAï¼Œå¯¹åº” cf2fa æ¨¡å¼ï¼‰
    train_ds = CFFADataset_v2(split="train", mode="cf2fa")
    val_ds = CFFADataset_v2(split="test", mode="cf2fa")

    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)

    # 2. æ¨¡å‹åŠ è½½ï¼ˆSDXLç‰ˆæœ¬ï¼‰
    print("\n========== SDXL æ¨¡å‹åŠ è½½ ==========")
    print(f"æ¨¡å‹è·¯å¾„: {BASE_MODEL_DIR}")
    
    # SDXL ä½¿ç”¨ä¸¤ä¸ª tokenizer å’Œ text encoder
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    tokenizer_2 = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer_2")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder_2").to(DEVICE)
    
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)

    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    text_encoder_2.requires_grad_(False)
    
    print(f"âœ“ SDXL ç»„ä»¶åŠ è½½å®Œæˆ")
    print(f"  - Text Encoder 1: CLIP-ViT-L/14")
    print(f"  - Text Encoder 2: OpenCLIP-ViT-bigG/14")
    print(f"  - VAE: SDXL VAE")
    print(f"  - UNet: SDXL UNet2DConditionModel")

    print(f"\n========== UNet LoRA é…ç½® ==========")
    unet.requires_grad_(False)

    target_modules = [
        "to_k", "to_q", "to_v", "to_out.0",  # Attention å±‚ï¼šå…¨å±€ç‰¹å¾è·¯ç”±
        "ff.net.0.proj", "ff.net.2",          # ä»‹å…¥å‰é¦ˆç½‘ç»œï¼Œä¸»ç®¡å±€éƒ¨ç»“æ„ç”Ÿæˆ
        "proj_in", "proj_out"                 # ä»‹å…¥è·¨å±‚æŠ•å½±ï¼Œå¢å¼ºç‰¹å¾è¿è´¯æ€§
    ]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )

    unet = get_peft_model(unet, lora_config)

    unet_lora_params = [p for p in unet.parameters() if p.requires_grad]
    unet_lora_num = sum(p.numel() for p in unet_lora_params)
    unet_total_num = sum(p.numel() for p in unet.parameters())

    print(f"âœ“ UNet LoRA å·²åº”ç”¨")
    print(f"  - Rank: {args.unet_lora_rank}, Alpha: {args.unet_lora_alpha}")
    print(f"  - ç›®æ ‡æ¨¡å—: {target_modules}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°: {unet_lora_num:,} ({unet_lora_num/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°: {unet_total_num:,} ({unet_total_num/1e6:.2f}M)")
    print(f"  - å‚æ•°å æ¯”: {unet_lora_num/unet_total_num*100:.2f}%")

    total_trainable = unet_lora_num
    print(f"\nâœ“ æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,} ({total_trainable/1e6:.2f}M)")

    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    # ã€å…³é”®ä¿®å¤ã€‘é™ä½å­¦ä¹ ç‡ï¼ˆåŸ 5e-5 -> 2.5e-5ï¼‰
    optimizer = torch.optim.AdamW(unet_lora_params, lr=2.5e-5, weight_decay=1e-2)

    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=2.5e-5 [é™ä½50%], weight_decay=1e-2)")
    print(f"  - Offset Noise å¼ºåº¦: {args.offset_noise_strength}")
    print(f"  - é«˜é¢‘æŸå¤±æƒé‡ (hf_lambda): {args.hf_lambda} (ä»…åœ¨ t<300 æ—¶ç”Ÿæ•ˆ)")
    print(f"  - ã€å…³é”®ä¿®å¤ã€‘é«˜é¢‘æ»¤æ³¢æ ¸: kernel_size=3, sigma=0.5 (åŸ 7/1.5)")
    print(f"  - å¯è§†åŒ– img2img å¼ºåº¦: {args.vis_strength}")
    print(f"  - ã€å…³é”®ä¿®å¤ã€‘CFG å¼•å¯¼ç³»æ•°: {args.guidance_scale}")

    # 3. è®­ç»ƒçŠ¶æ€
    global_step = 0
    best_val_loss = float("inf")
    start_time = time.time()
    loss_accumulator = []

    print(f"\n========== å¼€å§‹è®­ç»ƒ Joint CF-FA ç”Ÿæˆæ¨¡å‹ ==========")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_ds)} (å…¨é‡ï¼Œå›ºå®šæ—¶é—´æ­¥ {VAL_TIMESTEPS})")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}\n")

    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps:
                break

            cf, fa, cp, fp = batch
            cf, fa = cf.to(DEVICE), fa.to(DEVICE)
            b = cf.shape[0]

            # æ„å»º joint å›¾åƒå¹¶ç¼–ç 
            joint = build_joint_image(cf, fa)
            latents = vae.encode(joint).latent_dist.sample() * vae.config.scaling_factor

            # Offset Noise
            noise = torch.randn_like(latents)
            if args.offset_noise_strength > 0:
                noise += args.offset_noise_strength * torch.randn(
                    latents.shape[0], latents.shape[1], 1, 1, device=latents.device
                )

            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE
            ).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            
            # SDXL prompt embedsï¼ˆè®­ç»ƒæ—¶ä¸ä½¿ç”¨ CFGï¼Œuse_cfg=Falseï¼‰
            prompt_embeds, pooled_prompt_embeds = get_joint_prompt_embeds_sdxl(
                b, tokenizer, tokenizer_2, text_encoder, text_encoder_2, use_cfg=False
            )
            time_ids = compute_time_ids().repeat(b, 1)  # [b, 6]

            # UNet å‰å‘ï¼ˆSDXLç‰ˆæœ¬ï¼‰
            if hasattr(unet, "base_model"):
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": time_ids,
                    },
                    return_dict=False,
                )[0]
            else:
                noise_pred = unet(
                    noisy_latents, 
                    timesteps, 
                    prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": time_ids,
                    },
                ).sample

            # æŸå¤±
            loss, loss_mse_val, loss_hf_val = compute_total_loss(
                noise_pred,
                noise,
                noisy_latents,
                latents,
                noise_scheduler.alphas_cumprod,
                timesteps,
                hf_lambda=args.hf_lambda,
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups:
                param_group["lr"] = current_lr

            loss_accumulator.append((loss.item(), loss_mse_val, loss_hf_val))

            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                arr = np.array(loss_accumulator)
                avg_loss, avg_mse, avg_hf = arr[:, 0].mean(), arr[:, 1].mean(), arr[:, 2].mean()
                loss_accumulator = []

                t_val = timesteps[0].item()
                msg = (
                    f"[joint-gen] Step {global_step:5d}/{args.max_steps} | "
                    f"lr:{current_lr:.2e} | loss:{avg_loss:.4f} "
                    f"(mse:{avg_mse:.4f} hf:{avg_hf:.4f}) | t={t_val:3d} | "
                    f"{elapsed:.1f}s"
                )
                print(msg)
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")

                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯ + å¯è§†åŒ– + checkpoint
            if global_step % 500 == 0:
                val_loss = evaluate_joint(
                    val_loader, vae, unet, noise_scheduler, 
                    tokenizer, tokenizer_2, text_encoder, text_encoder_2, args
                )

                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")

                # ã€å…³é”®ä¿®å¤ã€‘å¯è§†åŒ–æ—¶ä½¿ç”¨ CFGï¼Œç”Ÿæˆé”åˆ©å›¾åƒ
                vis_dir = os.path.join(out_dir, f"step_{global_step:06d}_random_pairs")
                print(f"[å¯è§†åŒ–] åœ¨ {vis_dir} åŸºäºçœŸå®æ•°æ®ç”Ÿæˆ 10 ç»„ CF-FA å›¾åƒå¢å¼ºæ ·æœ¬ (strength={args.vis_strength}, CFG={args.guidance_scale})...")
                visualize_random_pairs(
                    unet, vae, tokenizer, tokenizer_2, text_encoder, text_encoder_2, 
                    10, vis_dir, 50, strength=args.vis_strength, guidance_scale=args.guidance_scale
                )

                # latest checkpoints (æ»šåŠ¨ä¿ç•™æœ€è¿‘ 3 ä¸ª)
                latest_root = os.path.join(out_dir, "latest_checkpoints")
                os.makedirs(latest_root, exist_ok=True)
                latest_step_dir = os.path.join(latest_root, f"step_{global_step:06d}")
                os.makedirs(latest_step_dir, exist_ok=True)

                unet_lora_dir = os.path.join(latest_step_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)

                with open(os.path.join(latest_step_dir, "info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                    f.write(f"HF Lambda: {args.hf_lambda}\n")
                    f.write(f"CFG Guidance Scale: {args.guidance_scale}\n")

                # æ»šåŠ¨åˆ é™¤å¤šä½™çš„ latest
                subdirs = sorted(
                    [d for d in os.listdir(latest_root) if d.startswith("step_")]
                )
                if len(subdirs) > 3:
                    for old in subdirs[:-3]:
                        shutil.rmtree(os.path.join(latest_root, old))

                # best checkpoint
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)

                    best_unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(best_unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(best_unet_lora_dir)

                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Offset Noise: {args.offset_noise_strength}\n")
                        f.write(f"HF Lambda: {args.hf_lambda}\n")
                        f.write(f"CFG Guidance Scale: {args.guidance_scale}\n")

                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„ Joint CF-FA ç”Ÿæˆæ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)

            global_step += 1


if __name__ == "__main__":
    main()

