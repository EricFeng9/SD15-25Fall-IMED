# -*- coding: utf-8 -*-
"""
Joint CF-FA Generation Training Script (v25-SDXL)
-------------------------------------------------

ç›®æ ‡ï¼š
- åŸºäº CFFA é…å‡†å¥½çš„çœŸå® CF-FA å›¾åƒå¯¹ï¼Œè®­ç»ƒä¸€ä¸ª"è”åˆç”Ÿæˆ"æ‰©æ•£æ¨¡å‹ï¼Œ
  ä½¿å…¶å¯ä»¥ä»çº¯å™ªå£°ç›´æ¥ç”Ÿæˆç»“æ„å…¨æ–°ã€ä½†é£æ ¼/åŸŸä¸ CFFA å®Œå…¨ä¸€è‡´çš„ CF-FA æˆå¯¹å›¾åƒã€‚

æ ¸å¿ƒè®¾è®¡ï¼ˆSDXLç‰ˆæœ¬ï¼‰ï¼š
- å°†ä¸€å¯¹é…å‡†å¥½çš„ CF(å·¦) å’Œ FA(å³) **ç›´æ¥æ‹¼æ¥**æˆ 1024x512 çš„å•å›¾ï¼š
    - CF: 512x512ï¼ŒFA: 512x512
    - åœ¨å®½åº¦ç»´åº¦ä¸Šæ‹¼æ¥ï¼š joint = cat([CF, FA], dim=3) -> [B,3,512,1024]
    - **æ— éœ€å‹ç¼©ï¼Œä¿ç•™å®Œæ•´åˆ†è¾¨ç‡å’Œè¡€ç®¡ç»†èŠ‚**
- ä½¿ç”¨ SDXL çš„ VAE å¯¹ joint è¿›è¡Œç¼–ç ï¼Œlatent shape: [B,4,64,128]ï¼ˆä¿¡æ¯é‡ç¿»å€ï¼‰
- ä½¿ç”¨ SDXL çš„åŒ Text Encoder + Time IDs æœºåˆ¶
- UNet + LoRA åªå¯¹ joint å›¾åƒå»ºæ¨¡
- æ–‡æœ¬æç¤ºåªç”¨ä¸€æ¡å›ºå®š promptï¼Œè®­ç»ƒç›®æ ‡ä¸ºæ ‡å‡†å™ªå£° MSE + å¯é€‰ latent é«˜é¢‘ L1ã€‚

è®­ç»ƒè¾“å‡ºç›®å½•ï¼š
- /results/out_joint_sdxl_cffa_pairs/{name}/
  - training_log.txt / validation_log.txt
  - step_xxxxxx_random_pairs/ ä¸‹ä¿å­˜è‹¥å¹² joint ç”Ÿæˆå›¾ï¼ˆæ‹†åˆ†ä¸º cf.png / fa.pngï¼‰
  - latest_checkpoints/step_xxxxxx/unet_lora/
  - best_checkpoint/unet_lora/
"""

import os
import math
import time
import argparse
import shutil

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# å¯¼å…¥ CFFA æ•°æ®é›†
import sys

CURRENT_DIR = os.path.dirname(__file__)
sys.path.append(os.path.join(CURRENT_DIR, "../../data/operation_pre_filtered_cffa_augmented"))
from operation_pre_filtered_cffa_augmented_dataset import CFFADataset as CFFADataset_v2  # noqa: E402


# ============ å…¨å±€é…ç½® ============

SIZE = 512  # å•å¼ å›¾åƒå°ºå¯¸ï¼ˆCFå’ŒFAå„512x512ï¼‰
JOINT_HEIGHT = 512  # Jointå›¾åƒé«˜åº¦
JOINT_WIDTH = 1024  # Jointå›¾åƒå®½åº¦ï¼ˆ512+512ï¼‰
DEVICE = torch.device("cuda")

# SDXL æ¨¡å‹è·¯å¾„
BASE_MODEL_DIR = "/data/student/Fengjunming/SDXL_ControlNet/models/sdxl-base"

# Joint ç”Ÿæˆæ¨¡å‹è¾“å‡ºæ ¹ç›®å½•
OUT_ROOT = "/data/student/Fengjunming/SDXL_ControlNet/results/out_joint_sdxl_cffa_pairs"


# ============ 1. è¾…åŠ©å‡½æ•° ============

def get_joint_prompt_embeds_sdxl(bs, tokenizer, tokenizer_2, text_encoder, text_encoder_2):
    """
    ç”¨äº Joint CF-FA ç”Ÿæˆçš„å›ºå®šæ–‡æœ¬æç¤ºï¼ˆSDXLç‰ˆæœ¬ï¼‰ã€‚
    SDXL ä½¿ç”¨ä¸¤ä¸ª Text Encoderï¼š
    - text_encoder: CLIP-ViT-L/14
    - text_encoder_2: OpenCLIP-ViT-bigG/14
    
    è¿”å›ï¼š
    - prompt_embeds: [bs, 77, 2048] æ‹¼æ¥åçš„æ–‡æœ¬åµŒå…¥
    - pooled_prompt_embeds: [bs, 1280] æ± åŒ–åçš„æ–‡æœ¬åµŒå…¥
    """
    prompt = (
        "A single seamless retinal image divided exactly in half. "
        "[LEFT HALF]: colorful fundus photography, natural orange and red retina, macular details. "
        "[RIGHT HALF]: monochrome fluorescein angiography, absolute grayscale, high contrast white vessels on black background. "
        "The vascular tree must perfectly connect and mirror across the center line."
    )
    prompts = [prompt] * bs
    
    # ç¬¬ä¸€ä¸ª Text Encoder (CLIP-ViT-L/14)
    inputs_1 = tokenizer(
        prompts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    outputs_1 = text_encoder(inputs_1.input_ids, output_hidden_states=True)
    prompt_embeds_1 = outputs_1.hidden_states[-2]  # å€’æ•°ç¬¬äºŒå±‚ [bs, 77, 768]
    
    # ç¬¬äºŒä¸ª Text Encoder (OpenCLIP-ViT-bigG/14)
    inputs_2 = tokenizer_2(
        prompts,
        padding="max_length",
        max_length=tokenizer_2.model_max_length,
        truncation=True,
        return_tensors="pt",
    ).to(DEVICE)
    outputs_2 = text_encoder_2(inputs_2.input_ids, output_hidden_states=True)
    prompt_embeds_2 = outputs_2.hidden_states[-2]  # å€’æ•°ç¬¬äºŒå±‚ [bs, 77, 1280]
    pooled_prompt_embeds = outputs_2.text_embeds  # æ± åŒ–è¾“å‡º [bs, 1280]
    
    # æ‹¼æ¥ä¸¤ä¸ªç¼–ç å™¨çš„è¾“å‡º
    prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)  # [bs, 77, 2048]
    
    return prompt_embeds, pooled_prompt_embeds


def compute_time_ids(original_size=(JOINT_HEIGHT, JOINT_WIDTH), crops_coords_top_left=(0, 0)):
    """
    è®¡ç®— SDXL çš„ Time IDsï¼ˆç”¨äºå‘ŠçŸ¥æ¨¡å‹å›¾åƒå°ºå¯¸ä¿¡æ¯ï¼‰ã€‚
    
    Args:
        original_size: (height, width) åŸå§‹å›¾åƒå°ºå¯¸
        crops_coords_top_left: (top, left) è£å‰ªèµ·ç‚¹åæ ‡
    
    Returns:
        add_time_ids: [1, 6] tensor
    """
    target_size = original_size  # è®­ç»ƒæ—¶ä¸åšè£å‰ªï¼Œtarget = original
    add_time_ids = list(original_size + crops_coords_top_left + target_size)
    # ç»“æœ: [512, 1024, 0, 0, 512, 1024]
    add_time_ids = torch.tensor([add_time_ids], dtype=torch.long, device=DEVICE)
    return add_time_ids


def get_dynamic_lr(step, max_steps, base_lr=5e-5, min_lr=1e-5):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡è¡°å‡ï¼ˆä¸ v22 ä¸€è‡´ï¼‰ã€‚"""
    if step < 4000:
        return base_lr
    progress = min((step - 4000) / (max_steps - 4000), 1.0)
    return min_lr + (base_lr - min_lr) * (1 + math.cos(progress * math.pi)) / 2


# ============ 2. æŸå¤±ï¼ˆå¤ç”¨ v22 é€»è¾‘ï¼‰ ============

def _gaussian_kernel_1d(kernel_size: int, sigma: float, device, dtype):
    half = kernel_size // 2
    coords = torch.arange(kernel_size, device=device, dtype=dtype) - half
    gauss = torch.exp(-0.5 * (coords / sigma) ** 2)
    return gauss / gauss.sum()


def gaussian_blur_latent(x, kernel_size=7, sigma=1.5):
    C = x.shape[1]
    k = _gaussian_kernel_1d(kernel_size, sigma, x.device, x.dtype)
    pad = kernel_size // 2
    # æ°´å¹³æ–¹å‘
    kw = k.view(1, 1, 1, kernel_size).expand(C, 1, 1, kernel_size)
    x = F.conv2d(x, kw, padding=(0, pad), groups=C)
    # å‚ç›´æ–¹å‘
    kh = k.view(1, 1, kernel_size, 1).expand(C, 1, kernel_size, 1)
    x = F.conv2d(x, kh, padding=(pad, 0), groups=C)
    return x


def compute_hf_texture_loss(pred_x0, gt_x0, kernel_size=7, sigma=1.5):
    pred_blur = gaussian_blur_latent(pred_x0, kernel_size, sigma)
    gt_blur = gaussian_blur_latent(gt_x0, kernel_size, sigma)
    pred_hf = pred_x0 - pred_blur
    gt_hf = gt_x0 - gt_blur
    return F.l1_loss(pred_hf, gt_hf)


def compute_total_loss(noise_pred, noise, noisy_latents, latents,
                       alphas_cumprod, timesteps, hf_lambda=0.5):
    """
    å™ªå£° MSE + latent é«˜é¢‘ L1ã€‚
    """
    loss_mse = F.mse_loss(noise_pred, noise)

    alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1, 1).to(noisy_latents.device)
    pred_x0 = (noisy_latents - (1.0 - alpha_t).sqrt() * noise_pred) / (alpha_t.sqrt() + 1e-8)
    pred_x0 = pred_x0.clamp(-10.0, 10.0)

    loss_hf = compute_hf_texture_loss(pred_x0, latents)

    total = loss_mse + hf_lambda * loss_hf
    return total, loss_mse.item(), loss_hf.item()


# ============ 3. éªŒè¯ä¸å¯è§†åŒ– ============

VAL_TIMESTEPS = [200, 500, 800]


def build_joint_image(cf, fa):
    """
    å°† CF, FA (B,3,512,512, [-1,1]) **ç›´æ¥æ‹¼æ¥**æˆ joint (B,3,512,1024)ã€‚
    
    SDXLç‰ˆæœ¬ï¼šæ— éœ€å‹ç¼©ï¼Œä¿ç•™å®Œæ•´åˆ†è¾¨ç‡ï¼
    """
    # ç›´æ¥åœ¨å®½åº¦ç»´åº¦æ‹¼æ¥ï¼Œæ— éœ€æ’å€¼
    joint = torch.cat([cf, fa], dim=3)  # [B, 3, 512, 1024]
    return joint


def evaluate_joint(val_loader, vae, unet, noise_scheduler, tokenizer, tokenizer_2, 
                   text_encoder, text_encoder_2, args):
    """
    éªŒè¯ Joint ç”Ÿæˆæ¨¡å‹ï¼šåœ¨å›ºå®šæ—¶é—´æ­¥ä¸Šè¯„ä¼°å™ªå£°é¢„æµ‹ MSEï¼ˆSDXLç‰ˆæœ¬ï¼‰ã€‚
    """
    if hasattr(unet, "eval"):
        unet.eval()

    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            cf, fa, _, _ = batch  # dataset: (cond_tile=CF, tgt=FA, ...)
            cf, fa = cf.to(DEVICE), fa.to(DEVICE)
            b = cf.shape[0]

            joint = build_joint_image(cf, fa)  # [B, 3, 512, 1024]
            latents = vae.encode(joint).latent_dist.sample() * vae.config.scaling_factor
            # latents shape: [B, 4, 64, 128]
            
            prompt_embeds, pooled_prompt_embeds = get_joint_prompt_embeds_sdxl(
                b, tokenizer, tokenizer_2, text_encoder, text_encoder_2
            )
            
            # è®¡ç®— time_ids
            time_ids = compute_time_ids().repeat(b, 1)  # [b, 6]

            sample_losses = []
            for t_val in VAL_TIMESTEPS:
                timesteps = torch.full((b,), t_val, device=DEVICE, dtype=torch.long)
                noise = torch.randn_like(latents)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                # SDXL UNet è°ƒç”¨
                if hasattr(unet, "base_model"):
                    noise_pred = unet.base_model(
                        sample=noisy_latents,
                        timestep=timesteps,
                        encoder_hidden_states=prompt_embeds,
                        added_cond_kwargs={
                            "text_embeds": pooled_prompt_embeds,
                            "time_ids": time_ids,
                        },
                        return_dict=False,
                    )[0]
                else:
                    noise_pred = unet(
                        noisy_latents, 
                        timesteps, 
                        prompt_embeds,
                        added_cond_kwargs={
                            "text_embeds": pooled_prompt_embeds,
                            "time_ids": time_ids,
                        },
                    ).sample

                sample_losses.append(F.mse_loss(noise_pred, noise).item())

            val_losses.append(np.mean(sample_losses))

    if hasattr(unet, "train"):
        unet.train()

    torch.cuda.empty_cache()
    return np.mean(val_losses)


@torch.no_grad()
def visualize_random_pairs(unet, vae, tokenizer, tokenizer_2, text_encoder, text_encoder_2,
                           num_samples: int, out_dir: str, steps: int = 50):
    """
    ä»çº¯å™ªå£°ç”Ÿæˆè‹¥å¹² joint CF-FA å›¾åƒï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ï¼ˆSDXLç‰ˆæœ¬ï¼‰ã€‚
    æ¯ä¸ªæ ·æœ¬ä¿å­˜ä¸ºï¼š
      pair_xx/cf.png
      pair_xx/fa.png
      pair_xx/joint.png
    """
    os.makedirs(out_dir, exist_ok=True)

    if hasattr(unet, "eval"):
        unet.eval()
    if hasattr(vae, "eval"):
        vae.eval()
    if hasattr(text_encoder, "eval"):
        text_encoder.eval()
    if hasattr(text_encoder_2, "eval"):
        text_encoder_2.eval()

    prompt_embeds, pooled_prompt_embeds = get_joint_prompt_embeds_sdxl(
        1, tokenizer, tokenizer_2, text_encoder, text_encoder_2
    )
    time_ids = compute_time_ids()  # [1, 6]

    scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    scheduler.set_timesteps(steps)

    in_channels = (
        unet.base_model.config.in_channels
        if hasattr(unet, "base_model")
        else unet.config.in_channels
    )
    # SDXL latent shape: [1, 4, 64, 128] for 512x1024 image
    latent_shape = (1, in_channels, JOINT_HEIGHT // 8, JOINT_WIDTH // 8)

    def tensor_to_pil(x: torch.Tensor) -> Image.Image:
        x = (x.clamp(-1, 1) + 1) / 2.0
        x = x.cpu().permute(1, 2, 0).numpy()
        x = (x * 255).round().astype("uint8")
        return Image.fromarray(x)

    for idx in range(num_samples):
        latents = torch.randn(latent_shape, device=DEVICE)

        for t in scheduler.timesteps:
            if hasattr(unet, "base_model"):
                noise_pred = unet.base_model(
                    sample=latents,
                    timestep=t,
                    encoder_hidden_states=prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": time_ids,
                    },
                    return_dict=False,
                )[0]
            else:
                noise_pred = unet(
                    latents,
                    t,
                    prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": time_ids,
                    },
                ).sample

            latents = scheduler.step(noise_pred, t, latents).prev_sample

        latents_final = latents / vae.config.scaling_factor
        imgs_joint = vae.decode(latents_final).sample  # [1,3,512,1024], [-1,1]
        joint_img = imgs_joint[0]

        # æ‹†åˆ† joint -> CF/FAï¼ˆæ— éœ€æ’å€¼ï¼Œç›´æ¥æŒ‰å®½åº¦åˆ‡åˆ†ï¼‰
        # joint_img shape: [3, 512, 1024]
        cf_full = joint_img[:, :, :SIZE]  # [3, 512, 512]
        fa_full = joint_img[:, :, SIZE:]  # [3, 512, 512]

        img_joint = tensor_to_pil(joint_img)
        img_cf = tensor_to_pil(cf_full)
        img_fa = tensor_to_pil(fa_full)

        pair_dir = os.path.join(out_dir, f"pair_{idx:02d}")
        os.makedirs(pair_dir, exist_ok=True)
        img_joint.save(os.path.join(pair_dir, "joint.png"))
        img_cf.save(os.path.join(pair_dir, "cf.png"))
        img_fa.save(os.path.join(pair_dir, "fa.png"))


# ============ 4. ä¸»è®­ç»ƒæµç¨‹ ============


def main():
    parser = argparse.ArgumentParser(description="Joint CF-FA ç”Ÿæˆæ¨¡å‹è®­ç»ƒè„šæœ¬ v25-SDXL")
    parser.add_argument("-n", "--name", default="joint_cffa_v25_sdxl")
    parser.add_argument("--max_steps", type=int, default=15000)
    parser.add_argument("--unet_lora_rank", type=int, default=16, help="UNet LoRA rank")
    parser.add_argument("--unet_lora_alpha", type=int, default=16, help="UNet LoRA alpha")
    parser.add_argument("--offset_noise_strength", type=float, default=0.1, help="Offset noise strength for better contrast")
    parser.add_argument("--hf_lambda", type=float, default=0.5, help="é«˜é¢‘çº¹ç†æŸå¤±æƒé‡ï¼Œæ¨è 0.3~1.0")
    args = parser.parse_args()

    out_dir = os.path.join(OUT_ROOT, args.name)
    os.makedirs(out_dir, exist_ok=True)

    # 1. æ•°æ®åŠ è½½ï¼ˆä»…ä½¿ç”¨ CFFAï¼Œå¯¹åº” cf2fa æ¨¡å¼ï¼‰
    train_ds = CFFADataset_v2(split="train", mode="cf2fa")
    val_ds = CFFADataset_v2(split="test", mode="cf2fa")

    train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)

    # 2. æ¨¡å‹åŠ è½½ï¼ˆSDXLç‰ˆæœ¬ï¼‰
    print("\n========== SDXL æ¨¡å‹åŠ è½½ ==========")
    print(f"æ¨¡å‹è·¯å¾„: {BASE_MODEL_DIR}")
    
    # SDXL ä½¿ç”¨ä¸¤ä¸ª tokenizer å’Œ text encoder
    tokenizer = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer")
    tokenizer_2 = CLIPTokenizer.from_pretrained(BASE_MODEL_DIR, subfolder="tokenizer_2")
    text_encoder = CLIPTextModel.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder").to(DEVICE)
    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(BASE_MODEL_DIR, subfolder="text_encoder_2").to(DEVICE)
    
    vae = AutoencoderKL.from_pretrained(BASE_MODEL_DIR, subfolder="vae").to(DEVICE)
    unet = UNet2DConditionModel.from_pretrained(BASE_MODEL_DIR, subfolder="unet").to(DEVICE)

    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    text_encoder_2.requires_grad_(False)
    
    print(f"âœ“ SDXL ç»„ä»¶åŠ è½½å®Œæˆ")
    print(f"  - Text Encoder 1: CLIP-ViT-L/14")
    print(f"  - Text Encoder 2: OpenCLIP-ViT-bigG/14")
    print(f"  - VAE: SDXL VAE")
    print(f"  - UNet: SDXL UNet2DConditionModel")

    print(f"\n========== UNet LoRA é…ç½® ==========")
    unet.requires_grad_(False)

    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
    lora_config = LoraConfig(
        r=args.unet_lora_rank,
        lora_alpha=args.unet_lora_alpha,
        target_modules=target_modules,
        lora_dropout=0.0,
        bias="none",
        task_type=TaskType.FEATURE_EXTRACTION,
    )

    unet = get_peft_model(unet, lora_config)

    unet_lora_params = [p for p in unet.parameters() if p.requires_grad]
    unet_lora_num = sum(p.numel() for p in unet_lora_params)
    unet_total_num = sum(p.numel() for p in unet.parameters())

    print(f"âœ“ UNet LoRA å·²åº”ç”¨")
    print(f"  - Rank: {args.unet_lora_rank}, Alpha: {args.unet_lora_alpha}")
    print(f"  - ç›®æ ‡æ¨¡å—: {target_modules}")
    print(f"  - LoRA å¯è®­ç»ƒå‚æ•°: {unet_lora_num:,} ({unet_lora_num/1e6:.2f}M)")
    print(f"  - UNet æ€»å‚æ•°: {unet_total_num:,} ({unet_total_num/1e6:.2f}M)")
    print(f"  - å‚æ•°å æ¯”: {unet_lora_num/unet_total_num*100:.2f}%")

    total_trainable = unet_lora_num
    print(f"\nâœ“ æ€»å¯è®­ç»ƒå‚æ•°: {total_trainable:,} ({total_trainable/1e6:.2f}M)")

    noise_scheduler = DDPMScheduler.from_pretrained(BASE_MODEL_DIR, subfolder="scheduler")
    optimizer = torch.optim.AdamW(unet_lora_params, lr=5e-5, weight_decay=1e-2)

    print(f"\nâœ“ ä¼˜åŒ–å™¨: AdamW (lr=5e-5, weight_decay=1e-2)")
    print(f"  - Offset Noise å¼ºåº¦: {args.offset_noise_strength}")

    # 3. è®­ç»ƒçŠ¶æ€
    global_step = 0
    best_val_loss = float("inf")
    start_time = time.time()
    loss_accumulator = []

    print(f"\n========== å¼€å§‹è®­ç»ƒ Joint CF-FA ç”Ÿæˆæ¨¡å‹ ==========")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_ds)} (å…¨é‡ï¼Œå›ºå®šæ—¶é—´æ­¥ {VAL_TIMESTEPS})")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}\n")

    while global_step < args.max_steps:
        for batch in train_loader:
            if global_step >= args.max_steps:
                break

            cf, fa, cp, fp = batch
            cf, fa = cf.to(DEVICE), fa.to(DEVICE)
            b = cf.shape[0]

            # æ„å»º joint å›¾åƒå¹¶ç¼–ç 
            joint = build_joint_image(cf, fa)
            latents = vae.encode(joint).latent_dist.sample() * vae.config.scaling_factor

            # Offset Noise
            noise = torch.randn_like(latents)
            if args.offset_noise_strength > 0:
                noise += args.offset_noise_strength * torch.randn(
                    latents.shape[0], latents.shape[1], 1, 1, device=latents.device
                )

            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (b,), device=DEVICE
            ).long()
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
            
            # SDXL prompt embeds
            prompt_embeds, pooled_prompt_embeds = get_joint_prompt_embeds_sdxl(
                b, tokenizer, tokenizer_2, text_encoder, text_encoder_2
            )
            time_ids = compute_time_ids().repeat(b, 1)  # [b, 6]

            # UNet å‰å‘ï¼ˆSDXLç‰ˆæœ¬ï¼‰
            if hasattr(unet, "base_model"):
                noise_pred = unet.base_model(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": time_ids,
                    },
                    return_dict=False,
                )[0]
            else:
                noise_pred = unet(
                    noisy_latents, 
                    timesteps, 
                    prompt_embeds,
                    added_cond_kwargs={
                        "text_embeds": pooled_prompt_embeds,
                        "time_ids": time_ids,
                    },
                ).sample

            # æŸå¤±
            loss, loss_mse_val, loss_hf_val = compute_total_loss(
                noise_pred,
                noise,
                noisy_latents,
                latents,
                noise_scheduler.alphas_cumprod,
                timesteps,
                hf_lambda=args.hf_lambda,
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            current_lr = get_dynamic_lr(global_step, args.max_steps)
            for param_group in optimizer.param_groups:
                param_group["lr"] = current_lr

            loss_accumulator.append((loss.item(), loss_mse_val, loss_hf_val))

            if global_step % 100 == 0:
                elapsed = time.time() - start_time
                arr = np.array(loss_accumulator)
                avg_loss, avg_mse, avg_hf = arr[:, 0].mean(), arr[:, 1].mean(), arr[:, 2].mean()
                loss_accumulator = []

                t_val = timesteps[0].item()
                msg = (
                    f"[joint-gen] Step {global_step:5d}/{args.max_steps} | "
                    f"lr:{current_lr:.2e} | loss:{avg_loss:.4f} "
                    f"(mse:{avg_mse:.4f} hf:{avg_hf:.4f}) | t={t_val:3d} | "
                    f"{elapsed:.1f}s"
                )
                print(msg)
                with open(os.path.join(out_dir, "training_log.txt"), "a", encoding="utf-8") as f:
                    f.write(msg + "\n")

                start_time = time.time()

            # æ¯ 500 æ­¥éªŒè¯ + å¯è§†åŒ– + checkpoint
            if global_step % 500 == 0:
                val_loss = evaluate_joint(
                    val_loader, vae, unet, noise_scheduler, 
                    tokenizer, tokenizer_2, text_encoder, text_encoder_2, args
                )

                val_msg = f"[éªŒè¯] Step {global_step} | Loss: {val_loss:.6f} | Best: {best_val_loss:.6f}"
                print(f"\n{val_msg}")
                with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                    f.write(val_msg + "\n")

                # å¯è§†åŒ–éšæœºç”Ÿæˆçš„ joint CF-FA å›¾åƒ
                vis_dir = os.path.join(out_dir, f"step_{global_step:06d}_random_pairs")
                print(f"[å¯è§†åŒ–] åœ¨ {vis_dir} ç”Ÿæˆ 10 ç»„éšæœº CF-FA å›¾åƒ...")
                visualize_random_pairs(
                    unet, vae, tokenizer, tokenizer_2, text_encoder, text_encoder_2, 10, vis_dir, 50
                )

                # latest checkpoints (æ»šåŠ¨ä¿ç•™æœ€è¿‘ 3 ä¸ª)
                latest_root = os.path.join(out_dir, "latest_checkpoints")
                os.makedirs(latest_root, exist_ok=True)
                latest_step_dir = os.path.join(latest_root, f"step_{global_step:06d}")
                os.makedirs(latest_step_dir, exist_ok=True)

                unet_lora_dir = os.path.join(latest_step_dir, "unet_lora")
                os.makedirs(unet_lora_dir, exist_ok=True)
                unet.save_pretrained(unet_lora_dir)

                with open(os.path.join(latest_step_dir, "info.txt"), "w", encoding="utf-8") as f:
                    f.write(f"Step: {global_step}\n")
                    f.write(f"Validation Loss: {val_loss:.6f}\n")
                    f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                    f.write(f"Offset Noise: {args.offset_noise_strength}\n")

                # æ»šåŠ¨åˆ é™¤å¤šä½™çš„ latest
                subdirs = sorted(
                    [d for d in os.listdir(latest_root) if d.startswith("step_")]
                )
                if len(subdirs) > 3:
                    for old in subdirs[:-3]:
                        shutil.rmtree(os.path.join(latest_root, old))

                # best checkpoint
                if val_loss < best_val_loss - 1e-4:
                    best_val_loss = val_loss
                    best_dir = os.path.join(out_dir, "best_checkpoint")
                    os.makedirs(best_dir, exist_ok=True)

                    best_unet_lora_dir = os.path.join(best_dir, "unet_lora")
                    os.makedirs(best_unet_lora_dir, exist_ok=True)
                    unet.save_pretrained(best_unet_lora_dir)

                    with open(os.path.join(best_dir, "best_info.txt"), "w", encoding="utf-8") as f:
                        f.write(f"Best Step: {global_step}\n")
                        f.write(f"Best Validation Loss: {best_val_loss:.6f}\n")
                        f.write(f"UNet LoRA Rank: {args.unet_lora_rank}\n")
                        f.write(f"Offset Noise: {args.offset_noise_strength}\n")

                    best_msg = f"ğŸ‰ å‘ç°æ›´å¥½çš„ Joint CF-FA ç”Ÿæˆæ¨¡å‹ (Step {global_step})ï¼Œå·²ä¿å­˜è‡³ best_checkpoint\n"
                    print(best_msg)
                    with open(os.path.join(out_dir, "validation_log.txt"), "a", encoding="utf-8") as f:
                        f.write(best_msg)

            global_step += 1


if __name__ == "__main__":
    main()

